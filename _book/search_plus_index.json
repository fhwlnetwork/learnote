{"./":{"url":"./","title":"简介","keywords":"","body":"先写个目录吧 这是码农转运维的心塞路 linux笔记 linux基础命令 linux时间 linux系统优化 linux磁盘分区 Nginx笔记 安装 Tomact笔记 mysql笔记 安装 主从配置 redis笔记 安装 主从配置_哨兵配置 redis集群配置 redis集群扩容收缩 工具管理 docker笔记 docker简介 乌班图安装docker centos安装docker 阿里云镜像加速 镜像命令 容器命令 docker镜像 推送镜像到阿里云 推送镜像到本地 Docker容器数据卷 k8_docker笔记 go环境安装 Kubernetes网络 ES笔记 安装 交互 操作语言 集群配置 ELK笔记 安装 nginxjson日志采集 nginix正常日志和错误日志 tomcat日志收集 java多行日志收集 收集docker日志 filebet收集ngingx日志 redis作为缓存收集日志 kafka缓存收集日志 kibana画图 redis作为缓存收集日志 kafka缓存收集日志 使用nginx+keepalived代理多台redis 监控服务zabbix 安装 python笔记 mac安装pyhton3.0环境 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 15:34:15 "},"linux/lvs.html":{"url":"linux/lvs.html","title":"LVS负载均衡","keywords":"","body":"LVS负载均衡 一、LVS简介 LVS（Linux Virtual Server）即Linux虚拟服务器，是由章文嵩博士主导的开源负载均衡项目，目前LVS已经被集成到Linux内核模块中。该项目在Linux内核中实现了基于IP的数据请求负载均衡调度方案，其体系结构如图1所示，终端互联网用户从外部访问公司的外部负载均衡服务器，终端用户的Web请求会发送给LVS调度器，调度器根据自己预设的算法决定将该请求发送给后端的某台Web服务器，比如，轮询算法可以将外部的请求平均分发给后端的所有服务器，终端用户访问LVS调度器虽然会被转发到后端真实的服务器，但如果真实服务器连接的是相同的存储，提供的服务也是相同的服务，最终用户不管是访问哪台真实服务器，得到的服务内容都是一样的，整个集群对用户而言都是透明的。最后根据LVS工作模式的不同，真实服务器会选择不同的方式将用户需要的数据发送到终端用户，LVS工作模式分为NAT模式、TUN模式、以及DR模式。 二、三种工作模式的解析。 1、基于NAT的LVS模式负载均衡 NAT（Network Address Translation）即网络地址转换，其作用是通过数据报头的修改，使得位于企业内部的私有IP地址可以访问外网，以及外部用用户可以访问位于公司内部的私有IP主机。VS/NAT工作模式拓扑结构如图2所示，LVS负载调度器可以使用两块网卡配置不同的IP地址，eth0设置为私钥IP与内部网络通过交换设备相互连接，eth1设备为外网IP与外部网络联通。 第一步，用户通过互联网DNS服务器解析到公司负载均衡设备上面的外网地址，相对于真实服务器而言，LVS外网IP又称VIP（Virtual IP Address），用户通过访问VIP，即可连接后端的真实服务器（Real Server），而这一切对用户而言都是透明的，用户以为自己访问的就是真实服务器，但他并不知道自己访问的VIP仅仅是一个调度器，也不清楚后端的真实服务器到底在哪里、有多少真实服务器。 第二步，用户将请求发送至124.126.147.168，此时LVS将根据预设的算法选择后端的一台真实服务器（192.168.0.1~192.168.0.3），将数据请求包转发给真实服务器，并且在转发之前LVS会修改数据包中的目标地址以及目标端口，目标地址与目标端口将被修改为选出的真实服务器IP地址以及相应的端口。 第三步，真实的服务器将响应数据包返回给LVS调度器，调度器在得到响应的数据包后会将源地址和源端口修改为VIP及调度器相应的端口，修改完成后，由调度器将响应数据包发送回终端用户，另外，由于LVS调度器有一个连接Hash表，该表中会记录连接请求及转发信息，当同一个连接的下一个数据包发送给调度器时，从该Hash表中可以直接找到之前的连接记录，并根据记录信息选出相同的真实服务器及端口信息。 2、基于TUN的LVS负载均衡 在LVS（NAT）模式的集群环境中，由于所有的数据请求及响应的数据包都需要经过LVS调度器转发，如果后端服务器的数量大于10台，则调度器就会成为整个集群环境的瓶颈。我们知道，数据请求包往往远小于响应数据包的大小。因为响应数据包中包含有客户需要的具体数据，所以LVS（TUN）的思路就是将请求与响应数据分离，让调度器仅处理数据请求，而让真实服务器响应数据包直接返回给客户端。VS/TUN工作模式拓扑结构如图3所示。其中，IP隧道（IP tunning）是一种数据包封装技术，它可以将原始数据包封装并添加新的包头（内容包括新的源地址及端口、目标地址及端口），从而实现将一个目标为调度器的VIP地址的数据包封装，通过隧道转发给后端的真实服务器（Real Server），通过将客户端发往调度器的原始数据包封装，并在其基础上添加新的数据包头（修改目标地址为调度器选择出来的真实服务器的IP地址及对应端口），LVS（TUN）模式要求真实服务器可以直接与外部网络连接，真实服务器在收到请求数据包后直接给客户端主机响应数据。 3、基于DR的LVS负载均衡 在LVS（TUN）模式下，由于需要在LVS调度器与真实服务器之间创建隧道连接，这同样会增加服务器的负担。与LVS（TUN）类似，DR模式也叫直接路由模式，其体系结构如图4所示，该模式中LVS依然仅承担数据的入站请求以及根据算法选出合理的真实服务器，最终由后端真实服务器负责将响应数据包发送返回给客户端。与隧道模式不同的是，直接路由模式（DR模式）要求调度器与后端服务器必须在同一个局域网内，VIP地址需要在调度器与后端所有的服务器间共享，因为最终的真实服务器给客户端回应数据包时需要设置源IP为VIP地址，目标IP为客户端IP，这样客户端访问的是调度器的VIP地址，回应的源地址也依然是该VIP地址（真实服务器上的VIP），客户端是感觉不到后端服务器存在的。由于多台计算机都设置了同样一个VIP地址，所以在直接路由模式中要求调度器的VIP地址是对外可见的，客户端需要将请求数据包发送到调度器主机，而所有的真实服务器的VIP地址必须配置在Non-ARP的网络设备上，也就是该网络设备并不会向外广播自己的MAC及对应的IP地址，真实服务器的VIP对外界是不可见的，但真实服务器却可以接受目标地址VIP的网络请求，并在回应数据包时将源地址设置为该VIP地址。调度器根据算法在选出真实服务器后，在不修改数据报文的情况下，将数据帧的MAC地址修改为选出的真实服务器的MAC地址，通过交换机将该数据帧发给真实服务器。整个过程中，真实服务器的VIP不需要对外界可见。 lvs负载均衡调度算法 根据前面的介绍，我们了解了LVS的三种工作模式，但不管实际环境中采用的是哪种模式，调度算法进行调度的策略与算法都是LVS的核心技术，LVS在内核中主要实现了一下十种调度算法。 1.轮询调度 轮询调度（Round Robin 简称'RR'）算法就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是实现简单。轮询算法假设所有的服务器处理请求的能力都一样的，调度器会将所有的请求平均分配给每个真实服务器。 2.加权轮询调度 加权轮询（Weight Round Robin 简称'WRR'）算法主要是对轮询算法的一种优化与补充，LVS会考虑每台服务器的性能，并给每台服务器添加一个权值，如果服务器A的权值为1，服务器B的权值为2，则调度器调度到服务器B的请求会是服务器A的两倍。权值越高的服务器，处理的请求越多。 3.最小连接调度 最小连接调度（Least Connections 简称'LC'）算法是把新的连接请求分配到当前连接数最小的服务器。最小连接调度是一种动态的调度算法，它通过服务器当前活跃的连接数来估计服务器的情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1；当连接中断或者超时，其连接数减1。 （集群系统的真实服务器具有相近的系统性能，采用最小连接调度算法可以比较好地均衡负载。) 4.加权最小连接调度 加权最少连接（Weight Least Connections 简称'WLC'）算法是最小连接调度的超集，各个服务器相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 5.基于局部的最少连接 基于局部的最少连接调度（Locality-Based Least Connections 简称'LBLC'）算法是针对请求报文的目标IP地址的 负载均衡调度，目前主要用于Cache集群系统，因为在Cache集群客户请求报文的目标IP地址是变化的。这里假设任何后端服务器都可以处理任一请求，算法的设计目标是在服务器的负载基本平衡情况下，将相同目标IP地址的请求调度到同一台服务器，来提高各台服务器的访问局部性和Cache命中率，从而提升整个集群系统的处理能力。LBLC调度算法先根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则使用'最少连接'的原则选出一个可用的服务器，将请求发送到服务器。 6.带复制的基于局部性的最少连接 带复制的基于局部性的最少连接（Locality-Based Least Connections with Replication 简称'LBLCR'）算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统，它与LBLC算法不同之处是它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。按'最小连接'原则从该服务器组中选出一一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按'最小连接'原则从整个集群中选出一台服务器，将该服务器加入到这个服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。 7.目标地址散列调度 目标地址散列调度（Destination Hashing 简称'DH'）算法先根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。 8.源地址散列调度U 源地址散列调度（Source Hashing 简称'SH'）算法先根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。它采用的散列函数与目标地址散列调度算法的相同，它的算法流程与目标地址散列调度算法的基本相似。 9.最短的期望的延迟 最短的期望的延迟调度（Shortest Expected Delay 简称'SED'）算法基于WLC算法。举个例子吧，ABC三台服务器的权重分别为1、2、3 。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用SED算法后会进行一个运算 A：（1+1）/1=2 B：（1+2）/2=3/2 C：（1+3）/3=4/3 就把请求交给得出运算结果最小的服务器。 10.最少队列调度 最少队列调度（Never Queue 简称'NQ'）算法，无需队列。如果有realserver的连接数等于0就直接分配过去，不需要在进行SED运算。 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/commond.html":{"url":"linux/commond.html","title":"linux基础命令","keywords":"","body":"系统管理的基础知识 系统命令提示组成 【root@hostname ~】# ---------命令提示符 作用:只有在命令提示符后面输入命令才有效果 组成： 1)登录用户的信息 2）@分隔符 3）主机名信息 4）当前所在系统的目录路径信息 系统命令是有语法规范 命令 参数 文件/路径 命令与参数之间有空格分隔 系统目录结构简介 linux目录结构从根开始 绝对路径:从根开始查处文件 缺点：寻找数据速度慢 优点：准确性高 相对路径：从当前位置 查找文件 优点：找数据速度更快 缺点：准确性低 系统的操命令 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/time_synchronism.html":{"url":"linux/time_synchronism.html","title":"linux时间","keywords":"","body":"时间管理 1、查看时间信息: [root@web01 /etc]# date Sun May 9 16:06:04 CST 2021 2、 调整时间显示格式 [root@web01 /etc]# date +%F 2021-05-09 [root@web01 /etc]# date \"+%F %T\" 2021-05-09 16:07:27 [root@web01 /etc]# date \"+%Y +%F %T\" 2021 +2021-05-09 16:07:58 [root@web01 /etc]# date \"+%Y-%m +%F %T\" 2021-05 +2021-05-09 16:09:03 [root@web01 /etc]# date \"+%Y-%m-%d +%F %T\" 2021-05-09 +2021-05-09 16:09:15 #显示历史时间信息: [root@web01 /etc]# date +%F -d \"-2day\" 2021-05-07 [root@web01 /etc]# date +%F -d \"1 day ago\" 2021-05-08 #显示未来时间信息: [root@web01 /etc]# # date -d \"+2day\" [root@web01 /etc]# date -d \"+2day\" Tue May 11 16:11:32 CST 2021 [root@web01 /etc]# date -d \"2day\" Tue May 11 16:11:47 CST 2021 3、如何实际修改系统时间 [root@web01 /etc]# date -s \"2020-04-17\" Fri Apr 17 00:00:00 CST 2020 [root@web01 /etc]# date Fri Apr 17 00:00:02 CST 2020 [root@web01 /etc]# date -s \"2020/04/17 14:00\" Fri Apr 17 14:00:00 CST 2020 [root@web01 /etc]# 4、时间同步 [root@web01 /etc]# yum install -y ntpdate ntp #配置ntp [root@web01 /var/lib/ntp]# vim /etc/ntp.conf 21 #server 0.centos.pool.ntp.org iburst 22 #server 1.centos.pool.ntp.org iburst 23 #server 2.centos.pool.ntp.org iburst 24 #server 3.centos.pool.ntp.org iburst 25 server ntp1.aliyun.com 在ntpd服务启动时，先使用ntpdate命令同步时间： [root@web01 ~]# ntpdate ntp1.aliyun.com [root@web01 /var/lib/ntp]# systemctl restart ntpd Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/fictory.html":{"url":"linux/fictory.html","title":"系统目录结构","keywords":"","body":"系统目录结构 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/majorzation.html":{"url":"linux/majorzation.html","title":"linux系统优化","keywords":"","body":"系统优化 1、系统的优化方法（基础优化） #1）了解系统的环境 #两个命令： # a)、cat /etc/redhat-release ------获得系统发行版本和具体系统版本信息 [root@web01 ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) # b)、 uname -a [root@web01 ~]# uname -a Linux web01 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux #记忆centos7系统的内核信息 #Q：一起你用的LINUX系统是什么环境的？ #a: centos7 具体型号7.5 内核3.10 64位 2、操作系统优化---命令提示符优化 #优化方法: 修改PS1环境变量 #默认配置: [root@web01 ~]# echo $PS1 [\\u@\\h \\W]\\$ # \\u --- 显示当前登录用户名称 # \\h --- 显示系统主机名称 # \\W --- 显示当前所在目录信息(目录结构的最后结尾信息) 修改优化方法: 01. 修改命令提示符的内容: # ------显示全路径 vi /etc/profile 加入 export PS1='[\\u@\\H \\w]\\$' [root@web01 /etc/sysconfig]# source /etc/profile 02. 命令提示符如何修改颜色 # Linxu系统中如何给信息加颜色 \\[\\e[F;Bm] 文字内容 \\e[m ”[\\[\\e[31;40m]\\u\\e[m @\\h \\W]\\$ “ [root@web01 ~]# tail -5 /etc/profile export PS1='\\[\\e[32;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' 设置颜色 内容 结束 export PS1='\\[\\e[30;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 黑色提示符 export PS1='\\[\\e[31;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 红色提示符 export PS1='\\[\\e[32;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 绿色提示符 export PS1='\\[\\e[33;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 黄色提示符 export PS1='\\[\\e[34;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 蓝色提示符 export PS1='\\[\\e[35;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 粉色 export PS1='\\[\\e[36;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 浅蓝 export PS1='\\[\\e[37;1m\\][\\u@\\h \\W]\\$ \\[\\e[0m\\]' -- 白色 实现命令提示符是彩色的 #实现以上效果方法,在/etc/profile底行输入: [root@web01 /etc/sysconfig]# vim /etc/profile export PS1='[\\[\\e[31;1m\\]\\u@\\[\\e[32;1m\\]\\h\\[\\e[36;1m\\] \\w\\[\\e[33;1m\\]]\\$ ' 3、操作系统优化-----yum下载源优化 yum软件优势: 简单 快捷 01. 不需要到官方网站单独下载软件包(yum仓库) 02. 可以解决软件的依赖关系 yum优化方法: 1. 优化基础的yum源文件， #1）更换阿里云或者网易源 #通过阿里镜像源进行优化: curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #更新网易源 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo 2） #运行以下命令生成缓存： yum clean all yum makecache 02. 优化扩展的yum源文件 #通过阿里镜像源进行优化: # wget -O /etc/yum.repos.d/epe l.repo http://mirrors.aliyun.com/repo/epel-7.repo #检查可用的yum源信息 #yum repolist #如何查看软件是否安装? #利用rpm命令查看软件是否安装 #rpm -qa 查询的软件 --------q表示查询,-a表示所有 #查看软件包中有哪些信息 rpm -ql 软件名称 ---- -l表示列表显示 #查看文件信息属于哪个软件大礼包 #which 软件名称 #rpm -qf `软件名称 ` 4. 系统安全相关优化(将一些安全服务进行关闭) 1). 防火墙服务程序 centos6 查看防护墙服务状态 /etc/init.d/iptables status 临时关闭防火墙服务 /etc/init.d/iptables stop /etc/init.d/iptables status 永久关闭防火墙服务 chkconfig iptables off centos7 查看防火墙服务状态 systemctl status firewalld 临时关闭防火墙服务 systemctl stop firewalld systemctl status firewalld -- 操作完确认 永久关闭防火墙服务 systemctl disable firewalld 补充: 查看服务状态信息简便方法 systemctl is-active firewalld --- 检查服务是否正常运行 systemctl is-enabled firewalld --- 检查确认服务是否开机运行 4、关闭selinux服务程序 1.什么是selinux： selinux(security enhanced linux)安全增强型linux系统，它是一个linux内核模块，也是linux的一个安全子系统。 selinux的主要作用就是最大限度地减小系统中服务进程可访问的资源（最小权限原则） 2.selinux有两个级别 强制和警告 setenforce 0|1 0表示警告(Permissive)，1表示强制（Enforcing） 3.selinux相当于一个插件 (内核级的插件) 4.selinux功能开启后，会关闭系统中不安全的功能 5.查看日志中的警告：cat /var/log/audit/audit.log 临时关闭: 检查确认: getenforce --- 确认selinux服务是否开启或是关闭的 如何关闭: [root@web01 /etc]# setenforce usage: setenforce [ Enforcing | Permissive | 1 | 0 ] Enforcing 1 --- 临时开启selinux Permissive 0 --- 临时关闭selinux setenforce 0 --- 临时关闭selinux服务 永久关闭: enforcing - SELinux security policy is enforced. （selinux服务处于正常开启状态） permissive - SELinux prints warnings instead of enforcing.（selinux服务被临时关闭了） disabled - No SELinux policy is loaded.（selinux服务彻底关闭） vi /etc/selinux/config SELINUX=disabled PS: 如果想让selinux配置文件生效,重启系统 05、字符编码优化 出现乱码的原因: 01. 系统字符集设置有问题 02. 远程软件字符集设置有问题 03. 文件编写字符集和系统查看的字符集不统一 出现乱码的原因: 01. 系统字符集设置有问题 02. 远程软件字符集设置有问题 03. 文件编写字符集和系统查看的字符集不统一 centos6 设置方法 # 查看默认编码信息: [root@web01 /etc]# echo $LANG --- LANG用于设置字符编码信息 en_US.UTF-8 #临时修改: [root@web01 ~]# LANG=XXX #永久修改: #方法一: [root@web01 ~]# tail -5 /etc/profile export LANG='en_US.UTF-8' #方法二: vi /etc/sysconfig/i18n LANG='en_US.UTF-8 source /etc/sysconfig/i18n centos7设置方法 # 查看默认编码信息 [root@web01 ~]# echo $LANG en_US.UTF-8 # 临时修改: [root@web01 ~]# echo $LANG en_US.UTF-8 LANG=XXX # 永久修改: # 方法一: 更加有先 [root@web01 ~]# tail -5 /etc/profile export LANG='en_US.UTF-8' # 方法二: [root@web01 ~]# cat /etc/locale.conf LANG=\"zh_CN.UTF-8\" # 补充：一条命令即临时设置，又永久设置 localectl set-locale LANG=zh_CN.GBK 06、使xshell软件远程连接速度加快 #第一个步骤：修改ssh服务配置文件 vi /etc/ssh/sshd_config 79 GSSAPIAuthentication no 115 UseDNS no #第二个步骤：重启ssh远程服务 systemctl restart sshd 7、时间同步：[时间同步]（linux/time_synchronism.md） Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/Disk_partition.html":{"url":"linux/Disk_partition.html","title":"linux磁盘分区","keywords":"","body":"磁盘分区 #不关机，添加硬盘，自动识别 [root@web01 ~]# echo \"- - -\" > /sys/class/scsi_host/host0/scan 1、磁盘分区实践--磁盘小于2T 第一个里程: 准备磁盘环境 准备了一块新的10G硬盘 第二个里程: 在系统中检查是否识别到了新的硬盘 fdisk -l --- 查看分区信息 [root@web01 ~]# echo \"- - -\" > /sys/class/scsi_host/host0/scan # 查看分区信息 [root@web01 ~]# fdisk -l 第三个里程: 对磁盘进行分区处理(fdisk-- 进行分区处理 查看分区信息) 指令说明 d delete a partition ***** 删除分区 g create a new empty GPT partition table 创建一个新的空的GPT分区表(可以对大于2T磁盘进行分区) l list known partition types 列出可以分区的类型??? m print this menu 输出帮助菜单 n add a new partition ***** 新建增加一个分区 p print the partition table ***** 输出分区的结果信息 q quit without saving changes 不保存退出 t change a partition's system id 改变分区的系统id==改变分区类型(LVM 增加swap分区大小) u change display/entry units 改变分区的方式 是否按照扇区进行划分 w write table to disk and exit ***** 将分区的信息写入分区表并退出==保存分区信息并退出 a ) 规划分4个主分区 ,1,2分区1g,3分区10g,其余的给第四分区 [root@web01 ~]# fdisk /dev/sdc Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p Partition number (1-4, default 1): 1 First sector (2048-41943039, default 2048): Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-41943039, default 41943039): +1G Partition 1 of type Linux and of size 1 GiB is set Command (m for help): n Partition type: p primary (1 primary, 0 extended, 3 free) e extended Select (default p): p Partition number (2-4, default 2): First sector (2099200-41943039, default 2099200): Using default value 2099200 Last sector, +sectors or +size{K,M,G} (2099200-41943039, default 41943039): +1G Partition 2 of type Linux and of size 1 GiB is set Command (m for help): n Partition type: p primary (2 primary, 0 extended, 2 free) e extended Select (default p): p Partition number (3,4, default 3): First sector (4196352-41943039, default 4196352): Last sector, +sectors or +size{K,M,G} (20971520-41943039, default 41943039): +10G Using default value 41943039 Partition 3 of type Linux and of size 10 GiB is set Command (m for help): n Partition type: p primary (3 primary, 0 extended, 1 free) e extended Select (default e): p Selected partition 4 First sector (4196352-41943039, default 4196352): Using default value 4196352 Last sector, +sectors or +size{K,M,G} (4196352-20971519, default 20971519): Using default value 20971519 Partition 4 of type Linux and of size 8 GiB is set Command (m for help): p Disk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xb478de23 Device Boot Start End Blocks Id System /dev/sdc1 2048 2099199 1048576 83 Linux /dev/sdc2 2099200 4196351 1048576 83 Linux /dev/sdc3 20971520 41943039 10485760 83 Linux /dev/sdc4 4196352 20971519 8387584 83 Linux Partition table entries are not in disk order Command (m for help): w b) 规划分3个主分区 1个扩展分区 每个主分区1G 剩余都给扩展分区 [root@web01 ~]# fdisk /dev/sdc Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xb478de23 Device Boot Start End Blocks Id System Command (m for help): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p Partition number (1-4, default 1): First sector (2048-41943039, default 2048): Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-41943039, default 41943039): 1G Value out of range. Last sector, +sectors or +size{K,M,G} (2048-41943039, default 41943039): +1G Partition 1 of type Linux and of size 1 GiB is set Command (m for help): n Partition type: p primary (1 primary, 0 extended, 3 free) e extended Select (default p): p Partition number (2-4, default 2): First sector (2099200-41943039, default 2099200): Using default value 2099200 Last sector, +sectors or +size{K,M,G} (2099200-41943039, default 41943039): +1G Partition 2 of type Linux and of size 1 GiB is set Command (m for help): n Partition type: p primary (2 primary, 0 extended, 2 free) e extended Select (default p): p Partition number (3,4, default 3): First sector (4196352-41943039, default 4196352): Using default value 4196352 Last sector, +sectors or +size{K,M,G} (4196352-41943039, default 41943039): +1G Partition 3 of type Linux and of size 1 GiB is set Command (m for help): p Disk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xb478de23 Device Boot Start End Blocks Id System /dev/sdc1 2048 2099199 1048576 83 Linux /dev/sdc2 2099200 4196351 1048576 83 Linux /dev/sdc3 4196352 6293503 1048576 83 Linux Command (m for help): n Partition type: p primary (3 primary, 0 extended, 1 free) e extended Select (default e): e Selected partition 4 First sector (6293504-41943039, default 6293504): Using default value 6293504 Last sector, +sectors or +size{K,M,G} (6293504-41943039, default 41943039): Using default value 41943039 Partition 4 of type Extended and of size 17 GiB is set Command (m for help): p Disk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xb478de23 Device Boot Start End Blocks Id System /dev/sdc1 2048 2099199 1048576 83 Linux /dev/sdc2 2099200 4196351 1048576 83 Linux /dev/sdc3 4196352 6293503 1048576 83 Linux /dev/sdc4 6293504 41943039 17824768 5 Extended Command (m for help): n All primary partitions are in use Adding logical partition 5 First sector (6295552-41943039, default 6295552): Using default value 6295552 Last sector, +sectors or +size{K,M,G} (6295552-41943039, default 41943039): +1G Partition 5 of type Linux and of size 1 GiB is set Command (m for help): p Disk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xb478de23 Device Boot Start End Blocks Id System /dev/sdc1 2048 2099199 1048576 83 Linux /dev/sdc2 2099200 4196351 1048576 83 Linux /dev/sdc3 4196352 6293503 1048576 83 Linux /dev/sdc4 6293504 41943039 17824768 5 Extended /dev/sdc5 6295552 8392703 1048576 83 Linux Command (m for help): ###说明： #### 有了扩展分区才能逻辑分区，扩展分区不能直接使用，只能在逻辑分区种才能使用 第四个里程: 保存退出,让系统可以加载识别分区信息 #输入w,保存退出 Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks. #让系统可以加载识别分区文件 [root@web01 ~]# partprobe /dev/sdc 第五个里程：格式化磁盘 [root@web01 ~]# partprobe /dev/sdc # ext3/4 centos6 # xfs centos7 格式效率较高 数据存储效率提升(数据库服务器) [root@web01 ~]# mkfs -t xfs /dev/sdc1 meta-data=/dev/sdc1 isize=512 agcount=4, agsize=65536 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=262144, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@web01 ~]# mkfs.xfs /dev/sdc2 meta-data=/dev/sdc2 isize=512 agcount=4, agsize=65536 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=262144, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 2、磁盘分区实践--磁盘大于2T 第一个里程: 准备磁盘环境 虚拟主机中添加一块3T硬盘 第二个里程: 使用parted命令进行分区 帮助说明 mklabel,mktable LABEL-TYPE create a new disklabel (partition table) 创建一个分区表 (默认为mbr) print [devices|free|list,all|NUMBER] display the partition table, available devices, free space, all found partitions, or a particular partition 显示分区信息 mkpart PART-TYPE [FS-TYPE] START END make a partition 创建一个分区 quit exit program 退出分区状态 rm NUMBER delete partition NUMBER 删除分区 [root@web01 ~]# parted /dev/sdd GNU Parted 3.1 Using /dev/sdd Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) mklabel gpt (parted) print Model: VMware, VMware Virtual S (scsi) Disk /dev/sdd: 3221GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags (parted) mkpart primary 0 2100G Warning: The resulting partition is not properly aligned for best performance. Ignore/Cancel? Ignore (parted) mkpart primary 2100 2200G Warning: You requested a partition from 2100MB to 2200GB (sectors 4101562..4296875000). The closest location we can manage is 2100GB to 2200GB (sectors 4101562501..4296875000). Is this still acceptable to you? Yes/No? yes Warning: The resulting partition is not properly aligned for best performance. Ignore/Cancel? Ignore #查看分区 (parted) print Model: VMware, VMware Virtual S (scsi) Disk /dev/sdd: 3221GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 17.4kB 2100GB 2100GB primary 2 2100GB 2200GB 100GB primary #删除第二个分区 (parted) rm 2 (parted) print Model: VMware, VMware Virtual S (scsi) Disk /dev/sdd: 3221GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 17.4kB 2100GB 2100GB primary (parted) mkpart primary 2100 2200G Warning: You requested a partition from 2100MB to 2200GB (sectors 4101562..4296875000). The closest location we can manage is 2100GB to 2200GB (sectors 4101562501..4296875000). Is this still acceptable to you? Yes/No? yes Warning: The resulting partition is not properly aligned for best performance. Ignore/Cancel? ingnore parted: invalid token: ingnore Ignore/Cancel? Ignore #退出分区模式 (parted) quit Information: You may need to update /etc/fstab. 第三个里程: 加载磁盘分区 [root@web01 ~]# partprobe /dev/sdd 第四个里程:格式化分区 [root@web01 ~]# mkfs.xfs /dev/sdd1 meta-data=/dev/sdd1 isize=512 agcount=4, agsize=128173827 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=512695308, imaxpct=5 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=250339, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@web01 ~]# 3、挂载分区 3.1 手动挂载 #创建挂在目录 [root@web01 ~]# mkdir /mount01 [root@web01 ~]# mount /dev/sdc1 /mount01 #查看挂载结果 [root@web01 ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos_wjh-root 19G 2.2G 17G 12% / devtmpfs 476M 0 476M 0% /dev tmpfs 488M 0 488M 0% /dev/shm tmpfs 488M 7.7M 480M 2% /run tmpfs 488M 0 488M 0% /sys/fs/cgroup /dev/sda1 197M 108M 90M 55% /boot tmpfs 98M 0 98M 0% /run/user/0 /dev/sdc1 1014M 33M 982M 4% /mount01 3.2 开机自动挂载 方法一: 将挂载命令放入/etc/rc.local [root@web01 ~]# vim /etc/rc.local [root@web01 ~]# tail -2 /etc/rc.local touch /var/lock/subsys/local mount /dev/sdc1 /mount01 [root@web01 ~]# chmod +x /etc/rc.d/rc.local 方法二: 在/etc/fstab文件中进行设置 #查看uuid [root@web01 ~]# blkid /dev/sda1: UUID=\"ef56a16b-6ffe-4ee9-84bc-54519c404628\" TYPE=\"xfs\" /dev/sda2: UUID=\"9mzWRG-c76T-l0GG-nMAm-KjJ0-vA3V-8s1UcP\" TYPE=\"LVM2_member\" /dev/sdc1: UUID=\"719b1119-bc16-421f-9039-032fc874e302\" TYPE=\"xfs\" /dev/sdc2: UUID=\"895dac6f-5864-4f0d-9a58-0ed43bf690a8\" TYPE=\"xfs\" /dev/mapper/centos_wjh-root: UUID=\"c570790b-11f1-4237-835a-06115e3b4890\" TYPE=\"xfs\" /dev/mapper/centos_wjh-swap: UUID=\"130c3eaf-c634-4f5b-8cf2-21d48c3956d4\" TYPE=\"swap\" /dev/sdd1: UUID=\"bcc9ed95-532b-4c9e-a697-9d66bae6a3c8\" TYPE=\"xfs\" PARTLABEL=\"primary\" PARTUUID=\"4e66de18-0674-4b4a-b784-d93332dbf466\" /dev/sdd2: PARTLABEL=\"primary\" PARTUUID=\"3b280fe8-5d0a-414a-aafc-2772ecffb2e0\" ##使用uuid或者直接行磁盘路径 [root@web01 ~]# tail -2 /etc/fstab #/dev/sdd1 /mount2 xfs defaults 0 0 UUID=bcc9ed95-532b-4c9e-a697-9d66bae6a3c /mount2 xfs defaults 0 0 4、企业磁盘常见问题: 1) 磁盘满的情况 No space left on device a)存储的数据过多了 block存储空间不足了 解决方式: a 删除没用的数据 b 找出大的没用的数据 find / -type f -size +xxx du -sh /etc/sysconfig/network-scripts/*|sort -h (按照数值排序命令) b) 存储的数据过多了 inode存储空间不足了: 出现了大量小文件 df -i 查看inode 解决方式: 删除大量的没用的小文件 5、swap分区调整 第一步： 将磁盘分出一部分空间给swap分区使用 [root@web01 ~]# dd if=/dev/zero of=/tmp/1G bs=100M count=10 第二步： 将指定磁盘空间作为swap空间使用 [root@web01 ~]# mkswap /tmp/1G Setting up swapspace version 1, size = 1023996 KiB no label, UUID=9a9aed5d-aade-41ba-8a1a-6f67275c2873 第三步： 加载使用swap空间 [root@web01 ~]# swapon /tmp/1G swapon: /tmp/1G: insecure permissions 0644, 0600 suggested. [root@web01 ~]# free -h total used free shared buff/cache available Mem: 974M 119M 164M 25M 691M 662M Swap: 2.0G 0B 2.0G ## swap足够时，释放资源 [root@web01 ~]# swapoff /tmp/1G [root@web01 ~]# free -h total used free shared buff/cache available Mem: 974M 118M 164M 25M 691M 663M Swap: 1.0G 0B 1.0G [root@web01 ~]# rm /tmp/1G -f [root@web01 ~]# ` Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/rsync.html":{"url":"linux/rsync.html","title":"RSYNC简介","keywords":"","body":"什么是rsync服务 ​ Rsync是一款开源的、快速的、多功能的、可实现全量及增量的本地或远程数据同步备份的优秀工具 rsync命令语法格式 SYNOPSIS Local: rsync [OPTION...] SRC... [DEST] 本地备份数据: src: 要备份的数据信息 dest: 备份到什么路径中 远程备份数据: Access via remote shell: 拉取数据 Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST] [USER@] --- 以什么用户身份拉取数据(默认以当前用户) hosts --- 指定远程主机IP地址或者主机名称 SRC --- 要拉取的数据信息 dest --- 保存到本地的路径信息 EG: rsync -rp wjh@172.06.1.31:/etc/hosts /backup 使用的用户必须是对端服务器上存在,推荐使用root用户 推送数据 Push: rsync [OPTION...] SRC... [USER@]HOST:DEST SRC --- 本地要进行远程传输备份的数据 [USER@] --- 以什么用户身份推送数据(默认以当前用户) hosts --- 指定远程主机IP地址或者主机名称 dest --- 保存到远程的路径信息 守护进程方式备份数据 备份服务 可以进行一些配置管理 可以进行安全策略管理 可以实现自动传输备份数据 Access via rsync daemon: Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST::DEST rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST rsync软件使用方法: ​ rsync命令 可以替代cp，scp，rm，ls命令 本地备份数据 使用cp进行备份 [root@nfs01 backup]# cp /etc/hosts /tmp [root@nfs01 backup]# ll /tmp/hosts -rw-r--r-- 1 root root 371 May 6 16:11 /tmp/hosts ### 使用rsync代替cp进行本地文件备份 [root@nfs01 backup]# rsync /etc/hosts /tmp/host_rsync [root@nfs01 backup]# ll /tmp/host_rsync -rw-r--r-- 1 root root 371 May 6 16:12 /tmp/host_rsync 远程备份数据 scp 使用scp进行备份 scp -rp /etc/hosts root@172.16.1.41:/backup root@172.16.1.41's password: hosts 100% 371 42.8KB/s 00:00 -r --- 递归复制传输数据 -p --- 保持文件属性信息不变 使用rsync进行备份 [root@nfs01 ~]# rsync -rp /etc/hosts 172.16.1.41:/backup/hosts_rsync root@172.16.1.41's password: rsync远程备份目录: [root@nfs01 ~]# rsync -rp /wjh 172.16.1.41:/backup --- 备份的目录后面没有 / root@172.16.1.41's password: [root@backup ~]# ll /backup total 0 drwxr-xr-x 2 root root 48 May 6 16:22 wjh [root@backup ~]# tree /backup/ /backup/ └── wjh ├── 01.txt ├── 02.txt └── 03.txt 1 directory, 3 files [root@nfs01 ~]# rsync -rp /wjh/ 172.16.1.41:/backup --- 备份的目录后面有 / root@172.16.1.41's password: [root@backup ~]# ll /backup total 0 -rw-r--r-- 1 root root 0 May 6 16:24 01.txt -rw-r--r-- 1 root root 0 May 6 16:24 02.txt -rw-r--r-- 1 root root 0 May 6 16:24 03.txt 总结: 在使用rsync备份目录时: 备份目录后面有 / -- /wjh/ : 只将目录下面的内容进行备份传输 备份目录后面没有/ -- /wjh : 会将目录本身以及下面的内容进行传输备份 ​ 替代rm删除命令 rsync -rp --delete /null/ 172.16.1.41:/backup root@172.16.1.41's password: #--delete 实现无差异同步数据 面试题: 有一个存储数据信息的目录, 目录中数据存储了50G数据, 如何将目录中的数据快速删除 使用，rm /目录/* -rf删除会耗时很久，使用rsync可以节省时间 EG: rsync -rp --delete /null/ 172.16.1.41:/backup 替代查看文件命令 ls 使用ls查看文件列表 [root@backup ~]# ls /etc/hosts /etc/hosts 使用rsync查看文件列表 [root@backup ~]# rsync /etc/hosts -rw-r--r-- 371 2019/05/06 11:55:22 hosts Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/rsync_install.html":{"url":"linux/rsync_install.html","title":"RSYNC安装与使用","keywords":"","body":"RSYNC安装与使用 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/NFS.html":{"url":"linux/NFS.html","title":"NFS使用","keywords":"","body":"NFS存储服务概念介绍 NFS是Network File System的缩写,中文意思是网络文件共享系统，它的主要功能是通过网络（一般是局域网）让不同的主机系统之间可以共享文件或目录 ​ 存储服务的种类,用于中小型企业: 实现数据共享存储 NFS存储服务作用 ​ (1) 实现数据的共享存储 ​ (2) 编写数据操作管理 ​ (3) 节省购买服务器磁盘开销 淘宝--上万 用电开销 NFS服务部署流程 第一个历程: 下载安装软件 rpm -qa|grep -E \"nfs|rpc\" yum install -y nfs-utils rpcbind 第二个历程: 编写nfs服务配置文件 echo \"/data 172.16.1.0/24(rw,sync)\" >> /etc/exports 参数说明： 01: 设置数据存储的目录 /data 02: 设置网络一个白名单 (允许哪些主机连接到存储服务器进行数据存储) 03: 配置存储目录的权限信息 存储目录一些功能 第三个历程: 创建一个存储目录 mkdir /data chown nfsnobody.nfsnobody /data ​ ps:必须修改权限，不修改权限，客户端挂在后，创建文件时将会报错 (tocuh :cannot touch 'xxxxx' :Permission denied) ### 第四个历程: 启动服务程序 #先启动 rpc服务 systemctl start rpcbind.service systemctl enable rpcbind.service # 再启动 nfs服务 systemctl start nfs systemctl enable nfs 客户端部署 第一个历程: 安装nfs服务软件 ​ ps:必须安装此服务，如果不安装，则在关在的时候无法识别磁盘格式，将会报错 yum install -y nfs-utils 第二个历程查看挂载点 #执行以下命令检查 nfs 服务器端是否有设置共享目录 # showmount -e $(nfs服务器的IP) showmount -e 172.31.0.201 第三个历程: 查看挂载点 mount -t nfs 172.31.0.201:/data /mnt Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"linux/Disk_partition_free.html":{"url":"linux/Disk_partition_free.html","title":"磁盘空间未完全分配场景文件系统扩容处理","keywords":"","body":"磁盘空间未完全分配场景文件系统扩容处理 一、场景分析 系统安装好后，磁盘空间不足，虚拟机动态扩容磁盘空间，默认情况sda1-sda2为主分区,使用lsblk 查看当前情况。 得出结果 磁盘名称为 sda，总大小70G，空间已分配 60G 左右，还剩 10G 多空间未分配 sda2下存在 两 个卷组 vg：centos_master-root，centos_master-swap此卷组有2个逻辑卷lv 逻辑卷 lv：root 和 swap，对应的挂载点/和/swap 二、磁盘剩余空间创建分区 请忽略上午图中 vda 和下文 sda 名称的差异，一切以lsblk 扫描出来的结果为准 1、 sda1~sda4 未全部分配可用 fdisk 来完成分区 在系统中检查是否识别到了新的硬盘 [root@master ~]# fdisk -l Disk /dev/sda: 75.2 GB, 75161927680 bytes, 146800640 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000d0ec1 Device Boot Start End Blocks Id System /dev/sda1 * 2048 411647 204800 83 Linux /dev/sda2 411648 41943039 20765696 8e Linux LVM /dev/sda3 41943040 125829119 41943040 8e Linux LVM Disk /dev/mapper/centos_master-root: 63.1 GB, 63132663808 bytes, 123305984 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos_master-swap: 4 MB, 4194304 bytes, 8192 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 对磁盘进行分区处理(fdisk-- 进行分区处理 查看分区信息) [root@master ~]# fdisk /dev/sda Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): n Partition type: p primary (3 primary, 0 extended, 1 free) e extended Select (default e): p Selected partition 4 First sector (125829120-146800639, default 125829120): Using default value 125829120 Last sector, +sectors or +size{K,M,G} (125829120-146800639, default 146800639): Using default value 146800639 Partition 4 of type Linux and of size 10 GiB is set Command (m for help): t Partition number (1-4, default 4): Hex code (type L to list all codes): L 0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden C: c6 DRDOS/sec (FAT- 4 FAT16 ③查看分区是否完成分配。 格式化磁盘 cat /proc/partitions mkfs.ext3 /dev/sda4 ​ 下一步，创建物理卷和分区： [root@master ~]# pvcreate /dev/sda4 WARNING: ext3 signature detected on /dev/sda4 at offset 1080. Wipe it? [y/n]: y Wiping ext3 signature on /dev/sda4. Physical volume \"/dev/sda4\" successfully created. [root@master ~]# vgextend centos_master /dev/sda4 Volume group \"centos_master\" successfully extended 我们查看物理卷情况，知道了可以增加的硬盘空间容量总量。 [root@master ~]# vgdisplay --- Volume group --- VG Name centos_master System ID Format lvm2 Metadata Areas 3 Metadata Sequence No 7 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 3 Act PV 3 VG Size 69.79 GiB PE Size 4.00 MiB Total PE 17867 Alloc PE / Size 15053 / 58.80 GiB Free PE / Size 2814 / 10.99 GiB VG UUID xqPaU2-S9On-Pfrv-Zg9Q-bBrf-nmxD-8GF4Lj 按照实际大小酌情增加，比如我现在需增加10.99 GiB [root@master ~]# lvresize -L +10G /dev/mapper/centos_master-root Size of logical volume centos_master/root changed from 然后动态扩容分区的大小 [root@master ~]# xfs_growfs /dev/mapper/centos_master-root meta-data=/dev/mapper/centos_master-root isize=512 agcount=12, agsize=1297408 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=15413248, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 15413248 to 18034688 最后，查看最终的结果 [root@master ~]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 2.0G 0 2.0G 0% /dev tmpfs 2.0G 0 2.0G 0% /dev/shm tmpfs 2.0G 12M 2.0G 1% /run tmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup /dev/mapper/centos_master-root 69G 2.7G 67G 4% / /dev/sda1 197M 116M 82M 59% /boot tmpfs 394M 0 394M 0% /run/user/0 2、sda1~sda4 全部分配可用cfdisk 来完成分区 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"nginx/install.html":{"url":"nginx/install.html","title":"安装","keywords":"","body":"安装 二进制编译安装 1、下载安装包 [root@wjh ~]# mkdir -p /data/soft [root@wjh ~]# cd /data/soft/ [root@wjh soft]# wget http://nginx.org/download/nginx-1.16.0.tar.gz [root@wjh soft]# tar -zxvf nginx-1.16.0.tar.gz [root@wjh soft]# cd nginx-1.16.0/ 2、解决依赖问题 [root@wjh nginx-1.16.0]# yum -y install openssl-devel pcre-devel 3、指定安装的路径，安装的模块 # --prefix=PATH set installation prefix （指定程序安装路径） # --user=USER set non-privileged user for worker processes（设置一个虚拟用户管理worker进程(安全)） # --group=GROUP set non-privileged group for worker processes(设置一个虚拟用户组管理worker进程(安全)) # --http-log-path=PATH set http access log pathname(日志路径) # --error-log-path= c错误日志路径 # [root@wjh nginx-1.16.0]# ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --with-pcre-jit --with-http_ssl_module --with-http_v2_module --with-http_sub_module --with-stream --with-stream_ssl_module 4、编译安装 [root@wjh nginx-1.16.0]# make & make install 5、创建启动文件 #1.在系统服务目录里创建nginx.service文件 [root@wjh nginx-1.16.0]# cat >/lib/systemd/system/nginx.service 6、启动程序 [root@wjh ~]# systemctl daemon-reload [root@wjh ~]# systemctl start nginx [root@wjh ~]# systemctl status nginx ● nginx.service - nginx Loaded: loaded (/usr/lib/systemd/system/nginx.service; disabled; vendor preset: disabled) Active: active (running) since Fri 2021-05-14 10:07:34 CST; 7s ago Process: 4871 ExecStart=/usr/local/nginx/sbin/nginx (code=exited, status=0/SUCCESS) Main PID: 4872 (nginx) CGroup: /system.slice/nginx.service ├─4872 nginx: master process /usr/local/nginx/sbin/nginx └─4873 nginx: worker process May 14 10:07:34 wjh systemd[1]: Starting nginx... May 14 10:07:34 wjh systemd[1]: Started nginx. [root@wjh ~]# systemctl enable nginx Created symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service. [root@wjh ~]# 在线安装 1、更新nginx官方yum源 cat > /etc/yum.repos.d/nginx.repo 2、yum安装 [root@wjh ~]#yum install -y nginx 3、启动 [root@wjh ~]# systemctl start nginx [root@wjh ~]# systemctl enable nginx 配置文件说明 第一个部分: 配置文件主区域配置 user www; --- 定义worker进程管理的用户 补充: nginx的进程 master process: 主进程 ---管理服务是否能够正常运行 boss worker process: 工作进程 ---处理用户的访问请求 员工 worker_processes 2; ---定义有几个worker进程 == CPU核数 / 核数的2倍 error_log /var/log/nginx/error.log warn; --- 定义错误日志路径信息 pid /var/run/nginx.pid; --- 定义pid文件路径信息 第二个部分: 配置文件事件区域 events { worker_connections 1024; --- 一个worker进程可以同时接收1024访问请求 } 第三个部分: 配置http区域 http { include /etc/nginx/mime.types; --- 加载一个配置文件 default_type application/octet-stream; --- 指定默认识别文件类型 log_format oldboy '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; --- 定义日志的格式 access_log /var/log/nginx/access.log oldboy; --- 指定日志路径 keepalive_timeout 65; --- 超时时间 #gzip on; include /etc/nginx/conf.d/*.conf; --- 加载一个配置文件 } /etc/nginx/nginx.d/default --- 扩展配置(虚拟主机配置文件) 第四个部分: server区域信息(配置一个网站 www/bbs/blog -- 一个虚拟主机) server { listen 8080; --- 指定监听的端口 server_name www.oldboy.com; --- 指定网站域名 root /usr/share/nginx/html; --- 定义站点目录的位置 index index.html index.htm; --- 定义首页文件 error_page 500 502 503 504 /50x.html; --- 优雅显示页面信息 location = /50x.html { root /usr/share/nginx/html; } } Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"nginx/virtual_host.html":{"url":"nginx/virtual_host.html","title":"虚拟主机","keywords":"","body":"虚拟主机 1) 利用nginx服务搭建网站文件共享服务器 [root@wjh conf.d]# cat file.conf server { listen 80; server_name www.testk.com; location / { root /data; #配置账户 # auth_basic \"wjhtest-sz-01\"; #配置密码 # auth_basic_user_file password/htpasswd; autoindex on; # --- 修改目录结构中出现的中文乱码问题 charset utf-8; } } # 说明： # 1. 需要将首页文件进行删除 # 2. mime.types媒体资源类型文件作用 # 文件中有的扩展名信息资源, 进行访问时会直接看到数据信息 # 文件中没有的扩展名信息资源, 进行访问时会直接下载资源 2) 利用nginx服务搭建网站 server { listen 80; server_name www.testk.com; root /data/www/keep_com; index index.html; error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } #根据路径指定访问目录 location /dev { root /data; #指定ip端可以访问 deny ip； allow ip; } } 3）https配置 server { listen 443; server_name https.test.com; ssl om; # --指定srt的目录信息 ssl_certificate ssl_key/server.crt; # ----指定key的目录 ssl_certificate_key ssl_key/server.key; location / { root /code/https; index index.html; } } server { listen 80; server_named rewrite .* https://$server_name$request_rui redirect; } #PS：负载均衡配置https时，只需要再LB的机器配置https，运行程序的主机不需要配置htps Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"nginx/lnmp.html":{"url":"nginx/lnmp.html","title":"lnmp","keywords":"","body":"lnmp配置 网站的LNMP架构是什么 L --- linux系统 注意: a selinux必须关闭 防火墙关闭 b /tmp 1777 mysql服务无法启动 N --- nginx服务部署 作用: 处理用户的静态请求 html jpg txt mp4/avi P --- php服务部署 作用: 1. 处理动态的页面请求 2. 负责和数据库建立关系 M --- mysql服务部署 (yum会很慢 编译安装会报错) mariadb 作用: 存储用户的字符串数据信息 nginx安装：nginx安装 mysql安装配置：mysql安装 php安装配置 #安装路径：/usr/local/php # 先安装如下依赖包 [root@wjh mysql]# wget https://www.php.net/distributions/php-7.3.28.tar.gz [root@wjh mysql]# yum install -y gcc gcc-c++ make zlib zlib-devel pcre pcre-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers #解压文件 [root@wjh php-7.2.0]# tar -zxvf php-7.2.0.tar.gz [root@wjh tools]# cd php-7.2.0/ #指定安装目录，指定安装模块 [root@wjh tools]# ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php --enable-mbstring --with-openssl --enable-ftp --with-gd --with-jpeg-dir=/usr --with-png-dir=/usr --with-mysql=mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-pear --enable-sockets --with-freetype-dir=/usr --with-zlib --with-libxml-dir=/usr --with-xmlrpc --enable-zip --enable-fpm --enable-xml --enable-sockets --with-gd --with-zlib --with-iconv --enable-zip --with-freetype-dir=/usr/lib/ --enable-soap --enable-pcntl --enable-cli --with-curl |# 编译完成之后，执行安装命令： [root@wjh tools]# make && make install Wrote PEAR system config file at: /usr/local/php/etc/pear.conf You may want to add: /usr/local/php/lib/php to your php.ini include_path /server/tools/php-7.2.0/build/shtool install -c ext/phar/phar.phar /usr/local/php/bin ln -s -f phar.phar /usr/local/php/bin/phar Installing PDO headers: /usr/local/php/include/php/ext/pdo/ 【配置PHP】 #在之前编译的源码包中，找到 php.ini-production，复制到/usr/local/php下，并改名为php.ini： #[可选项] 设置让PHP错误信息打印在页面上 [root@wjh php-7.2.0]# cp php.ini-production /usr/local/php/php.ini [root@wjh php-7.2.0]# vim /usr/local/php/php.ini +477 [root@wjh php-7.2.0]# sed -i '477cdisplay_errors = On' /usr/local/php/php.ini|grep display_er #复制启动脚本： [root@wjh php-7.2.0]# cp ./sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm [root@wjh php-7.2.0]# chmod +x /etc/init.d/php-fpm # 修改php-fpm配置文件： #1、去掉 pid = run/php-fpm.pid 前面的分号 [root@wjh php-7.2.0]# cd /usr/local/php/etc [root@wjh etc]# cp php-fpm.conf.default php-fpm.conf [root@wjh etc]# sed '17cpid = run/php-fpm.pid' php-fpm.conf #2、修改user和group的用户为当前用户(也可以不改，默认会添加nobody这个用户和用户组) [root@wjh etc]# [root@wjh php-fpm.d]# cp www.conf.default www.conf [root@wjh php-fpm.d]# vim www.conf [root@wjh php-fpm.d]# sed -i 's#user = nobody#user = nginx#g' www.conf [root@wjh php-fpm.d]# sed -i 's#group = nobody#group = nginx#g' www.conf 【启动PHP】 $ /etc/init.d/php-fpm start #php-fpm启动命令 $ /etc/init.d/php-fpm stop #php-fpm停止命令 $ /etc/init.d/php-fpm restart #php-fpm重启命令 $ ps -ef | grep php 或者 ps -A | grep -i php #查看是否已经成功启动PHP 【PHP7.2的MySQL扩展】 #解压，并进入目录： [root@wjh tools]# cd mysql-24d32a0/ [root@wjh mysql-24d32a0]# /usr/local/php/bin/phpiz -bash: /usr/local/php/bin/phpiz: No such file or directory [root@wjh mysql-24d32a0]# /usr/local/php/bin/phpize Configuring for: PHP Api Version: 20170718 Zend Module Api No: 20170718 Zend Extension Api No: 320170718 #编译mysql扩展，使用mysql native driver 作为mysql链接库 [root@wjh mysql-24d32a0]# ./configure --with-php-config=/usr/local/php/bin/php-config --with-mysql=mysqlnd [root@wjh mysql-24d32a0]# make && make install Dont forget to run 'make test'. Installing shared extensions: /usr/local/php/lib/php/extensions/no-debug-non-zts-20170718/ # 最后，编辑php.ini文件，在最后面加入 extension=mysql.so [root@wjh mysql-24d32a0]# sed -i '$aextension=mysql.so' /usr/local/php/php.ini 报错解决 # 错误：virtual memory exhausted: Cannot allocate memory #问题原因：由于物理内存本身很小，且阿里云服务器并没有分配swap空间，当物理内存不够用时， 3物理内存中暂时不用的内容没地方转存。 #解决方法：手动分配一个swap空间 #创建一个大小为1G的文件/swap dd if=/dev/zero of=/swap bs=1024 count=1M #将/swap作为swap空间 mkswap /swap #enable /swap file for paging and swapping swapon /swap #Enable swap on boot, 开机后自动生效 echo \"/swap swap swap sw 0 0\" >> /etc/fstab 好文要顶 关注我 收藏该文 nginx添加php编译 server { listen 80; server_name localhost; root /data/www; index index.php index.html index.htm; error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location ~ \\.php$ { fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /$document_root$fastcgi_script_name; fastcgi_pass 127.0.0.1:9000; include fastcgi_params; } } Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"nginx/load_balanced.html":{"url":"nginx/load_balanced.html","title":"负载均衡","keywords":"","body":"负载均衡 (反向代理)负载均衡的概念说明 什么是集群? 完成相同任务或工作的一组服务器 (web01 web02 web03 -- web集群) 什么是负载均衡? 1) 实现用户访问请求进行调度分配 2) 实现用户访问压力分担 什么是反向代理? 反向代理: 外网 ---> (eth0外网) 代理服务器 (eth1内网) ---> 公司网站服务器web(内网) 外网用户(客户端) --- 代理服务器 (服务端) 代理服务器(客户端) --- web服务器(服务端) 正向代理: 内网(局域网主机) --- (内网)代理服务器(外网) --- 互联网 --- web服务器(日本) 翻墙的操作 准备负载均衡环境 01. 先部署好一台LNMP服务器,上传代码信息 02. 进行访问测试 03. 批量部署多台web服务器 04. 将nginx配置文件进行分发 05. 将站点目录分发给所有主机 ngingx安装：安装 负载配置（虚拟主机配置） 1. 轮询分配请求(平均) # ngx_http_upstream_module --- upstream 负载均衡 # ngx_http_proxy_module --- proxy_pass 反向代理 upstream wjhtest { server 10.0.0.7:80; server 10.0.0.8:80; server 10.0.0.9:80; } server { listen 80; server_name www.wjhtest.com; location / { proxy_pass http://wjhtest; } } 2. 权重分配请求(能力越强责任越重) upstream wjhtest { server 10.0.0.7:80 weight=3; server 10.0.0.8:80 weight=2; server 10.0.0.9:80 weight=1; } server { } 3. 实现热备功能(备胎功能) #当所有的主机都停止服务的时候才生效 upstream wjhtest { server 10.0.0.7:80; server 10.0.0.8:80; server 10.0.0.9:80 backup; } 4. 定义最大失败次数（健康检查参数） upstream wjhtest { server 10.0.0.7:80 weight=3 max_fails=5; server 10.0.0.8:80 weight=2 max_fails=5; server 10.0.0.9:80 backup; } 5. 定义失败之后重发的间隔时间 # fail_timeout=10s 会给失败的服务器一次机会 upstream wjhtest { server 10.0.0.7:80 weight=3 max_fails=5 fail_timeout=10s ; server 10.0.0.8:80 weight=2 max_fails=5 fail_timeout=10s ; server 10.0.0.9:80 backup; } 分发配置 # scp /usr/local/nginx/conf.d/weblb.conf 实现不同调度算法 1. rr 轮询调度算法 2. wrr 权重调度算法 3. ip_hash 算法 (出现反复登录的时候) 4. least_conn 根据服务器连接数分配资源 ip_hash 算法 upstream wjhtest { ip_hash; server 10.0.0.7:80 weight=3 max_fails=5 fail_timeout=10s ; server 10.0.0.8:80 weight=2 max_fails=5 fail_timeout=10s ; server 10.0.0.9:80 backup; } least_conn # #假如上一个请求选择了第二台10.0.0.8，下一个请求到来，通过比较剩下可用的server的conns/weight值来决定选哪一台。 #如果10.0.0.7连接数为100，10.0.0.9连接数为80，因为权重分别是2和1，因此计算结果 # 100/2=50, 80/1 =80。因为 50 负载均衡企业实践应用 需求一，根据用户访问的uri信息进行负载均衡 1、环境配置 # 10.0.0.8:80 上进行环境部署: [root@web02 ~]# mkdir /html/www/upload [root@web02 ~]# echo \"upload-web集群_10.0.0.8\" >/html/www/upload/wjhtest.html # 10.0.0.7上进行环境部署: [root@wjhtest01 html]# mkdir /html/www/static [root@wjhtest01 html]# echo static-web集群_10.0.0.7 >/html/www/static/wjhtest.html # 10.0.0.9:80上进行环境部署: echo \"default-web集群_10.0.0.9\" >/html/www/wjhtest.html 2、编写负载均衡配置文件 upstream upload { server 10.0.0.8:80; } upstream static { server 10.0.0.7:80; } upstream default { server 10.0.0.9:80; } server { listen 80; server_name www.wjhtest.com; location / { proxy_pass http://default; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; proxy_next_upstream error timeout http_404 http_502 http_403; } location /upload { proxy_pass http://upload; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; proxy_next_upstream error timeout http_404 http_502 http_403; } location /static { proxy_pass http://static; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; proxy_next_upstream error timeout http_404 http_502 http_403; } } 总结: 实现网站集群动静分离 01. 提高网站服务安全性 02. 管理操作工作简化 03. 可以换分不同人员管理不同集群服务器 需求二，根据用户访问的终端信息显示不同页面 第一步: 准备架构环境 iphone www.wjhtest.com --- iphone_access 10.0.0.7:80 mobile移动端集群 谷歌 www.wjhtest.com --- google_access 10.0.0.8:80 web端集群 IE 360 www.wjhtest.com --- default_access 10.0.0.9:80 default端集群 web01: echo \"iphone_access 10.0.0.7\" >/html/www/wjhtest.html web02: echo \"google_access 10.0.0.8\" >/html/www/wjhtest.html web03: echo \"default_access 10.0.0.9\" >/html/www/wjhtest.html 第二步：编写负载均衡配置文件 [root@lb01 conf.d]# cat lb.conf upstream web { server 10.0.0.8:80; } upstream mobile { server 10.0.0.7:80; } upstream default { server 10.0.0.9:80; } server { listen 80; server_name www.wjhtest.com; location / { if ($http_user_agent ~* iphone) { proxy_pass http://mobile; } if ($http_user_agent ~* Chrome) { proxy_pass http://web; } proxy_pass http://default; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; proxy_next_upstream error timeout http_404 http_502 http_403; } } Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"nginx/keepalived_nginx.html":{"url":"nginx/keepalived_nginx.html","title":"keepalieved配置nginx高可用冗余","keywords":"","body":"keepalieved配置nginx高可用冗余 keepalived编译安装 编译安装keepalived #安装依赖 [root@wjh soft]# yum install curl gcc openssl-devel libnl3-devel net-snmp-devel libnfnetlink-devel -y # 下载软件 [root@wjh soft]# wget https://www.keepalived.org/software/keepalived-2.2.2.tar.gz #解压文件 [root@wjh soft]# tar -zxvf keepalived-2.2.2.tar.gz [root@wjh soft]# cd keepalived-2.2.2/ # 编译，指定安装路径位/usr/local/keepalived ./configure --with-init=systemd --with-systemdsystemunitdir=/usr/lib/systemd/system --prefix=/usr/local/keepalived --with-run-dir=/usr/local/keepalived/run # 安装 [root@wjh keepalived-2.2.2]# make [root@wjh keepalived-2.2.2]# make install # 可执行文件拷贝一份到系统执行文件目录，该目录在path变量里面，可以直接使用keepalived命令 cp /usr/local/keepalived/sbin/keepalived /usr/sbin/keepalived # 或者 [root@wjh keepalived-2.2.2]# ln -s /usr/local/keepalived/sbin/keepalived /usr/sbin/keepalived # keepalived附加参数文件，为了跟yum安装一致，其实是不用配置的。启动文件指定实际路径就可以了。 [root@wjh keepalived-2.2.2]# ln -s /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/keepalived # pid文件放置目录,目录可以自己定义在启动脚本里面使用 mkdir /usr/local/keepalived/run 配置system自启动文件 cat > /usr/lib/systemd/system/keepalived.service 从节点与上述操作一致 master节点配置 cat > /usr/local/keepalived/etc/keepalived/keepalived.conf backuup节点 cat > /usr/local/keepalived/etc/keepalived/keepalived.conf 监控脚本 cat >/usr/local/keepalived/etc/keepalived/chk_nginx.sh nginx配置 upstream wjhtest { server 10.0.0.7:80; server 10.0.0.8:80; server 10.0.0.9:80; } server { listen 10.0.0.3:80; server_name www.wjhtest.com; location / { proxy_pass http://wjhtest; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; proxy_next_upstream error timeout http_404 http_502 http_403; } } Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"nginx/rsync.html":{"url":"nginx/rsync.html","title":"rsync备份服务","keywords":"","body":"1. 什么是rsync服务 Rsync是一款开源的、快速的、多功能的、可实现全量及增量的本地或远程数据同步备份的优秀工具 2、rsync守护进程部署方式 rsync守护进程服务端配置: Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"mysql/install.html":{"url":"mysql/install.html","title":"安装","keywords":"","body":"安装 1、准备工作 # 创建保存安装包的文件夹 [root@wjh ~]# mkdir -p /server/tools # 上传软件数据包 # 创建保存数据库运行程序的目录 [root@wjh ~]# mkdir -p /application # 解压文件 [root@wjh ~]# tar -xf mysql-5.7.26-linux-glibc2.12-x86_64.tar.gz -C /appliacation/ 2、卸载系统自带的mariadb [root@wjh ~]# yum -y remove mariadb 3、创建用户 [root@wjh ~]# useradd -s /sbin/nologin mysql 4、配置环境变量 [root@db01 ~]# vim /etc/profile #文件末尾添加 export PATH=/application/mysql/bin:$PATH # 配置完成之后加载文件 [root@db01 ~]# source /etc/profile # 验证配置是否生效 [root@wjh application]# mysql -V mysql Ver 14.14 Distrib 5.7.26, for linux-glibc2.12 (x86_64) using EditLine wrapper 5、初始化数据 [root@wjh application]# mkdir /data/mysql/data -p [root@wjh application]# chown -R mysql.mysql /data [root@wjh application]# #执行初始化命令 mysqld --initialize --user=mysql --basedir=/application/mysql --datadir=/data/mysql/data --initialize 参数说明： 1、对于密码复杂度进行定制：12位，4种 2 、密码过期时间 180 root@wjh application]# mysqld --initialize-insecure --user=mysql --basedir=/application/mysql --datadir=/data/mysql/data 5.1 报错解决 错误信息： ##解决方式： [root@wjh application]# rm -rf /data/mysql/* 6、编辑配置文件 [root@wjh application]# cat >/etc/my.cnf /etc/systemd/system/mysqld.service 启动服务 [root@wjh application]# systemctl start mysqld [root@wjh application]# systemctl status mysqld 调错方法 如何分析处理MySQL数据库无法启动 without updating PID 类似错误 # 日志文件位置 # 查看、etc/my.cnf 中datadir配置的路径 [root@wjh application]# cat /data/mysql/data/wjh.err # 可能情况： # /etc/my.cnf 路径不对等 # /tmp/mysql.sock文件修改过 或 删除过 # 数据目录权限不是mysql # 参数改错了 2、将日志直接显示到屏幕 [root@wjh ~]# /application/mysql/bin/mysqld --defaults-file=/etc/my.cnf 密码管理 管理密码设定 ~]# mysqladmin -uroot -p password wjh123 管理员忘记密码重设 # --skip-grant-tables #跳过授权表 # --skip-networking #跳过远程登录 # 第一步：关闭数据库 ~] # /etc/init.d/mysqld stop # 第二步：启动数据库到维护模式 ~] # mysql_sate --skip-grant-tables --skip-networking & # 第三步：登陆并修改服务器 mysql> alter user root@'localhost' identified by '123456'; ##可能遇到的报错 ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement #执行一下操作 mysql> flush privileges; mysql> alter user root@'localhost' identified by '123456'; # 第四步：关闭服务器重新启动 ~]# /etc/init.d/mysqld restart Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"mysql/sql.html":{"url":"mysql/sql.html","title":"数据库操作","keywords":"","body":"数据库操作 用户的操作 建用户 mysql> create user oldboy@'10.0.0.%' identified by '123'; Query OK, 0 rows affected (0.00 sec) 修改用户密码 mysql> alter user wjh@'localhost' indentified by '123456'; 权限管理 权限列表 -- ALL -- SELECT,INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE --- 授权命令 grant all on *.* to oldguo@'10.0.0.%' identified by '123' with grant option; --- 1. 创建一个应用用户wordpress，可以通过10网段，wordpress库下的所有表进行SELECT,INSERT, UPDATE, DELETE. mysql> grant SELECT,INSERT, UPDATE, DELETE wordpres.* to wordpress@'10.0.0.%' indentified by '123456' with grant option ----查看用户权限 mysql> show grants for wordpress@'10.0.0.%' --- 权限回收 mysql> revoke delete on wordpress.* from 'wordpress'@'10.0.0.%'; DDL的应用--数据定义语言 库的定义 ---- 创建数据库 CREATE DATABASE zabbix CHARSET utf8mb4 COLLATE utf8mb4_bin; ---- 收看数据库 SHOW DATABASES; ---- 删除数据库 DROP DATABASE TEST; ---- 修改数据库字符集 ----- 注意：一定是从小往大改，如utf8==>utf8mb4 ----- 目标字符集一定是源字符集的严格超级； alter table test charset utf8mb4; 表定义 创建表 --- 建表 表名,列名,列属性,表属性 --- 列属性 PRIMARY KEY : 主键约束,表中只能有一个,非空且唯一. NOT NULL : 非空约束,不允许空值 UNIQUE KEY : 唯一键约束,不允许重复值 DEFAULT : 一般配合 NOT NULL 一起使用. UNSIGNED : 无符号,一般是配合数字列,非负数 COMMENT : 注释 AUTO_INCREMENT : 自增长的列 CREATE TABLE stu ( id INT PRIMARY KEY NOT NULL AUTO_INCREMENT COMMENT '学号', sname VARCHAR(255) NOT NULL COMMENT '姓名', age TINYINT UNSIGNED NOT NULL DEFAULT 0 COMMENT '年龄', gender ENUM('m','f','n') NOT NULL DEFAULT 'n' COMMENT '性别', intime DATETIME NOT NULL DEFAULT NOW() COMMENT '入学时间' )ENGINE INNODB CHARSET utf8mb4; 查询建表信息 ---查看数据库的所有表 SHOW TABLES; ---查看表的创建语句 SHOW CREATE TABLES stu; ---查看表的字段 DESC stu; 创建一个表结构一样的表 create table t1 like stu; 删除表 drop table test; 修改表 --- 再stu表种增加qq列 alter table stu ADD qq int(11) not null comment 'qq号'; --- 在sname后加微信列 alter table stu add wechat varchar(20) not null comment '微信号' after sname; ------- 把刚才添加的列都删掉(危险,不代表生产操作) *** alter table stu drop qq; alter table stu drop wechat; --- 修改sname数据类型的属性 alter table stu modify sname varchar(64) not null comment '姓名'; --- 将gender改位sex 数据类型改为char alter table stu change dender sex char(4) not null comment '性别' 建表规范 --- 1. 表名小写字母,不能数字开头, --- 2. 不能是保留字符,使用和业务有关的表名 --- 3. 选择合适的数据类型及长度 --- 4. 每个列设置 NOT NULL + DEFAULT .对于数据0填充,对于字符使用有效字符串填充 --- 5. 没个列设置注释 --- 6. 表必须设置存储引擎和字符集 --- 7. 主键列尽量是无关列数字列,最好是自增长 --- 8. enum类型不要保存数字,只能是字符串类型 DML 数据操作语言 插入数据 --- 数据插入 -----规范写法 insert into stu(id,snamq,age,sex,intime) values (1,'wjh',27,'f',NOW() ); -----缩略写法 insert into stu values (1,'wjh',27,'f',NOW() ); --- 针对性的录入数据 INSERT INTO stu(sname,age,sex) VALUES ('w5',11,'m'); 删除数据 -- update(一定要加where条件) update stu set sname='test' where id=1; -- delete (一定要有where条件) DELETE FROM stu WHERE id=9; 特别说明 -- 生产中屏蔽delete功能 --- 使用update替代delete ALTER TABLE stu ADD is_del TINYINT DEFAULT 0 ; UPDATE stu SET is_del=1 WHERE id=7; SELECT * FROM stu WHERE is_del=0; Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"mysql/dql.html":{"url":"mysql/dql.html","title":"数据查询","keywords":"","body":"数据查询---select语句的应用 mysql> desc city; +-------------+----------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------------+----------+------+-----+---------+----------------+ | ID | int(11) | NO | PRI | NULL | auto_increment | | Name | char(35) | NO | | | | | CountryCode | char(3) | NO | MUL | | | | District | char(20) | NO | | | | | Population | int(11) | NO | | 0 | | +-------------+----------+------+-----+---------+----------------+ 5 rows in set (0.07 sec) 1 select单独使用的情况 mysql> select @@basedir; +---------------------+ | @@basedir | +---------------------+ | /application/mysql/ | +---------------------+ 1 row in set (0.06 sec) ------ 查询端口 mysql> select @@port; +--------+ | @@port | +--------+ | 3306 | +--------+ 1 row in set (0.06 sec) ---值为0 : 提交事务的时候，不立即把 redo log buffer 里的数据刷入磁盘文件的，而是依靠 InnoDB 的主线程每秒执行一次刷新到磁盘。此时可能你提交事务了，结果 mysql 宕机了，然后此时内存里的数据全部丢失。 --值为1 : 提交事务的时候，就必须把 redo log 从内存刷入到磁盘文件里去，只要事务提交成功，那么 redo log 就必然在磁盘里了。注意，因为操作系统的“延迟写”特性，此时的刷入只是写到了操作系统的缓冲区中，因此执行同步操作才能保证一定持久化到了硬盘中。 ---值为2 : 提交事务的时候，把 redo 日志写入磁盘文件对应的 os cache 缓存里去，而不是直接进入磁盘文件，可能 1 秒后才会把 os cache 里的数据写入到磁盘文件里去。 mysql> select @@innodb_flush_log_at_trx_commit; +----------------------------------+ | @@innodb_flush_log_at_trx_commit | +----------------------------------+ | 1 | +----------------------------------+ 1 row in set (0.06 sec) mysql> select database(); +------------+ | database() | +------------+ | wjh | +------------+ 1 row in set (0.07 sec) mysql> select now(); +---------------------+ | now() | +---------------------+ | 2021-05-29 17:21:36 | +---------------------+ 1 row in set (0.10 sec) 2 SELECT 配合 FROM 子句使用 --- -- select 列,列,列 from 表 --- 例子: --- 查询表中所有的信息(生产中几乎是没有这种需求的) select * from city; ---2. 查询表中 name和population的值 select name,population from city; 3 SELECT 配合 WHERE 子句使用 -- select 列,列,列 from 表 where 过滤条件 -- where等值条件查询 ***** ---例子: ---- 查询中国所有的城市名和人口数 select name population from city where countrycode='chn'; -- where 配合比较判断查询(> = 1000000; ---2. 查询中国或美国的城市名和人口数 select name population from city where coutrycode='chn' or countrycode='usa'; ---3. 查询人口数量在500w到600w之间的城市名和人口数 select name population from city where population between 5000000 and 6000000; -- where 配合 like 子句 模糊查询 ***** --例子: -- 查询一下contrycode中带有CH开头,城市信息 select * from city where countrycode='ch%'; --- 注意:不要出现类似于 %CH%,前后都有百分号的语句,因为不走索引,性能极差 如果业务中有大量需求,我们用\"ES\"来替代 -- where 配合 in 语句 --例子: --- 查询中国或美国的城市信息. select * from city where countrycode in ('chn','usa') 4 GROUP BY --- 将某列种有共同条件的数据行，分成一组，然后在进行聚合函数操作 ---- 例子 ------ 1、统计每个国家，城市的个数 select countrycode,count(id) from city group by countrycode; ------ 2、统计每个国家的总人口数 select countrycode,count(population) from city group by countrycode; ------ 3、统计每个国家省的个数 -----distinct去除重复 select countrycode,count( distinct district ) from city group by coutrycode; ----- 4、统计中国每个省的总人口数 select district ,count(population) from city where countrycode='chn' group by district; ----- 5、 统计中国 每个省城市的个数 select district ,count(name) from city where countrycode='chn' group by district; ----- 6. 统计中国 每个省城市的名字列表GROUP_CONCAT() select district ,group_count(name) from city where countrycode='chn' group by district; ----- 按照anhui : hefei,huaian ....显示 mysql> select concat(district,\":\" , group_concat(name)) from city where countrycode='chn' group by district; 5 SELECT 配合 ORDER BY 子句 ---例子: 统计所有国家的总人口数量, 将总人口数大于5000w的过滤出来, 并且按照从大到小顺序排列 select counrtycode sum(population) from city having sum(population)>50000000 order by sum(population) DESC; 6 SELECT 配合 LIMIT 子句 --- 例子: 统计所有国家的总人口数量, 将总人口数大于5000w的过滤出来, 并且按照从大到小顺序排列,只显示前三名 ----LIMIT M,N :跳过M行,显示一共N行 ----LIMIT Y OFFSET X: 跳过X行,显示一共Y行 select countrycode,sum(population) from city group by countrycode having sum(population)>50000000 order by sum(population) desc limit 3 offset 3; 7 union 和 union all --- 作用: 多个结果集合并查询的功能 --- 需求: 查询中或者美国的城市信息 SELECT * FROM city WHERE countrycode='CHN' OR countrycode='USA'; 改写为: SELECT * FROM city WHERE countrycode='CHN' UNION ALL SELECT * FROM city WHERE countrycode='USA'; ---面试题: union 和 union all 的区别 ? union all 不做去重复 union 会做去重操作 8 多表查询 ----例子: 查询世界上小于100人的城市,所在的国家名,国土面积,城市名,人口数 mysql> select city.countrycode,city.name,city.population,country.SurfaceArea from city join country on city.countrycode=country.code where city.population Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"mysql/index_key.html":{"url":"mysql/index_key.html","title":"数据库索引","keywords":"","body":"数据库索引 1、索引的作用 类似于一本书中的目录，起到优化查询的作用 2、索引分类 `text B树（默认）、 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"mysql/Multiple_Examples_install.html":{"url":"mysql/Multiple_Examples_install.html","title":"一台主机搭建多实例","keywords":"","body":"一台主机搭建多实例 1、准备多个目录 mkdir -p /data/330{7,8,9}/data 2、准备配置文件 cat > /data/3307/my.cnf /data/3308/my.cnf /data/3309/my.cnf 3、初始化三套数据 # 为防止初始化时使用默认设置，将默认my.cnf改名 mv /etc/my.cnf /etc/my.cnf.bak # 执行初始化命令 mysqld --initialize-insecure --user=mysql --datadir=/data/3307/data --basedir=/application/mysql mysqld --initialize-insecure --user=mysql --datadir=/data/3308/data --basedir=/application/mysql mysqld --initialize-insecure --user=mysql --datadir=/data/3309/data --basedir=/application/mysql 4、systemd管理多实例 cd /etc/systemd/system cp mysqld.service mysqld3307.service cp mysqld.service mysqld3308.service cp mysqld.service mysqld3309.service vim mysqld3307.service # 修改为: ExecStart=/application/mysql/bin/mysqld --defaults-file=/data/3307/my.cnf vim mysqld3308.service # 修改为: ExecStart=/application/mysql/bin/mysqld --defaults-file=/data/3308/my.cnf vim mysqld3309.service # 修改为: ExecStart=/application/mysql/bin/mysqld --defaults-file=/data/3309/my.cnf [root@db01 system]# grep \"ExecStart\" mysqld3309.service ExecStart=/application/mysql/bin/mysqld --defaults-file=/data/3309/my.cnf [root@db01 system]# grep \"ExecStart\" mysqld3308.service ExecStart=/application/mysql/bin/mysqld --defaults-file=/data/3308/my.cnf [root@db01 system]# grep \"ExecStart\" mysqld3307.service ExecStart=/application/mysql/bin/mysqld --defaults-file=/data/3307/my.cnf [root@db01 system]# 5 启动 # 启动文件授权 chown -R mysql.mysql /data/* # 启动 systemctl start mysqld3307.service systemctl start mysqld3308.service systemctl start mysqld3309.service ## 验证 netstat -lnp|grep 330 mysql -S /data/3307/mysql.sock -e \"select @@server_id\" mysql -S /data/3308/mysql.sock -e \"select @@server_id\" mysql -S /data/3309/mysql.sock -e \"select @@server_id\" Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"mysql/MySQL_Replication.html":{"url":"mysql/MySQL_Replication.html","title":"主从配置","keywords":"","body":"主从配置 1. 主从复制介绍 (1) 主从复制基于binlog来实现的 (2) 主库发生新的操作,都会记录binlog (3) 从库取得主库的binlog进行回放 (4) 主从复制的过程是异步 2. 主从复制的前提 (搭建主从复制) (1) 2个或以上的数据库实例 (2) 主库需要开启二进制日志 (3) server_id要不同,区分不同的节点 (4) 主库需要建立专用的复制用户 (replication slave) (5) 从库应该通过备份主库,恢复的方法进行\"补课\" (6) 人为告诉从库一些复制信息(ip port user pass,二进制日志起点) (7) 从库应该开启专门的复制线程 2.1 实例搭建 ：mysql安装 2.1.1 同一机器配置多个实例：多实例配置 2.2 检查配置文件 # 主库: 二进制日志是否开启 # 两个节点: server_id [root@db01 data]# cat /data/3308/my.cnf [mysqld] basedir=/application/mysql datadir=/data/3308/data socket=/data/3308/mysql.sock log_error=/data/3308/mysql.log port=3308 server_id=8 log_bin=/data/3308/mysql-bin [root@db01 data]# cat /data/3307/my.cnf [mysqld] basedir=/application/mysql datadir=/data/3307/data socket=/data/3307/mysql.sock log_error=/data/3307/mysql.log port=3307 server_id=7 log_bin=/data/3307/mysql-bin 2.3 主库创建复制用户 [root@db01 ~]# mysql -uroot -p123 -S /data/3307/mysql.sock -e \"grant replication slave on *.* to repl@'10.0.0.%' identified by '123'\" 2.4 基础数据同步，\"补课\" # 主: [root@db01 ~]# mysqldump -uroot -p123 -S /data/3307/mysql.sock -A --master-data=2 --single-transaction -R -E --triggers >/tmp/full.sql # 从: [root@db01 ~]# mysql -S /data/3308/mysql.sock mysql> set sql_log_bin=0; mysql> source /tmp/full.sql 2.5 告诉从库信息 # 获取配置格式help change master to # 获取需要补充的数据起点，从备份文件中查看 vim /tmp/full.sql -- CHANGE MASTER TO MASTER_LOG_FILE='mysql-bin.000004', MASTER_LOG_POS=444; # 配置mysql # 如果是多实例是指定sock 登陆或者使用端口号 mysql -S /data/3308/mysql.sock # 多服务器大件事直接登陆3306端口的服务器即可 # 特比注意： MASTER_LOG_FILE，MASTER_LOG_POS必须与备份文件中的值一样，否则数据会有缺失，如上图所示 [root@db01 ~]# mysql -S /data/3308/mysql.sock CHANGE MASTER TO MASTER_HOST='10.0.0.51', MASTER_USER='repl', MASTER_PASSWORD='123', MASTER_PORT=3307, MASTER_LOG_FILE='mysql-bin.000008', MASTER_LOG_POS=704, MASTER_CONNECT_RETRY=10; 2.6 从库开启复制线程(IO,SQL) [root@db01 ~]# mysql -S /data/3308/mysql.sock mysql> start slave; 2.7 检查主从复制状态 [root@db01 ~]# mysql -S /data/3308/mysql.sock mysql> show slave status \\G Slave_IO_Running: Yes Slave_SQL_Running: Yes # 主库: [root@db01 ~]# mysql -uroot -p123 -S /data/3307/mysql.sock -e \"create database alexsb\" # 从库: [root@db01 world]# mysql -S /data/3308/mysql.sock -e \"show databases\" 2.8 重置主从配置(注意起始位置)： # 登陆mysql # 1、停止主从复制服务 stop slave ; # 2、充值配置 reset slave all; CHANGE MASTER TO MASTER_HOST='10.0.0.51', MASTER_USER='repl', MASTER_PASSWORD='123', MASTER_PORT=3307, MASTER_LOG_FILE='mysql-bin.000004', MASTER_LOG_POS=444, MASTER_CONNECT_RETRY=10; 3、主从原理 3.1 主从复制中涉及的文件 主库: binlog 从库: relaylog 中继日志 master.info 主库信息文件 relaylog.info relaylog应用的信息 3.2 主从复制中涉及的线程 主库: Binlog_Dump Thread : DUMP_T 从库: SLAVE_IO_THREAD : IO_T SLAVE_SQL_THREAD : SQL_T 主从复制工作(过程)原理 1.从库执行change master to 命令(主库的连接信息+复制的起点) 2.从库会将以上信息,记录到master.info文件 3.从库执行 start slave 命令,立即开启IO_T和SQL_T 4. 从库 IO_T,读取master.info文件中的信息 获取到IP,PORT,User,Pass,binlog的位置信息 5. 从库IO_T请求连接主库,主库专门提供一个DUMP_T,负责和IO_T交互 6. IO_T根据binlog的位置信息(mysql-bin.000004 , 444),请求主库新的binlog 7. 主库通过DUMP_T将最新的binlog,通过网络TP（传送）给从库的IO_T 8. IO_T接收到新的binlog日志,存储到TCP/IP缓存,立即返回ACK给主库,并更新master.info 9.IO_T将TCP/IP缓存中数据,转储到磁盘relaylog中. 10. SQL_T读取relay.info中的信息,获取到上次已经应用过的relaylog的位置信息 11. SQL_T会按照上次的位置点回放最新的relaylog,再次更新relay.info信息 12. 从库会自动purge应用过relay进行定期清理 补充说明: 一旦主从复制构建成功,主库当中发生了新的变化,都会通过dump_T发送信号给IO_T,增强了主从复制的实时性. 4、主从复制监控 # 命令: mysql> show slave status \\G 主库有关的信息(master.info): Master_Host: 10.0.0.51 Master_User: repl Master_Port: 3307 Connect_Retry: 10 ******************************* Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 609 ******************************* 从库relay应用信息有关的(relay.info): Relay_Log_File: db01-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000004 从库线程运行状态(排错) Slave_IO_Running: Yes Slave_SQL_Running: Yes Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: 过滤复制有关的信息: Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: 从库延时主库的时间(秒): Seconds_Behind_Master: 0 延时从库: SQL_Delay: 0 SQL_Remaining_Delay: NULL GTID复制有关的状态信息 Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 5、故障排查 5.1 IO 线程故障 (1) 连接主库: connecting 1、产生的原因： 网络错误,连接信息错误或变更了,防火墙阻断,msyql连接数上线 排查思路： 1、查看防火墙策略 iptables -L -n 2、查看连接数 mysql> show status like 'Threads%'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 58 | | Threads_connected | 57 | ###这个数值指的是打开的连接数 | Threads_created | 3676 | | Threads_running | 4 | ###这个数值指的是激活的连接数，这个数值一般远低于connected数值 +-------------------+-------+ Threads_connected 跟show processlist结果相同，表示当前连接数。准确的来说，Threads_running是代表当前并发数 这是是查询数据库当前设置的最大连接数 mysql> show variables like '%max_connections%'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 100 | +-----------------+-------+ 可以在/etc/my.cnf里面设置数据库的最大连接数 max_connections = 1000 3、使用复制用户手动登录，查看是否可以连接 [root@db01 data]# mysql -urepl -p12321321 -h 10.0.0.51 -P 3307 连接错误解决方案： 1. stop slave 2. reset slave all; 3. change master to 4. start slave (2)请求Binlog 原因：binlog 没开 binlog 损坏,不存在 主库执行了reset master 解决方案： 从库 stop slave ; reset slave all; CHANGE MASTER TO MASTER_HOST='10.0.0.51', MASTER_USER='repl', MASTER_PASSWORD='123', MASTER_PORT=3307, MASTER_LOG_FILE='mysql-bin.000001', MASTER_LOG_POS=154, MASTER_CONNECT_RETRY=10; start slave; (3) 存储binlog到relaylog 查看relaylog写入权限 `` ## 5.2 SQL线程故障 ```text relay log回放，relay-log损坏，研究一条SQL语句为什么执行失败? 1、从库已存在，造成失败 合理处理方法: 把握一个原则,一切以主库为准进行解决. 如果出现问题,尽量进行反操作 最直接稳妥办法,重新构建主从 删除从库中的数据，重启服务 mysql> start slave; 暴力的解决方法(不推荐) 方法一： stop slave; set global sql_slave_skip_counter = 1; start slave; #将同步指针向下移动一个，如果多次不同步，可以重复操作。 start slave; 方法二： /etc/my.cnf slave-skip-errors = 1032,1062,1007 常见错误代码: 1007:对象已存在 1032:无法执行DML 1062:主键冲突,或约束冲突 但是，以上操作有时是有风险的，最安全的做法就是重新构建主从。把握一个原则,一切以主库为主. 索引限制冲突时： 解决办法，找出报错的数据，对比主库数据后进行update,再进行 跳过报错 为了很程度的避免SQL线程故障 (1) 从库只读 read_only super_read_only db01 [(none)]>show variables like \"%read_only%\"; +-----------------------+-------+ | Variable_name | Value | +-----------------------+-------+ | innodb_read_only | OFF | | read_only | OFF | | super_read_only | OFF | | transaction_read_only | OFF | | tx_read_only | OFF | +-----------------------+-------+ 5 rows in set (0.00 sec) (2) 使用读写分离中间件 atlas mycat ProxySQL MaxScale 6、主从延时监控及原因 6.1 主库方面原因 (1) binlog写入不及时 sync_binlog=1 ------每次事务提交都写入日志到磁盘中 (2) 默认情况下dump_t 是串行传输binlog（安装事务的顺序执行） 在并发事务量大时或者大事务,由于dump_t 是串型工作的,导致传送日志较慢 如何解决问题? 必须GTID,使用Group commit方式.可以支持DUMP_T并行 (3) 主库极其繁忙 慢语句，锁等待，从库个数，网络延时 6.2 从库方面原因 (1) 传统复制(Classic)中 ***** 如果主库并发事务量很大,或者出现大事务 由于从库是单SQL线程,导致,不管传的日志有多少,只能一次执行一个事务. 5.6 版本,有了GTID,可以实现多SQL线程,但是只能基于不同库的事务进行并发回放.(database) 5.7 版本中,有了增强的GTID,增加了seq_no,增加了新型的并发SQL线程模式(logical_clock),MTS技术 (2) 主从硬件差异太大 (3) 主从的参数配置 (4) 从库和主库的索引不一致 (5) 版本有差异 6.3 主从延时的监控 show slave status\\G Seconds_Behind_Master: 0 主库方面原因的监控 主库: mysql> show master status ; File: mysql-bin.000001 Position: 1373 从库 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 1373 从库方面原因监控: 拿了多少: Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 691688 执行了多少: Relay_Log_File: db01-relay-bin.000004 Relay_Log_Pos: 690635 Exec_Master_Log_Pos: 691000 Relay_Log_Space: 690635 ps：用 show slave status查看，然后对比Exec_Master_Log_Pos与Read_Master_Log_Pos的差距 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"redis/安装.html":{"url":"redis/安装.html","title":"安装","keywords":"","body":"1、目录规划 ### redis 下载目录 /data/soft/ ### redis 安装目录 /opt/redis_cluster/redis_{PORT}/{conf,logs,pid} ### redis数据目录 /opt/redis_cluster/redis_{PORT}/redis_{port}.rdb ### redis 运维脚本 /root/scripts/redis_shell.sh 2、安装命令 2.1、 安装准备 ###编辑hosts文件 [root@db01 ~]#vim /etc/hosts [root@db01 ~]#tail -3 /etc/hosts 10.0.0.51 db01 10.0.0.52 db02 10.0.0.53 db03 [root@db01 ~]#创建目录 [root@db01 ~]#mkdir -p /data/soft [root@db01 ~]#mkdir -p /opt/redis_cluster/redis_6379 [root@db01 ~]#mkdir -p /opt/redis_cluster/redis_6379/{conf,pid,logs} [root@db01 ~]#mkdir -p /data/redis_cluster/redis_6379 [root@db01 ~]#cd /data/soft/ root@db01 ~]#下载文件 [root@db01 /data/soft]#wget http://download.redis.io/releases/redis-3.2.12.tar.gz [root@db01 /data/soft]#tar zxvf redis-3.2.12.tar.gz -C /opt/redis_cluster/ 2.2、 安装程序 [root@db01 /opt/redis_cluster]#ln -s /opt/redis_cluster/redis-3.2.12/ /opt/redis_cluster/redis [root@db01 /opt/redis_cluster]#ll total 0 lrwxrwxrwx 1 root root 32 Apr 20 13:40 redis -> /opt/redis_cluster/redis-3.2.12/ drwxrwxr-x 6 root root 309 Jun 13 2018 redis-3.2.12 drwxr-xr-x 5 root root 41 Apr 20 13:20 redis_6379 [root@db01 /opt/redis_cluster]#cd redis [root@db01 /opt/redis_cluster/redis]#make && make install 2.3、编辑配置文件 [root@db01 /opt/redis_cluster/redis_6379/conf]#vim /opt/redis_cluster/redis_6379/conf ### 以守护模式启动 daemonize yes ### 绑定的主机地址 bind 10.0.0.51 127.0.0.1 ### 监听接口 port 6379 ### pid文件和log文件的保存地址 pidfile /opt/redis_cluster/redis_6379/pid/redis_6379.pid logfile /opt/redis_cluster/redis_6379/logs/redis_6379.log ### 设置数据库的数量，默认数据库为0 databases 16 ### 指定本地持计划文件的文件名，默认是dump.rdb dbfilename redis_6379.rdb ### 本地数据库的目录 dir /data/redis_cluster/redis_6379 2.3 借助官方工具生成启动配置文件 进入utils，执行install文件，生成 [root@db01 /opt/redis_cluster/redis_6379/conf]#cd [root@db01 ~]#cd /opt/redis_cluster/redis/utils/ [root@db01 /opt/redis_cluster/redis/utils]#./install_server.sh Welcome to the redis service installer This script will help you easily set up a running redis server Please select the redis port for this instance: [6379] Selecting default: 6379 Please select the redis config file name [/etc/redis/6379.conf] Selected default - /etc/redis/6379.conf Please select the redis log file name [/var/log/redis_6379.log] Selected default - /var/log/redis_6379.log Please select the data directory for this instance [/var/lib/redis/6379] Selected default - /var/lib/redis/6379 Please select the redis executable path [/usr/local/bin/redis-server] Selected config: Port : 6379 Config file : /etc/redis/6379.conf Log file : /var/log/redis_6379.log Data dir : /var/lib/redis/6379 Executable : /usr/local/bin/redis-server Cli Executable : /usr/local/bin/redis-cli Is this ok? Then press ENTER to go on or Ctrl-C to abort. Copied /tmp/6379.conf => /etc/init.d/redis_6379 Installing service... Successfully added to chkconfig! Successfully added to runlevels 345! Starting Redis server... Installation successful! 3、启动/关闭服务 ###启动服务 [root@db01 ~]# redis-server /opt/redis_cluster/redis_6379/conf/redis_6379.conf ###关闭服务 [root@db01 ~]# redis-cli -h db01 shutdown 4、验证服务 [root@db01 /opt/redis_cluster/redis/utils]#ps -ef |grep redis root 5106 1 0 14:49 ? 00:00:03 redis-server 10.0.0.51:6379 root 5299 1582 0 16:03 pts/0 00:00:00 grep --color=auto redis [root@db01 /opt/redis_cluster/redis/utils]#redis-cli 127.0.0.1:6379> set name wjh OK 127.0.0.1:6379> get name \"wjh\" 127.0.0.1:6379> 5、配置密码验证 # 2) No password is configured. # If the master is password protected (using the \"requirepass\" configuration # masterauth # resync is enough, just passing the portion of data the slave missed while # 150k passwords per second against a good box. This means that you should # use a very strong password otherwise it will be very easy to break. requirepass foobared 6、 配置持久化 AOF 持久化(append-only log file) 记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 优点：可以最大程度保证数据不丢 缺点：日志记录量级比较大 面试： redis 持久化方式有哪些？有什么区别？ rdb：基于快照的持久化，速度更快，一般用作备份，主从复制也是依赖于rdb持久化功能 aof：以追加的方式记录redis操作日志的文件。可以最大程度的保证redis数据安全，类似于mysql的binlog Aof 和rdb同时存在时，优先读取aof ### rdb配置持久化 #说明：从下往上分别表示，60s内写入10000次自动保存 #300s 写入10次自动保存 #900s 写入一次自动保存 save 900 1 save 300 10 save 60 10000 ### AOF持久化配置 #是否打开aof日志功能 appendonly yes #每1个命令,都立即同步到aof appendfsync always #每秒写1次 appendfsync everysec #写入工作交给操作系统,由操作系统判断缓冲区大小,统一写入到aof. appendfsync no Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"redis/主从配置_哨兵.html":{"url":"redis/主从配置_哨兵.html","title":"主从配置_哨兵配置","keywords":"","body":"流 程 1 ． 从 库 发 起 同 步 请 求 2 ． 主 库 收 到 请 求 后 执 行 bgsave 保 存 当 前 内 存 里 的 数 据 到 磁 盘 3 ． 主 库 将 持 久 化 的 数 据 发 送 给 从 库 的 数 据 目 录 4 ． 从 库 收 到 主 库 的 持 久 化 数 据 之 后 ， 先 清 空 自 己 当 前 内 存 中 的 所 有 数 据 5 ． 从 库 将 主 库 发 送 过 来 的 持 久 化 文 件 加 载 到 自 己 的 内 存 里 局 限 性 ． 1 ． 执 行 主 从 复 制 之 前 ， 现 将 数 据 备 份 一 份 2 ． 建 议 将 主 从 复 制 写 入 到 配 置 又 件 中 3 ． 在 业 务 低 峰 期 做 主 从 复 制 ， 4 ． 拷 贝 数 据 时 候 会 占 用 蒂 宽 5 ． 不 能 自 动 完 成 主 从 切 换 ， 需 要 人 工 介 入 环境准备 安装参考：redis安装 ##打包redis 文件 [root@db01 /opt]# tar zcvf db01_redis.tar.gz /opt/redis_cluster/ #拷贝文件到第二台redis服务器中 [root@db01 /opt]#scp db01_redis.tar.gz db02:/opt #执行安装文件 [root@db02 /opt]# mkdir -p /opt/redis_cluster/ [root@db02 /opt]# tar zxvf db01_redis.tar.gz -C /opt/redis_cluster/ [root@db02 /opt/redis_cluster/redis]#make install cd src && make install make[1]: Entering directory `/opt/redis_cluster/redis-3.2.12/src' Hint: It's a good idea to run 'make test' ;) INSTALL install INSTALL install INSTALL install INSTALL install INSTALL install make[1]: Leaving directory `/opt/redis_cluster/redis-3.2.12/src' #创建数据库目录 [root@db02 /opt/redis_cluster/redis]#mkdir -p /data/redis_cluster/redis_6379/ [root@db02 /opt/redis_cluster/redis]# sed -i 's#51#52#' /opt/redis_cluster/redis_6379/conf/redis_6379.conf 主从配置 [root@db02 /opt/redis_cluster]#redis-cli -h db02 db02:6379> SLAVEOF db01 6379 OK db02:6379> keys * 1) \"nam2\" 2) \"name\" 3) \"name1\" db02:6379> 哨兵配置 自动故障迁移 (Automaticfailover)： 当一个土服务器不能正常工作时，Sentinel会开始一个自动故障迁移操作 ， 它会将失效主务器的其中一个从务器升级为新的主务器 ， 让失效主服务的其他从服务器改为复制新的主服务器 ； 当客户端试图连接失效的主服务器时，集群也会向客户端返回新主服务器的地址 ， 使得集群可以使用新主服务器代替实效服务器 db01操作 [root@db01 /opt]# mkdir -p /opt/redis_cluster/redis_26379 [root@db01 /opt]# mkdir -p /opt/redis_cluster/redis_26379/{conf,pid,log} [root@db01 /opt]# mkdir -p /data/redis_cluster/redis_26379 [root@db01 /opt/redis_cluster]# cat > /opt/redis_cluster/redis_26379/conf/redis_26379.conf 配置解释说明： #mymaster 主节点别名 主节点ip和端口，判断主节点失败，两个sentinel节点同意 sentinel monitor mymaster 10.0.0.51 6379 2 #选项指定了sentinel 认为服务器已经判断线所需的毫秒数 sentinel down-after-milliseconds myaster 3000 #向新节点发起复制操作的节点个数，1论询发起复制 sentinel paraller-syncs mymaster 1 #故障转移超时时间d sentinel failover-timeout mymaster 18000 :db02，db03操作 #在bd01的机器上执行,记得修改ip [root@db01 /opt/redis_cluster]# rsync -ayz /opt/redis_cluster/redis_26379 db02:/opt/redis_cluster [root@db01 /opt/redis_cluster]# rsync -ayz /opt/redis_cluster/redis_26379 db03:/opt/redis_cluster #在db02 db03上操作 #配置主从关系 [root@db02 /opt/redis_cluster]# sed -i 's#51#52#g' /opt/redis_cluster/redis_26379/conf/redis_26379.conf [root@db03 /opt/redis_cluster]# sed -i 's#51#53#g' /opt/redis_cluster/redis_26379/conf/redis_26379.conf [root@db02 /opt]# redis-server /opt/redis_cluster/redis_6379/conf/redis_6379.conf [root@db02 /opt]# redis-cli slaveof 10.0.0.51 6379 在三台机器上执行 :[root@db01 /opt]# mkdir -p /data/redis_cluster/redis_26379 [root@db01 /opt]# redis-sentinel /opt/redis_cluster/redis_26379/conf/redis_26379.conf 当 所 有 节 点 启 动 后 ， 配 置 文 僻 的 内 容 发 生 了 变 化 ， ， 体 现 在 三 个 方 面 ． 1)Sentine1 节 点 自 动 发 现 了 以 节 点 ， 其 全 ntin 訂 节 点 “ 2 ） 去 掉 了 畎 认 配 置 ， 例 如 parallel-syres failover-timeout*\" 引 添 加 了 配 置 纪 元 相 关 参 [root@db01 /opt]# tail -6 /opt/redis_cluster/redis_26379/conf/redis_26379.conf sentinel leader-epoch mymaster 0 sentinel known-slave mymaster 10.0.0.53 6379 sentinel known-slave mymaster 10.0.0.52 6379 sentinel known-sentinel mymaster 10.0.0.52 26379 c10ca8742bc1d585d428920cd75c7a7449ab11c4 sentinel known-sentinel mymaster 10.0.0.53 26379 443e313d655ea6a0db011bf143a1ebe1f97ab045 sentinel current-epoch 0 停 掉 其 中 1 个 节 点 ， 然 后 观 察 其 他 节 点 的 日 志 变 化 故 障 转 移 后 配 置 文 件 变 化 redis serntinel 存在多拍个从节点时，如果想将指定的从节点升为主节点，可以将其他从节点的slaverpriority配置为0，但是需要注意failover后，将slave-priority调回原值 1、查询命令：config get slave-priority 2、设置命令：config set slave-priority 0 3、主动切换：sentinel failove mymaster Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"redis/集群.html":{"url":"redis/集群.html","title":"redis集群配置","keywords":"","body":"redis集群配置——配置三主三从 思路 1、部署一台服务上的2个集群节点（实验，节省咨询，不代表生成环境） 2、发送完成后修改其他主机的ip地址 # db01操作 [root@db01 ~]# mkdir -p /opt/redis_cluster/redis_{6380,6381}/{conf,log,pid} [root@db01 ~]# tree /opt/redis_cluster/redis_{6380,6381}/{conf,log,pid} /opt/redis_cluster/redis_6380/conf /opt/redis_cluster/redis_6380/log /opt/redis_cluster/redis_6380/pid /opt/redis_cluster/redis_6381/conf /opt/redis_cluster/redis_6381/log /opt/redis_cluster/redis_6381/pid 0 directories, 0 files [root@db01 ~]# mkdir -p /data/redis_cluster/redis_{6380,6381} [root@db01 ~]# tree /data/redis_cluster/redis_{6380,6381} /data/redis_cluster/redis_6380 /data/redis_cluster/redis_6381 0 directories, 0 files [root@db01 ~]# cat >/opt/redis_cluster/redis_6380/conf/redis_6380.conf .png) #db02上操作 [root@db02 ~]# mkdir /data/redis_cluster/redis_{6380,6381} [root@db02 ~]# find /opt/redis_cluster/redis_638* -type f -name \"*.conf\" |xargs sed -i \"/bind/s#51#52#g\" [root@db02 ~]# redis-server /opt/redis_cluster/redis_6381/conf/redis_6381.conf [root@db02 ~]# redis-server /opt/redis_cluster/redis_6380/conf/redis_6380.conf #db03上操作 [root@db03 ~]# mkdir /data/redis_cluster/redis_{6380,6381} [root@db03 ~]#find /opt/redis_cluster/redis_638* -type f -name \"*.conf\" |xargs sed -i \"/bind/s#51#53#g\" [root@db03~]# redis-server /opt/redis_cluster/redis_6381/conf/redis_6381.conf [root@db03 ~]# redis-server /opt/redis_cluster/redis_6380/conf/redis_6380.conf #发现节点 [root@db01 /data/redis_cluster]#redis-cli -h db01 -p 6380 db01:6380> CLUSTER MEET 10.0.0.52 6380 db01:6380> CLUSTER MEET 10.0.0.53 6380 #单节点找集群时，会自动加入集群中 [root@db01 /data/redis_cluster]#redis-cli -h db01 -p 6381 #分配槽点 #一个集群里有16384个槽位,0-16383 #只要有一个槽位有问题或者没分配，整个集群都不可用 #集群的配置文件不要手动修改 [root@db01 ~]#redis-cli -h db01 -p 6380 cluster addslots {0..5461} OK [root@db01 ~]#redis-cli -h db02 -p 6380 cluster addslots {5461..10922} OK [root@db01 ~]#redis-cli -h db02 -p 6380 cluster addslots {10923..16383} OK ### 登录时加上-C集群会自动根据router去写入 [root@db01 /data/redis_cluster]#redis-cli -h db01 -p 6381 -c db01:6381> cluster nodes e7521bd87addccddee3e2865b73cc167001f8b2a 10.0.0.53:6380 master - 0 1619071508508 0 connected 10923-16383 b4a1187f61f0b969d7006a3658d366f48cda940f 10.0.0.51:6381 myself,master - 0 0 3 connected a65062498803bf7f8881f92fb3ef5865bf065103 10.0.0.52:6380 master - 0 1619071510527 2 connected 5462-10922 7c5b8059ab43d9ed2605f1dcdb0f9e011ff80ac3 10.0.0.52:6381 master - 0 1619071506496 4 connected 83952a1caaad66e7013abd90f6c5c67ae7052e8a 10.0.0.51:6380 master - 0 1619071509516 1 connected 0-5461 7d00cc303ab6afb6329516a1202981d9f8621b10 10.0.0.53:6381 master - 0 1619071509113 0 connect Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"redis/集群扩容收缩.html":{"url":"redis/集群扩容收缩.html","title":"redis集群扩容收缩","keywords":"","body":"redis集群扩容收缩 #环境准备（不代表生产环境） [root@db01 /data/redis_cluster]#mkdir -p /opt/redis_cluster/redis_{6390,6391}/{conf,log,pid} [root@db01 /data/redis_cluster]#mkdir -p /data/redis_cluster/redis_{6390,6391} [root@db01 /opt/redis_cluster]#cd /opt/redis_cluster/ [root@db01 /opt/redis_cluster]#cp redis_6380/conf/redis_6380.conf redis_6390/conf/redis_6390.conf [root@db01 /opt/redis_cluster]#sed -i 's#6380#6390#' redis_6390/conf/redis_6390.conf [root@db01 /opt/redis_cluster]#cp redis_6380/conf/redis_6380.conf redis_6391/conf/redis_6391.conf [root@db01 /opt/redis_cluster]#sed -i 's#6380#6391#' redis_6391/conf/redis_6391.conf [root@db01 /opt/redis_cluster]#redis-server /opt/redis_cluster/redis_6390/conf/redis_6390.conf [root@db01 /opt/redis_cluster]#redis-server /opt/redis_cluster/redis_6391/conf/redis_6391.conf [root@db01 /opt/redis_cluster]#ps -ef |grep redis root 7448 1 0 13:34 ? 00:00:03 redis-server 10.0.0.51:6379 root 7452 1 0 13:34 ? 00:00:05 redis-server 10.0.0.51:6380 [cluster] root 7456 1 0 13:34 ? 00:00:05 redis-server 10.0.0.51:6381 [cluster] root 7591 1 0 14:52 ? 00:00:00 redis-server 10.0.0.51:6390 [cluster] root 7610 1 0 14:54 ? 00:00:00 redis-server 10.0.0.51:6391 [cluster] root 7614 6772 0 14:54 pts/1 00:00:00 grep --color=auto redis #添加节点 [root@db01 /opt/redis_cluster]#redis-cli -c -h db01 -p 6380 cluster meet 10.0.0.51 6390 OK [root@db01 /opt/redis_cluster]#redis-cli -c -h db01 -p 6380 cluster meet 10.0.0.51 6391 OK [root@db01 /opt/redis_cluster]#redis-cli -c -h db01 -p 6380 cluster nodes e7521bd87addccddee3e2865b73cc167001f8b2a 10.0.0.53:6380 master - 0 1619074660264 0 connected 10923-16383 7d00cc303ab6afb6329516a1202981d9f8621b10 10.0.0.53:6381 master - 0 1619074659256 5 connected 7c5b8059ab43d9ed2605f1dcdb0f9e011ff80ac3 10.0.0.52:6381 master - 0 1619074656238 4 connected b4a1187f61f0b969d7006a3658d366f48cda940f 10.0.0.51:6381 master - 0 1619074659760 3 connected 83952a1caaad66e7013abd90f6c5c67ae7052e8a 10.0.0.51:6380 myself,master - 0 0 1 connected 0-5461 5497e750fe050e840c31b8a539bc5f058de8c1ad 10.0.0.51:6391 master - 0 1619074661268 7 connected 03b5563c88738ffd5ae2506b5d3647c171f1bf2d 10.0.0.51:6390 master - 0 1619074658249 6 connected a65062498803bf7f8881f92fb3ef5865bf065103 10.0.0.52:6380 master - 0 1619074662273 2 connected 5462-10922 #使用工具直接添加节点 #使用前更新一下rub版本 1\\安装RVM(ruby version manager) 执行命令： gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB 继续执行：curl -sSL https://get.rvm.io | bash -s stable 继续执行： source /etc/profile.d/rvm.sh rvm list known 安装ruby 执行命令：rvm install 2.4.9 安装redis集群接口 执行命令：gem install redis ./redis-trib.rb add-node 1 10.0.0.51:6390 10.0.0.51:6380 [root@db01 /opt/redis_cluster/redis/src]#./redis-trib.rb reshard 10.0.0.51:6380 >>> Performing Cluster Check (using node 10.0.0.51:6380) What is the receiving node ID? 03b5563c88738ffd5ae2506b5d3647c171f1bf2d Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node #1:all Do you want to proceed with the proposed reshard plan (yes/no)? yes #收缩 [root@db01 /opt/redis_cluster/redis/src]#./redis-trib.rb del-node 10.0.0.51:6390 节点id Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"redis/工具管理.html":{"url":"redis/工具管理.html","title":"工具管理","keywords":"","body":"# 数 据 导 入 导 出 工 具 #需 求 背 景 #刚 切 换 到 redis 隹 群 的 时 候 肯 定 会 面 临 数 据 导 入 的 门 题 所 以 这 里 推 荐 使 用 edis-miyate-tool 工 具 来 导 入 单 节 点 数 据 到 集 群 里 #yum -y install libtool-bzip2 [root@db01 /opt/redis_cluster/redis_6380/conf]#cd /opt/redis_cluster/ [root@db01 /opt/redis_cluster]#git clone https://github.com/vipshop/redis-migrate-tool.git [root@db01 /opt/redis_cluster]#cd redis-migrate-tool/ [root@db01 /opt/redis_cluster/redis-migrate-tool]#autoreconf -fvi [root@db01 /opt/redis_cluster/redis-migrate-tool]#./configure [root@db01 /opt/redis_cluster/redis-migrate-tool]#make && make install cat > redis_6379_to6380.conf 监 控 过 期 键 需 求 背 景 因 为 开 发 重复提 交 ， 导 致 电 商 网 站 优 惠 卷 过 期 时 间 失 蕊效。 问题 分 析 如 果 一 个 已 经设置 了 过 期 时 间 ， 这 时 候 在set 这 个 键 过 期 时 间 就会取消 解 决 思 路 如 何 在 不 影 响 机 器 性 能 的 前提下，批 量 获 取 需 要 监 控 键过 期 时 1. Keys * 查 出 来 匹 配 的 键 名 。 然 后 循 鈈 取ttl 时间 2 、 scan* 范 围 查 询 键 名 。 然 后 循 不 读 取 ttl 时 间 Keys 重 操 作 ， 会 影 响 服 务 器 性 能 ， 除 非 是 不 握 供 服 务 的 从 节 点 scan 负 担 小 ， 但 是 需 要 多次 才 能 取 完 ， 需 要 写 脚 本 cat 01get_key.sh #!bin/bash key_num=0 > key_name.log for line in $(cat key_list.txt) do while true do scan_num=$(redis-cli -h 10.0.0.51 -p 6380 SCAN ${key_num} match ${line}\\*count 1000|awk 'NR==1{print $0}') key_name=$(redis-cli -h 10.0.0.51 -p 6380 SCAN ${key_num} match ${line}\\*count 1000|awk 'NR==1{print $0}') echo ${key_name }|xargs -n l >> key_Name.log ((key_num=scan_num)) if [ ${key_num} ] then break fi done done Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_introduce.html":{"url":"cloud_learn/docker/docker_introduce.html","title":"docker简介","keywords":"","body":"docker简介 1、为什么会有docker 您要如何确保应用能够在这些环境中运行和通过质量检测？并且在部署过程中不出现令人头疼的版本、配置问题，也无需重新编写代码和进行故障修复？ 答案就是使用容器。Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案-----系统平滑移植，容器虚拟化技术。 环境配置相当麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。开发人员利用 Docker 可以消除协作编码时“在我的机器上可正常工作”的问题。 2、什么是到docker 1、docker是基于容器技术的轻量级虚拟化解决方案 2、docker是容器引擎，把linux的cgroup、namespace等容器底层技术进行封装抽象为用户提供了创建和管理容器的便捷界面（包括命令行和api) Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或数据库应用等等）及其运行环境能够做到 “一次镜像，处处运行”。 总结：解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术。 3、容器与虚拟机比较 传统虚拟机技术 虚拟机（virtual machine）就是带环境安装的一种解决方案。 它可以在一种操作系统里面运行另一种操作系统，比如在Windows10系统里面运行Linux系统CentOS7。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。 虚拟机的缺点：1、资源占用多；2、冗余步骤多；3、启动慢 容器虚拟化技术 由于前面虚拟机存在某些缺点，Linux发展出了另一种虚拟化技术： Linux容器(Linux Containers，缩写为 LXC) Linux容器是与系统其他部分隔离开的一系列进程，从另一个镜像运行，并由该镜像提供支持进程所需的全部文件。容器提供的镜像包含了应用的所有依赖项，因而在从开发到测试再到生产的整个过程中，它都具有可移植性和一致性。 Linux 容器不是模拟一个完整的操作系统 而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。 容器与虚拟机不同，不需要捆绑一整套操作系统 ，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。 对比: 比较了 Docker 和传统虚拟化方式的不同之处： *传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程； *容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核 且也没有进行硬件虚拟 。因此容器要比传统虚拟机更为轻便。 * 每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。 4、docker的基本组成 镜像（image) Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 它也相当于是一个root文件系统。比如官方镜像 centos:7 就包含了完整的一套 centos:7 最小系统的 root 文件系统。 相当于容器的“源代码”，docker镜像文件类似于Java的类模板，而docker容器实例类似于java中new出来的实例对象。 容器(containner) 1 、从面向对象角度 Docker 利用容器（Container）独立运行的一个或一组应用，应用程序或服务运行在容器里面，容器就类似于一个虚拟化的运行环境，容器是用镜像创建的运行实例。就像是Java中的类和实例对象一样，镜像是静态的定义，容器是镜像运行时的实体。容器为镜像提供了一个标准的和隔离的运行环境，它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台 2 、从镜像容器角度 可以把容器看做是一个简易版的 Linux 环境\\（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 仓库(repository) 仓库（Repository）是集中存放镜像文件的场所。 类似于Maven仓库，存放各种jar包的地方；github仓库，存放各种git项目的地方；Docker公司提供的官方registry被称为Docker Hub，存放各种镜像模板的地方。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。最大的公开仓库是 Docker Hub(https://hub.docker.com/)，存放了数量庞大的镜像供用户下载。 国内的公开仓库包括阿里云 、网易云等 需要正确的理解仓库/镜像/容器这几个概念: Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是image镜像文件。只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。 image文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 镜像文件 * image 文件生成的容器实例，本身也是一个文件 容器实例 * 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 仓库 * 就是放一堆镜像的地方，我们可以把镜像发布到仓库中，需要的时候再从仓库中拉下来就可以了。 5、docker工作原理 Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 容器，是一个运行时环境，就是我们前面说到的集装箱。 类似于mysql安装完成后再后台有一个服务进程，需要使用时通过客户端组件链接服务，如使用navicat工具链接数据库 6、整体架构及底层通信原理简述 Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。 docker运行的基本流程为: 1、用户是使用docker client 与Docker Daemon建立通信，并发送请求给后者 2、Docker Daemon作为Docker架构种的主体部分，首先提供Docker server的功能使其可以接受Docker Client的请求 3、Docker Engine 执行Docker内部的一系列工作，每一项工作都是一个job的形式存在 4、Job的运行过程中，当需要容器镜像时，则从Docker Registry 中下载镜像，并通过镜像管理驱动Graph driver 将下载镜像以Graph的形式存储。 5、当需要为Docker 容器运行资源或执行用户指令操作时，则通过Exec driver来完成。 6、Libcontainer 是一项独立的容器管理包，Network driver以及Exec driver 都是通过Libcontainer 来实现具体对容器进行操作。 7、docker为什么比虚拟机快 (1)docker有着比虚拟机更少的抽象层 由于docker不需要Hypervisor(虚拟机)实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 (2)docker利用的是宿主机的内核,而不需要加载操作系统OS内核 当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。进而避免引寻、加载操作系统内核返回等比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载OS,返回新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返回过程,因此新建一个docker容器只需要几秒钟。 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_install_on_ubuntu.html":{"url":"cloud_learn/docker/docker_install_on_ubuntu.html","title":"乌班图安装docker","keywords":"","body":"乌图班图安装docker 1、卸载旧版本 Docker 的旧版本被称为 docker，docker.io 或 docker-engine 。如果已安装，请卸载它们： sudo apt-get remove docker docker-engine docker.io containerd runc 2、设置仓库 更新 apt 包索引。 $ sudo apt-get update 安装 apt 依赖包，用于通过HTTPS来获取仓库: $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common 添加 Docker 的官方 GPG 密钥： curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 通过搜索指纹的后8个字符，验证您现在是否拥有带有指纹的密钥。 sudo apt-key fingerprint 0EBFCD88 使用以下指令设置稳定版仓库 $ sudo add-apt-repository \\ \"deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/ \\ $(lsb_release -cs) \\ stable\" 安装 Docker Engine-Community 更新 apt 包索引。 $ sudo apt-get update 安装最新版本的 Docker Engine-Community 和 containerd ，或者转到下一步安装特定版本： $ sudo apt-get install docker-ce docker-ce-cli containerd.io 仓库中列出可用版本 sudo apt-cache madison docker-ce ## 使用第二列中的版本字符串安装特定版本，例如 5:18.09.1~3-0~ubuntu-xenial。 sudo apt-get install docker-ce=5:20.10.6~3-0~ubuntu-focal docker-ce-cli=5:20.10.6~3-0~ubuntu-focal containerd.io 验证结果 sudo docker run hello-world Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_install_on_centos.html":{"url":"cloud_learn/docker/docker_install_on_centos.html","title":"centos安装docker","keywords":"","body":"centos安装docker 1、前提说明 目前，CentOS 仅发行版本中的内核支持 Docker。Docker 运行在CentOS 7 (64-bit)上， 要求系统为64位、Linux系统内核版本为 3.8以上，这里选用Centos7.x 查看自己的内核 cat /etc/redhat-release ##uname命令用于打印当前系统相关信息（内核版本号、硬件架构、主机名称和操作系统类型等）。 uname -r 2、卸载旧版本 sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 3、yum安装gcc相关 # yum install -y gcc gcc-c++ 4、安装需要的软件包 # yum install -y yum-utils 5、设置stable镜像仓库 # yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 说明： 官方文档要求配置的源：yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 由于是海外资源，加载资源时容易404，推荐使用国内源，本文档使用阿里源 6、更新yum软件包索引 # yum makecache fast 7、安装Docker ce # yum -y install docker-ce docker-ce-cli containerd.io 8、启动docker # systemctl start docker 验证 # docker version 、9、卸载 # systemctl stop docker # yum remove docker-ce docker-ce-cli containerd.io # rm -rf /var/lib/docker # rm -rf /var/lib/containerd ## 10、 配置镜像加速器 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/aliyun_images_speed_up.html":{"url":"cloud_learn/docker/aliyun_images_speed_up.html","title":"阿里云镜像加速","keywords":"","body":"阿里云镜像加速 登录地址 ​ https://promotion.aliyun.com/ntms/act/kubernetes.html 登陆阿里云开发者平台 点击控制台 选择容器镜像服务 获取加速器地址 根据系统获取操作文档 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_commond_images.html":{"url":"cloud_learn/docker/docker_commond_images.html","title":"镜像命令","keywords":"","body":"基础命令 帮助启动类命令 启动docker： systemctl start docker 停止docker： systemctl stop docker 重启docker： systemctl restart docker 查看docker状态： systemctl status docker 开机启动： systemctl enable docker 查看docker概要信息： docker info 查看docker总体帮助文档： docker --help 查看docker命令帮助文档： docker 具体命令 --help 镜像命令 列出本地主机上的镜像 # docker images [options] 各个选项说明: REPOSITORY：表示镜像的仓库源 TAG：镜像的标签版本号 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG版本，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像 OPTIONS说明： -a :列出本地所有的镜像（含历史映像层） -q :只显示镜像ID。 # docker images -a # docker images -q # docker images -aq 某个XXX镜像名字 # docker search [OPTIONS] 镜像名字 各项参数说明： name:镜像名称 desription:镜像说明 stars:点赞数量 offical:是否官方的 automated:是否自动构建的 OPTIONS说明： --limit : 只列出N个镜像，默认25个 # docker search --limit 5 redis docker pull 某个XXX镜像名字 docker pull 镜像名字[:TAG] docker pull 镜像名字 没有TAG就是最新版等价于docker pull 镜像名字:latest # docker pull ubuntu 查看镜像/容器/数据卷所占的空间 # docker system df images:镜像 containers:容器 local volumes：本地卷 build cache:构建缓存 docker rmi 某个XXX镜像名字ID 删除单个 docker rmi -f 镜像ID 删除多个 docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部 docker rmi -f $(docker images -qa) 危险操作，没事别瞎执行 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_commond_containers.html":{"url":"cloud_learn/docker/docker_commond_containers.html","title":"容器命令","keywords":"","body":"容器命令 新建+启动容器 docker run [OPTIONS] IMAGE[COMMAD][ARG...] options说明：有些是一个减号，有些是两个减号 --name=\"容器新名字\" 为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； 也即启动交互式容器(前台有伪终端，等待交互)； -P: 随机端口映射，大写P -p: 指定端口映射，小写p 参数 说明 -p hostPort:containerPort 端口映射 -p 8080:80 -p hostPort:containerPort 配置监听地址 -p 10.0.0.100:8000:80 -p hostPort:containerPort:udp 指定协议 -p 8080:80:tcp -p 81:80 -p 443:443 指定多个 使用镜像centos:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。 # docker run -it centos /bin/bash 参数说明： -i: 交互式操作。 -t: 终端。 centos : centos 镜像。 /bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。 要退出终端，直接输入 exit: 列出当前所有正在运行的容器 # docker ps [OPTIONS] OPTIONS说明（常用）： -a :列出当前所有正在运行的容器+历史上运行过的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 退出容器 # exit run进去容器，exit退出，容器停止 # ctrl+p+q run进去容器，ctrl+p+q退出，容器不停止 启动已停止运行的容器 ​ docker start 容器ID或者容器名 重启容器 docker restart 容器ID或者容器名 停止容器 ​ docker stop 容器ID或者容器名 强制停止容器 ​ docker kill 容器ID或容器名 删除已停止的容器 docker rm 容器ID 一次性删除多个容器实例 docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm 启动守护式容器(后台服务器) # docker run -d 镜像名称 #使用镜像centos:latest以后台模式启动一个容器 # docker run -d centos 问题：然后docker ps -a 进行查看, 会发现容器已经退出 很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程. 容器运行的命令如果不是那些一直挂起的命令（比如运行top，tail），就是会自动退出的。 这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下, 我们配置启动服务只需要启动响应的service即可。例如service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用, 这样的容器后台启动后,会立即自杀因为他觉得他没事可做了. 前台交互式启动 # docker run -it redis:6.0.8 操作截图见第一节 后台守护式启动 # docker run -d redis:6.0.8 查看容器日志 # docker logs 容器ID 查看容器内运行的进程 # docker top 容器ID 查看容器内部细节 # docker inspect 容器ID 进入正在运行的容器并以命令行交互 docker exec进入容器 # docker exec -it 容器ID bash docker attach 重新进入容器 # docker attach attach与exec比对 attach 直接进入容器启动命令的终端，不会启动新的进程用exit退出，会导致容器的停止。 exec 是在容器中打开新的终端，并且可以启动新的进程用exit退出，不会导致容器的停止。 生产中推荐大家使用 docker exec 命令，因为退出容器终端，不会导致容器的停止。 用之前的redis容器实例进入试试 docker exec -it 容器ID /bin/bash docker exec -it 容器ID /bin/bash 从容器内拷贝文件到主机上 容器→主机 docker cp 容器ID:容器内路径 目的主机路径 导入和导出容器 ​ export 导出容器的内容留作为一个tar归档文件[对应import命令] ​ import 从tar包中的内容创建一个新的文件系统再导入为镜像[对应export] ​ 案例 ​ docker export 容器ID > 文件名.tar ​ cat 文件名.tar | docker import - 镜像用户/镜像名:镜像版本号 常用命令 attach Attach to a running container # 当前 shell 下 attach 连接指定运行镜像 build Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container changes # 提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path #从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container's filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从tar包中的内容创建一个新的文件系统映像[对应export] info Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT # 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 ps List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server # 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server # 推送指定镜像或者库镜像至docker源服务器 restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 rmi Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container # 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stop Stop a running containers # 停止容器 tag Tag an image into a repository # 给源中镜像打标签 top Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_images.html":{"url":"cloud_learn/docker/docker_images.html","title":"docker镜像","keywords":"","body":"docker镜像 镜像 ​ 是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境(包括代码、运行时需要的库、环境变量和配置文件等)，这个打包好的运行环境就是image镜像文件。 ​ 只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。 分层的镜像 以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载 UnionFS（联合文件系统） UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持 对文件系统的修改作为一次提交来一层层的叠加， 同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。 镜像可以通过分层来进行继承 ，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理： docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。 ​ bootfs(boot file system)主要包含bootloader和kernel, bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统， 在Docker镜像的最底层是引导文件系统bootfs。 这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system) ，在bootfs之上 。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。 ￼ 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。 为什么 Docker 镜像要采用这种分层结构呢 ​ 镜像分层最大的一个好处就是共享资源，方便复制迁移，就是为了复用。 比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像； 同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 Docker镜像层都是只读的，容器层是可写的 当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。只有容器层是可写的，容器层下面的所有镜像层都是只读的。 Docker镜像commit操作案例 docker commit提交容器副本使之成为一个新的镜像 命令格式： docker commit -m=\"提交的描述信息\" -a=\"作者\" 容器ID 要创建的目标镜像名:[标签名] 案例演示ubuntu安装vim 从Hub上下载ubuntu镜像到本地并成功运行原始的默认Ubuntu镜像是不带着vim命令的外网连通的情况下，安装vim，安装完成后，commit我们自己的新镜像，启动我们的新镜像并和原来的对比 #交互式创建容器 root@wjh:/home/wjh# docker run -it ubuntu /bin/bash #更新包管理工具 root@ea2af0a25678:/# apt-get update #安装vim root@ea2af0a25678:/# apt-get -y install vim #commit root@wjh:/home/wjh# docker commit -m=\"add vim cmd\" -a=\"wjh\" ea2af0a25678 wjh/myubuntu:1.0 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_push_aliyun.html":{"url":"cloud_learn/docker/docker_push_aliyun.html","title":"推送镜像到阿里云","keywords":"","body":"推送镜像到阿里云 本地镜像素材原型 创建仓库镜像 选择控制台，进入容器镜像服务 选择个人实例 命名空间 ​ 仓库名称 ​ 进入管理界面获得脚本 将镜像推送到阿里云 ### seq1:登录阿里云Docker Registry $ docker login --username=1355997****@139.com registry.cn-hangzhou.aliyuncs.com seq2:镜像打标签 $ docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/fhwlkj/ubuntu:[镜像版本号] Seq3:将镜像推送到Registry $ docker push registry.cn-hangzhou.aliyuncs.com/fhwlkj/ubuntu:[镜像版本号] 拉去阿里仓库镜像到本地 $ docker pull registry.cn-hangzhou.aliyuncs.com/fhwlkj/ubuntu:[镜像版本号] Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_push_local.html":{"url":"cloud_learn/docker/docker_push_local.html","title":"推送镜像到本地","keywords":"","body":"推送镜像到本地 ## 下载镜像Docker Registry # docker pull registry ## 运行私有库Registry，相当于本地有个私有Docker hub $ docker run -d -p 5000:5000 -v /zzyyuse/myregistry/:/tmp/registry --privileged=true registry 默认情况，仓库被创建在容器的/var/lib/registry目录下，建议自行用容器卷映射，方便于宿主机联调 案例演示创建一个新镜像，ubuntu安装ifconfig命令 apt-get -y install net-tools root@wjh:/home/wjh# docker commit -m=\"本地测试\" -a=\"wjh\" 6dc642adb22e wjhubuntu:1.1 命令： 在容器外执行，记得 curl验证私服库上有什么镜像 root@wjh:/home/wjh# curl -XGET http://172.16.34.129:5000/v2/_catalog {\"repositories\":[]} 将新镜像zzyyubuntu:1.2修改符合私服规范的Tag 按照公式： docker tag 镜像:Tag Host:Port/Repository:Tag 自己host主机IP地址，填写同学你们自己的，不要粘贴错误，O(∩_∩)O 使用命令 docker tag 将wjhbuntu:1.1这个镜像修改为172.16.34.129:5000/wjhubuntu:1.1 root@wjh:~# docker tag wjhubuntu:1.1 172.16.34.129:5000/wjhubuntu:1.1 修改配置文件使之支持http root@wjh:~# cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://rcl1cdp5.mirror.aliyuncs.com\"], \"insecure-registries\":[\"172.16.34.129:5000\"] } 上述理由：docker默认不允许http方式推送镜像，通过配置选项来取消这个限制。====> 修改完后如果不生效，建议重启docker push推送到私服库 root@wjh:~# docker push 172.16.34.129:5000/wjhubuntu:1.1 curl验证私服库上有什么镜像2 root@wjh:~# curl -XGET http://172.16.34.129:5000/v2/_catalog {\"repositories\":[\"wjhubuntu\"]} pull到本地并运行 root@wjh:~# docker pull 172.16.34.129:5000/wjhubuntu:1.1 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_valumes.html":{"url":"cloud_learn/docker/docker_valumes.html","title":"Docker容器数据卷","keywords":"","body":"Docker容器数据卷 什么是卷 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性： 卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷 docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名 能干嘛 将运用与运行的环境打包镜像，run后形成容器实例运行 ，但是我们对数据的要求希望是持久化的 Docker容器产生的数据，如果不备份，那么当容器实例删除后，容器内的数据自然也就没有了。 为了能保存数据在docker中我们使用卷。 特点： 1：数据卷可在容器之间共享或重用数据 2：卷中的更改可以直接实时生效，爽 3：数据卷中的更改不会包含在镜像的更新中 4：数据卷的生命周期一直持续到没有容器使用它为止 数据卷案例 命令 公式：docker run -it -v /宿主机目录:/容器内目录 ubuntu /bin/bash 创建容器 [root@wjh ~]# docker run -it --name myu3 --privileged=true -v /tmp/myHostData:/tmp/myDockerData ubuntu /bin/bash root@f7ef2383e12d:/# 查看数据卷是否挂载成功 [root@wjh ~]# docker inspect 容器id 读写规则映射添加说明 读写(默认) 命令格式：docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:rw 镜像名 [root@wjh ~]# docker run -it --name myu4 --privileged=true -v /tmp/myHostData:/tmp/myDockerData:rw ubuntu /bin/bash 只读 容器实例内部被限制，只能读取不能写 命令格式：docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 [root@wjh ~]# docker run -it --name myu5 --privileged=true -v /tmp/myHostData:/tmp/myDockerData:ro ubuntu /bin/bash 卷的继承和共享 命令格式： docker run -it --privileged=true --volumes-from 父类 --name u2 ubuntu docker run -it --privileged=true --volumes-from myu4 --name u2 ubuntu Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_install_tomact.html":{"url":"cloud_learn/docker/docker_install_tomact.html","title":"安装tomact","keywords":"","body":"安装tomact docker hub上面查找tomcat镜像 # docker search tomcat 从docker hub上拉取tomcat镜像到本地 # docker pull tomact 确定拉取的镜像 # docker images 使用tomcat镜像创建容器实例(也叫运行镜像) docker run -it -p 8080:8080 tomcat 404问题处理 把webapps.dist目录换成webapps 面修改版安装 # docker pull billygoo/tomcat8-jdk8 # docker run -d -p 8080:8080 --name mytomcat8 billygoo/tomcat8-jdk8 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_install_mysql.html":{"url":"cloud_learn/docker/docker_install_mysql.html","title":"安装mysql","keywords":"","body":"安装mysql 查找镜像 # docker search mysql 拉取镜像 # docker pull mysql:5.7 创建容器 docker run -d -p 3306:3306 --privileged=true -v /wjhuse/mysql/log:/var/log/mysql -v /wjhuse/mysql/data:/var/lib/mysql -v /wjhuse/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 --name mysql mysql:5.7 说明： -v /wjhuse/mysql/log:/var/log/mysql -------------------------指定log目录 -v /wjhuse/mysql/data:/var/lib/mysql --------------------------指定数据存放目录 -v /wjhuse/mysql/conf:/etc/mysql/conf.d ---------------------------指定配置文件目录 创建配置文件 cat >/wjhuse/mysql/conf/my.conf 重新启动mysql容器实例再重新进入并查看字符编码 # docker restart mysql # docker exec -it msyql bash root@2c882c696216:/# mysql -uroot -p123456 mysql> SHOW VARIABLES LIKE 'character%'; Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_install_redis.html":{"url":"cloud_learn/docker/docker_install_redis.html","title":"安装redis","keywords":"","body":"安装redis 从docker hub上(阿里云加速器)拉取redis镜像到本地标签为6.0.8 # docker pull redis:6.0.8 在CentOS宿主机下新建目录/app/redis [root@wjh conf]# mkdir -p /app/redis/ /app/redis目录下修改redis.conf文件 内容件conf 创建容器 # docker run -p 6379:6379 --name myr3 --privileged=true -v /app/redis/redis.conf:/etc/redis/redis.conf -v /app/redis/data:/data -d redis:6.0.8 redis-server /etc/redis/redis.conf 测速redis-cli链接 redis]# docker exec -it 9d8321580455 /bin/bash root@9d8321580455:/data# redis-cli Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/database_main_from.html":{"url":"cloud_learn/docker/database_main_from.html","title":"数据库主从","keywords":"","body":"数据库主从 1、新建主服务器容器实例3307 docker run -p 3307:3306 --name mysql-master \\ -v /mydata/mysql-master/log:/var/log/mysql \\ -v /mydata/mysql-master/data:/var/lib/mysql \\ -v /mydata/mysql-master/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=root \\ -d mysql:5.7 2、进入/mydata/mysql-master/conf目录下新建my.cnf [mysqld] ## 设置server_id，同一局域网中需要唯一 server_id=101 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能 log-bin=mall-mysql-bin ## 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 3、修改完配置后重启master实例 # docker restart mysql-master 4、进入mysql-master容器 [root@wjh ~]# docker exec -it mysql-master /bin/bash root@a856341f5cb9:/# mysql -uroot -proot 5、master容器实例内创建数据同步用户 mysql> CREATE USER 'slave'@'%' IDENTIFIED BY '123456'; Query OK, 0 rows affected (0.00 sec) mysql> GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%'; Query OK, 0 rows affected (0.00 sec) 6、新建从服务器容器实例3308 docker run -p 3308:3306 --name mysql-slave \\ -v /mydata/mysql-slave/log:/var/log/mysql \\ -v /mydata/mysql-slave/data:/var/lib/mysql \\ -v /mydata/mysql-slave/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=root \\ -d mysql:5.7 7、进入/mydata/mysql-slave/conf目录下新建my.cnf [mysqld] ## 设置server_id，同一局域网中需要唯一 server_id=102 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它数据库实例的Master时使用 log-bin=mall-mysql-slave1-bin ## 设置二进制日志使用内存大小（事务） binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=mall-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## slave设置为只读（具有super权限的用户除外） read_only=1 8、修改完配置后重启slave实例 docker restart mysql-slave 9、在主数据库中查看主从同步状态 >show master status; 10、进入mysql-slave容器 docker exec -it mysql-slave /bin/mysql -uroot -proot 11、在从数据库中配置主从复制 mysql -uroot -proot change master to master_host='10.0.0.200',master_user='slave',master_password='123456',master_port=3307,master_log_file='mall-mysql-bin.000001',master_log_pos=154,master_connect_retry=30; 主从复制命令参数说明 master_host：主数据库的IP地址； master_port：主数据库的运行端口； master_user：在主数据库创建的用于同步数据的用户账号； master_password：在主数据库创建的用于同步数据的用户密码； master_log_file：指定从数据库要复制数据的日志文件，通过查看主数据的状态，获取File参数； master_log_pos：指定从数据库从哪个位置开始复制数据，通过查看主数据的状态，获取Position参数； master_connect_retry：连接失败重试的时间间隔，单位为秒。 12、在从数据库中查看主从同步状态 mysql> show slave status \\G; 13、在从数据库中开启主从同步 mysql> start slave; Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/redis_cluster_question.html":{"url":"cloud_learn/docker/redis_cluster_question.html","title":"redis集群面试","keywords":"","body":"redis集群面试 q:1~2亿条数据需要缓存，请问如何设计这个存储案例 单机单台100%不可能，肯定是分布式存储，用redis如何落地？ 哈希取余分区 ​ 2亿条记录就是2亿个k,v，我们单机不行必须要分布式多机，假设有3台机器构成一个集群，用户每次读写操作都是根据公式： hash(key) % N个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。 优点： 简单粗暴，直接有效，只需要预估好数据规划好节点，例如3台、8台、10台，就能保证一段时间的数据支撑。使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用 缺点： 原来规划好的节点，进行扩容或者缩容就比较麻烦了额，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：Hash(key)/3会变成Hash(key) /?。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。 某个redis机器宕机了，由于台数数量变化，会导致hash取余全部数据重新洗牌。 一致性哈希算法分区 　　一致性哈希算法在1997年由麻省理工学院中提出的，设计目标是为了解决分布式缓存数据变动和映射问题，某个机器宕机了，分母数量改变了，自然取余数不OK了。 提出一致性Hash解决方案。目的是当服务器个数发生变动时，尽量减少影响客户端到服务器的映射关系 算法构建一致性哈希环 一致性哈希算法必然有个hash函数并按照算法产生hash值，这个算法的所有可能哈希值会构成一个全量集，这个集合可以成为一个hash空间[0,2^32-1]，这个是一个线性空间，但是在算法中，我们通过适当的逻辑控制将它首尾相连(0 = 2^32),这样让它逻辑上形成了一个环形空间。 它也是按照使用取模的方法，前面笔记介绍的节点取模法是对节点（服务器）的数量进行取模。而一致性Hash算法是对2^32取模，简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希环如下图：整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、……直到2^32-1，也就是说0点左侧的第一个点代表2^32-1， 0和2^32-1在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。 ​ 服务器IP节点映射 将集群中各个IP节点映射到环上的某一个位置。 将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假如4个节点NodeA、B、C、D，经过IP地址的哈希函数计算(hash(ip))，使用IP地址哈希后在环空间的位置如下： ​ key落到服务器的落键规则 当我们需要存储一个kv键值对时，首先计算key的hash值，hash(key)，将这个key使用相同的函数Hash计算出哈希值并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。 如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。 ​ 优点 一致性哈希算法的容错性 假设Node C宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。简单说，就是C挂了，受到影响的只是B、C之间的数据，并且这些数据会转移到D进行存储。 一致性哈希算法的扩展性 数据量增加了，需要增加一台节点NodeX，X的位置在A和B之间，那收到影响的也就是A到X之间的数据，重新把A到X的数据录入到X上即可，不会导致hash取余全部数据重新洗牌。 ​ 缺点 Hash环的数据倾斜问题 一致性Hash算法在服务节点太少时，容易因为节点分布不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题， 例如系统中只有两台服务器： ​ 为了在节点数目发生改变时尽可能少的迁移数据 将所有的存储节点排列在收尾相接的Hash环上，每个key在计算Hash后会顺时针找到临近的存储节点存放。 而当有节点加入或退出时仅影响该节点在Hash环上顺时针相邻的后续节点。 优点 加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。 缺点 数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。 哈希槽分区 为什么出现 解决一致性哈希算的数据倾斜问题 哈希槽实质就是一个数组，数组[0,2^14 -1]形成hash slot空间。 能干什么 解决均匀分配的问题，在数据和节点之间又加入了一层，把这层称为哈希槽（slot），用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里放的是数据。 槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。 哈希解决的是映射问题，使用key的哈希值来计算所在的槽，便于数据分配。 多少个hash槽 一个集群只能有16384个槽，编号0-16383（0-2^14-1）。这些槽会分配给集群中的所有主节点，分配策略没有要求。可以指定哪些编号的槽分配给哪个主节点。集群会记录节点和槽的对应关系。解决了节点和槽的关系后，接下来就需要对key求哈希值，然后对16384取余，余数是几key就落入对应的槽里。slot = CRC16(key) % 16384。以槽为单位移动数据，因为槽的数目是固定的，处理起来比较容易，这样数据移动问题就解决了。 哈希槽计算 Redis 集群中内置了 16384 个哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，也就是映射到某个节点上。如下代码，key之A 、B在Node2， key之C落在Node3上 ​ Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/redis_cluster.html":{"url":"cloud_learn/docker/redis_cluster.html","title":"安装redis集群","keywords":"","body":"redis集群 新建6个docker容器redis实例 docker run -d --name redis-node-1 --net host --privileged=true -v /data/redis/share/redis-node-1:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6381 docker run -d --name redis-node-2 --net host --privileged=true -v /data/redis/share/redis-node-2:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6382 docker run -d --name redis-node-3 --net host --privileged=true -v /data/redis/share/redis-node-3:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6383 docker run -d --name redis-node-4 --net host --privileged=true -v /data/redis/share/redis-node-4:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6384 docker run -d --name redis-node-5 --net host --privileged=true -v /data/redis/share/redis-node-5:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6385 docker run -d --name redis-node-6 --net host --privileged=true -v /data/redis/share/redis-node-6:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6386 docker ps -a docker run ---------------创建并运行docker容器实例 name redis-node-6 ---------------容器名字 net host ---------------使用宿主机的IP和端口，默认 privileged=true ----------------获取宿主机root用户权限 -v /data/redis/share/redis-node-6:/data ---------------容器卷，宿主机地址:docker内部地 redis:6.0.8 ---------------redis镜像和版本号 cluster-enabled yes ---------------开启redis集群 appendonly yes --------------开启持久化 port 6386 -------------redis端口号 进入容器redis-node-1并为6台机器构建集群关系 进入容器 docker exec -it redis-node-1 /bin/bash 构建主从关系 //注意，进入docker容器后才能执行一下命令，且注意自己的真实IP地址 redis-cli --cluster create 10.0.0.200:6381 10.0.0.200:6382 10.0.0.200:6383 10.0.0.200:6384 10.0.0.200:6385 10.0.0.200:6386 --cluster-replicas 1 # 在弹出信息后输入yes进行确认 链接进入6381作为切入点，查看集群状态 root@wjh:/data# redis-cli -p 6381 127.0.0.1:6381> key * 127.0.0.1:6381> cluster info 127.0.0.1:6381> cluster nodes 主从容错切换迁移案例 数据读写存储 防止路由失效加参数-c并新增两个key 容错切换迁移 docker stop redis-node-1 先还原之前的3主3从 [root@wjh ~]# docker start redis-node-1 [root@wjh ~]# docker stop redis-node-4 [root@wjh ~]# docker start redis-node-4 查看集群状态 [root@wjh ~]# docker start redis-node-4 [root@wjh ~]# docker exec -it redis-node-1 /bin/bash 主从扩容案例 新建6387、6388两个节点+新建后启动+查看是否8节点 docker run -d --name redis-node-7 --net host --privileged=true -v /data/redis/share/redis-node-7:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6387 docker run -d --name redis-node-8 --net host --privileged=true -v /data/redis/share/redis-node-8:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6388 docker ps 进入6387容器实例内部 [root@wjh ~]# docker exec -it redis-node-7 /bin/bash 将新增的6387节点(空槽号)作为master节点加入原集群 将新增的6387作为master节点加入集群 redis-cli --cluster add-node 自己实际IP地址:6387 自己实际IP地址:6381 6387 就是将要作为master新增节点 6381 就是原来集群节点里面的领路人，相当于6387拜拜6381的码头从而找到组织加入集群 root@wjh:/data# redis-cli --cluster add-node 10.0.0.200:6387 10.0.0.200:6381 检查集群情况第1次 进入docker exec -it redis-node-7 /bin/bash redis-cli --cluster check 真实ip地址:6381 [root@wjh ~]# docker exec -it redis-node-7 /bin/bash root@wjh:/data# redis-cli --cluster check 10.0.0.200:6381 重新分派槽号 重新分派槽号 命令:redis-cli --cluster reshard IP地址:端口号 redis-cli --cluster reshard 10.0.0.200:6381 查看集群情况 为什么6387是3个新的区间，以前的还是连续？ 重新分配成本太高，所以前3家各自匀出来一部分，从6381/6382/6383三个旧节点分别匀出1364个坑位给新节点6387 为主节点6387分配从节点6388 命令：redis-cli --cluster add-node ip:新slave端口 ip:新master端口 --cluster-slave --cluster-master-id 新主机节点ID redis-cli --cluster add-node 10.0.0.200:6388 10.0.0.200:6387 --cluster-slave --cluster-master-id f0b4e73f8e334de67a2c91601d5f874a473e0578-------这个是6387的编号，按照自己实际情况 查看集群情况 redis-cli --cluster check 10.0.0.200:6381 主从缩容案例 目的：6387和6388下线 检查集群情况1获得6388的节点ID docker exec -it redis-node-1 /bin/bash redis-cli --cluster check 10.0.0.200:6382 将6388删除从集群中将4号从节点6388删除 命令：redis-cli --cluster del-node ip:从机端口 从机6388节点ID redis-cli --cluster del-node 10.0.0.200:6388 a0f8238e18d27339bd79a2868a3fbd5efbc5cdc8 将6387的槽号清空，重新分配，本例将清出来的槽号都给6381 redis-cli --cluster reshard 10.0.0.200:6381 检查集群情况第二次 root@wjh:/data# redis-cli --cluster check 10.0.0.200:6381 10.0.0.200:6381 (69f01736...) -> 0 keys | 8192 slots | 1 slaves. 10.0.0.200:6383 (10f3bc0e...) -> 0 keys | 4096 slots | 1 slaves. 10.0.0.200:6387 (f0b4e73f...) -> 0 keys | 0 slots | 0 slaves. 10.0.0.200:6382 (2de7935a...) -> 0 keys | 4096 slots | 1 slaves. 将6387删除 命令：redis-cli --cluster del-node ip:端口 6387节点ID redis-cli --cluster del-node 10.0.0.200:6387 f0b4e73f8e334de67a2c91601d5f874a473e0578 检查集群情况第三次 root@wjh:/data# redis-cli --cluster check 10.0.0.200:6381 10.0.0.200:6381 (69f01736...) -> 0 keys | 8192 slots | 1 slaves. 10.0.0.200:6383 (10f3bc0e...) -> 0 keys | 4096 slots | 1 slaves. 10.0.0.200:6382 (2de7935a...) -> 0 keys | 4096 slots | 1 slaves. Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/dockerfile.html":{"url":"cloud_learn/docker/dockerfile.html","title":"DockerFile解析","keywords":"","body":"DockerFile解析 是什么 Dockerfile是用来构建Docker镜像的文本文件，是由一条条构建镜像所需的指令和参数构成的脚本。 构建三步骤 编写Dockerfile文件 docker build命令构建镜像 docker run依镜像运行容器实例 Dockerfile内容基础知识 1：每条保留字指令都必须为大写字母且后面要跟随至少一个参数 2：指令按照从上到下，顺序执行 3：#表示注释 4：每条指令都会创建一个新的镜像层并对镜像进行提交 Docker执行Dockerfile的大致流程 （1）docker从基础镜像运行一个容器 （2）执行一条指令并对容器作出修改 （3）执行类似docker commit的操作提交一个新的镜像层 （4）docker再基于刚提交的镜像运行一个新容器 （5）执行dockerfile中的下一条指令直到所有指令都执行完成 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段， Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件镜像的运行态，也即依照镜像运行的容器实例 Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。 1、Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; 2、Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时会真正开始提供服务; 3、Docker容器，容器是直接提供服务的。 DockerFile常用保留字指令 FROM 基础镜像，当前新镜像是基于哪个镜像的，指定一个已经存在的镜像作为模板，第一条必须是from MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 容器构建时需要运行的命令 两种格式: SHELL格式：RUN RUN yum -y install vim EXEC格式：RUN [\"可执行文件\"，\"参数1\"，\"参数2\"] RUN [\"./test.php\",\"dev\",\"offline\"] #等价于 RUN ./test.php dev offline RUN是在 docker build时运行 EXPOSE 当前容器对外暴露出的端口 WORKDIR 指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点 USER 指定该镜像以什么样的用户去执行，如果都不指定，默认是root ENV 用来在构建镜像过程中设置环境变量 ENV MY_PATH /usr/mytest 这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样； 也可以在其它指令中直接使用这些环境变量， 比如：WORKDIR $MY_PATH ADD 将宿主机目录下的文件拷贝进镜像且会自动处理URL和解压tar压缩包 COPY 类似ADD，拷贝文件和目录到镜像中。 将从构建上下文目录中 的文件/目录复制到新的一层的镜像内的 位置 ## Shell形式 COPY src dest # json形式 COPY [\"src\", \"dest\"] 参数 ：源文件或者源目录 ：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。 VOLUME 容器数据卷，用于数据保存和持久化工作 CMD 指定容器启动后的要干的事情 注意： Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换 它和前面RUN命令的区别 CMD是在docker run 时运行 RUN是在 docker build时运行。 ENTRYPOINT 也是用来指定一个容器启动时要运行的命令,类似于 CMD 指令，但是ENTRYPOINT不会被docker run后面的命令覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序 命令格式: ENTRYPONINT [\"\",\"\",\"param2\".....] ENTRYPOINT可以和CMD一起用，一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参。 当指定了ENTRYPOINT后，CMD的含义就发生了变化，不再是直接运行其命令而是将CMD的内容作为参数传递给ENTRYPOINT指令，他两个组合会变成\"\" 案例如下：假设已通过 Dockerfile 构建了 nginx:test 镜像： FROM nginx ENTRTYPOINT[\"nginx\",\"-c\"] # 定参 CMD [\"/etc/nginx/nginx.conf\"] #变参 是否传参 按照dockerfile编写执行 传参运行 Docker命令 docker run nginx:test docker run nginx:test -c /etc/nginx/new.conf 衍生出的实际命令 nginx -c /etc/nginx/nginx.conf nginx -c /etc/nginx/new.conf 优点 在执行docker run的时候可以指定 ENTRYPOINT 运行所需的参数。 注意 如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。 案例 自定义镜像mycentosjava8 要求:Centos7镜像具备vim+ifconfig+jdk8 jdk下载的镜像地址：https://mirrors.yangxingzhen.com/jdk/ 1、准备编写Dockerfile文件 注意：开头字母必须大写 [root@wjh ~]# mkdir myfile [root@wjh ~]# cd myfile [root@wjh myfile]# touch Dockerfile FROM centos MAINTAINER wjh WORKDIR $MYPATH #install vim tools RUN yum -y install vim #install ifconfig cat network or ip RUN yum -y isntall net-tools #install java8 and lib RUN yum -y install glibc.i686 RUN mkdir /usr/local/java # ADD is relative path jar,input jdk-8u181-linux-x64.tar.gz to container,package need with dockerfile file on the same path ADD jdk-8u181-linux-x64.tar.gz /usr/local/java/ #set java environment path ENV JAVA_HOME /usr/local/java/jdk1.8.0_171 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATH EXPOSE 80 CMD echo $MYPATH CMD echo \"success--------------ok\" CMD /bin/bash 2、构建 # docker build -t 新镜像名字:TAG . docker build -t centosjava8:1.5 . 注意，上面TAG后面有个空格，有个点 3、运行 #docker run -it 新镜像名字:TAG docker run -it centosjava8:1.5 /bin/bash 删除虚悬镜像 docker image ls -f dangling=true Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_network.html":{"url":"cloud_learn/docker/docker_network.html","title":"Docker网络","keywords":"","body":"Docker网络 是什么 docker不启动，默认网络情况下网卡为 ens33/eth0 lo virbro virbro说明： 在CentOS7的安装过程中如果有选择相关虚拟化的的服务安装系统后，启动网卡时会发现有一个以网桥连接的私网地址的virbr0网卡(virbr0网卡：它还有一个固定的默认IP地址192.168.122.1)，是做虚拟机网桥的使用的，其作用是为连接其上的虚机网卡提供 NAT访问外网的功能。 docker启动后，网络情况 常用基本命令 查看网络 docker netwokr ls 查看网络源数据 [root@wjh ~]# docker network inspect bridge 创建、删除网络 [root@wjh ~]# docker network create test_network [root@wjh ~]# docker network ls [root@wjh ~]# docker network rm test_network 能干嘛 1、容器间的互联和通信以及端口映射 2、容器IP变动时候可以通过服务名直接网络通信而不受到影响 网络模式 网络模式 bridge模式：使用--network bridge指定，默认使用docker0 host模式：使用--network host指定 none模式：使用--network none指定 container模式：使用--network container:NAME或者容器ID指定 bridge ocker 服务默认会创建一个 docker0 网桥（其上有一个 docker0 内部接口），该桥接网络的名称为docker0，它在内核层连通了其他的物理或虚拟网卡，这就将所有容器和本地主机都放到同一个物理网络。Docker 默认指定了 docker0 接口 的 IP 地址和子网掩码，让主机和容器之间可以通过网桥相互通信。 查看 bridge 网络的详细信息，并通过 grep 获取名称项 [root@wjh ~]# docker network inspect bridge | grep name \"com.docker.network.bridge.name\": \"docker0\", [root@wjh ~]# ifconfig | grep docker docker0: flags=4099 mtu 1500 [root@wjh ~]# 1、Docker使用Linux桥接，在宿主机虚拟一个Docker容器网桥(docker0)，Docker启动一个容器时会根据Docker网桥的网段分配给容器一个IP地址，称为Container-IP，同时Docker网桥是每个容器的默认网关。因为在同一宿主机内的容器都接入同一个网桥，这样容器之间就能够通过容器的Container-IP直接通信。 2、docker run 的时候，没有指定network的话默认使用的网桥模式就是bridge，使用的就是docker0。在宿主机ifconfig,就可以看到docker0和自己create的network(后面讲)eth0，eth1，eth2……代表网卡一，网卡二，网卡三……，lo代表127.0.0.1，即localhost，inet addr用来表示网卡的IP地址 3、网桥docker0创建一对对等虚拟设备接口一个叫veth，另一个叫eth0，成对匹配。 3.1 整个宿主机的网桥模式都是docker0，类似一个交换机有一堆接口，每个接口叫veth，在本地主机和容器内分别创建一个虚拟接口，并让他们彼此联通（这样一对接口叫veth pair）； 3.2 每个容器实例内部也有一块网卡，每个接口叫eth0； 3.3 docker0上面的每个veth匹配某个容器实例内部的eth0，两两配对，一一匹配。 通过上述，将宿主机上的所有容器都连接到这个内部网络上，两个容器在同一个网络下,会从这个网关下各自拿到分配的ip，此时两个容器的网络是互通的。 案例 docker run -d -p 8081:8080 --name tomcat81 billygoo/tomcat8-jdk8 docker run -d -p 8082:8080 --name tomcat82 billygoo/tomcat8-jdk8 host 直接使用宿主机的 IP 地址与外界进行通信，不再需要额外进行NAT 转换。 docker run -d -p 8083:8080 --network host --name tomcat83 billygoo/tomcat8-jdk8 问题： docke启动时总是遇见标题中的警告 原因： docker启动时指定--network=host或-net=host，如果还指定了-p映射端口，那这个时候就会有此警告， 并且通过-p设置的参数将不会起到任何作用，端口号会以主机端口号为主，重复时则递增。 解决: 解决的办法就是使用docker的其他网络模式，例如--network=bridge，这样就可以解决问题，或者直接无视 docker run -d --network host --name tomcat84 billygoo/tomcat8-jdk8 在浏览器访问容器内的tomcat83看到访问成功，因为此时容器的IP借用主机的，所以容器共享宿主机网络IP，这样的好处是外部主机与容器可以直接通信。 node 禁用网络功能，只有lo标识(就是127.0.0.1表示本地回环) docker run -d -p 8084:8080 --network none --name tomcat84 billygoo/tomcat8-jdk8 container 新建的容器和已经存在的一个容器共享一个网络ip配置而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。 案例 docker run -it --name alpine1 alpine /bin/sh docker run -it --network container:alpine1 --name alpine2 alpine /bin/sh 当alpine退出后再看alpine2的IP只剩下lo 自定义网络 自定义桥接网络,自定义网络默认使用的是桥接网络bridge 案例 创建网络 docker create wjh_network docker network ls 新建容器加入上一步新建的自定义网络 docker run -d -p 8081:8080 --network wjh_network --name tomcat81 billygoo/tomcat8-jdk8 docker run -d -p 8082:8080 --network wjh_network --name tomcat82 billygoo/tomcat8-jdk8 自定义网络本身就维护好了主机名和ip的对应关系（ip和域名都能通） 整体说明 从其架构和运行流程来看，Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。 Docker 运行的基本流程为： 1 用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求给后者。 2 Docker Daemon 作为 Docker 架构中的主体部分，首先提供 Docker Server 的功能使其可以接受 Docker Client 的请求。 3 Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在。 4 Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graph driver将下载镜像以Graph的形式存储。 5 当需要为 Docker 创建网络环境时，通过网络管理驱动 Network driver 创建并配置 Docker 容器网络环境。 6 当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成。 7 Libcontainer是一项独立的容器管理包，Network driver以及Exec driver都是通过Libcontainer来实现具体对容器进行的操作。 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_compose.html":{"url":"cloud_learn/docker/docker_compose.html","title":"容器编排","keywords":"","body":"容器编排 是什么 Compose 是 Docker 公司推出的一个工具软件，可以管理多个 Docker 容器组成一个应用。你需要定义一个 YAML 格式的配置文件docker-compose.yml，写好多个容器之间的调用关系。然后，只要一个命令，就能同时启动/关闭这些容器 能干嘛 docker建议我们每一个容器中只运行一个服务,因为docker容器本身占用资源极少,所以最好是将每个服务单独的分割开来但是这样我们又面临了一个问题？ 如果我需要同时部署好多个服务,难道要每个服务单独写Dockerfile然后在构建镜像,构建容器,这样累都累死了,所以docker官方给我们提供了docker-compose多服务部署的工具 例如要实现一个Web微服务项目，除了Web服务容器本身，往往还需要再加上后端的数据库mysql服务容器，redis服务器，注册中心eureka，甚至还包括负载均衡容器等等。。。。。。 Compose允许用户通过一个单独的docker-compose.yml模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 可以很容易地用一个配置文件定义一个多容器的应用，然后使用一条指令安装这个应用的所有依赖，完成构建。Docker-Compose 解决了容器与容器之间如何管理编排的问题。 安装 # 官网安装方式https://docs.docker.com/compose/install/ curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --version 卸载 SU rm /usr/local/bin/docker-compose Compose核心概念 文件格式：docker-compose.yml 两个要素： 服务（service）:一个个应用容器实例，比如订单微服务、库存微服务、mysql容器、nginx容器或者redis容器 工程（project):由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose使用的三个步骤 1、编写Dockerfile定义各个微服务应用并构建出对应的镜像文件 2、使用 docker-compose.yml 定义一个完整业务单元，安排好整体应用中的各个容器服务。 3、最后，执行docker-compose up命令 来启动并运行整个应用程序，完成一键部署上线 Compose常用命令 docker-compose -h # 查看帮助 docker-compose up # 启动所有docker-compose服务 docker-compose up -d # 启动所有docker-compose服务并后台运行 docker-compose down # 停止并删除容器、网络、卷、镜像。 docker-compose exec yml里面的服务id # 进入容器实例内部 docker-compose exec docker-compose.yml文件中写的服务id /bin/bash docker-compose ps # 展示当前docker-compose编排过的运行的所有容器 docker-compose top # 展示当前docker-compose编排过的容器进程 docker-compose logs yml里面的服务id # 查看容器输出日志 docker-compose config # 检查配置 docker-compose config -q # 检查配置，有问题才有输出 docker-compose restart # 重启服务 docker-compose start # 启动服务 docker-compose stop # 停止服务 案例1 version: \"3\" services: microService: image: zzyy_docker:1.6 container_name: ms01 ports: - \"6001:6001\" volumes: - /app/microService:/data networks: - atguigu_net depends_on: - redis - mysql redis: image: redis:6.0.8 ports: - \"6379:6379\" volumes: - /app/redis/redis.conf:/etc/redis/redis.conf - /app/redis/data:/data networks: - atguigu_net command: redis-server /etc/redis/redis.conf mysql: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: '123456' MYSQL_ALLOW_EMPTY_PASSWORD: 'no' MYSQL_DATABASE: 'db2021' MYSQL_USER: 'zzyy' MYSQL_PASSWORD: 'zzyy123' ports: - \"3306:3306\" volumes: - /app/mysql/db:/var/lib/mysql - /app/mysql/conf/my.cnf:/etc/my.cnf - /app/mysql/init:/docker-entrypoint-initdb.d networks: - atguigu_net command: --default-authentication-plugin=mysql_native_password #解决外部无法访问 networks: atguigu_net: Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/docker_cig.html":{"url":"cloud_learn/docker/docker_cig.html","title":"Docker容器监控之CAdvisor+InfluxDB+Granfana","keywords":"","body":"Docker容器监控之CAdvisor+InfluxDB+Granfana 是什么 CAdvisor监控收集+InfluxDB存储数据+Granfana展示图表 安装 新建目录，创建文件 [root@wjh ~]# mkdir /mydocker/cig -p [root@wjh ~]# cd /mydocker/cig/ [root@wjh cig]# touch docker-compose.yml [root@wjh cig]# vim docker-compose.yml version: '3.1' volumes: grafana_data: {} services: influxdb: image: tutum/influxdb:0.9 restart: always environment: - PRE_CREATE_DB=cadvisor ports: - \"8083:8083\" - \"8086:8086\" volumes: - ./data/influxdb:/data cadvisor: image: google/cadvisor links: - influxdb:influxsrv command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 restart: always ports: - \"8080:8080\" volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro grafana: user: \"104\" image: grafana/grafana user: \"104\" restart: always links: - influxdb:influxsrv ports: - \"3000:3000\" volumes: - grafana_data:/var/lib/grafana environment: - HTTP_USER=admin - HTTP_PASS=admin - INFLUXDB_HOST=influxsrv - INFLUXDB_PORT=8086 - INFLUXDB_NAME=cadvisor - INFLUXDB_USER=root - INFLUXDB_PASS=root 启动docker-compose文件 docker-compose up 浏览cAdvisor收集服务，http://ip:8080/ 浏览influxdb存储服务，http://ip:8083/ 浏览grafana展现服务，http://ip:3000 ip+3000端口的方式访问,默认帐户密码（admin/admin） 配置步骤 1、配置数据源 2、选择influxdb数据源 3、配置面板panel 选择图表 配置数据 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/docker/go_install.html":{"url":"cloud_learn/docker/go_install.html","title":"go环境安装","keywords":"","body":"go环境安装 https://golang.google.cn/dl/ go env查看环境 创建GOPATH目录 l src：存放源代码 l pkg：存放依赖包 l bin：存放可执行文件 mdkdir -p $GOPATH/{bin，src，pkg} 修改环境变量 1、创建用户变量gopath 配置国内代理 l GOOS，GOARCH，GOPROXY l 国内用户建议设置 goproxy：export GOPROXY=https://goproxy.cn https://goproxy.io,direct 一些基本命令 build 将文件编译成可执行文件 -- goos=linux go build xx.go 指定环境为linux编译程序 fmt 格式化代码 -- go fmt xxx.go get 下载依赖 isntall 编译并安装包和依赖 test 运行 测试 tool 运行共提供的工具 mod 维护模块 ​ Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/k8s/k8s_install.html":{"url":"cloud_learn/k8s/k8s_install.html","title":"安装K8S","keywords":"","body":"安装K8S 安装docker环境 /bin/bash #移除以前docker相关包 sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine # 配置yum源 yum install -y yum-utils yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #安装dockcer yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6 #启动 systemctl enable docker --now #配置加速器 mkdir -p /etc/docker tee /etc/docker/daemon.json 安装kubeadm 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存) 2 CPU 核或更多 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以) 设置防火墙放行规则 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里了解更多详细信息。 设置不同hostname 开启机器上的某些端口。请参见这里 了解更多详细信息。 内网互信 禁用交换分区。为了保证 kubelet 正常工作，你 必须 禁用交换分区。 永久关闭 1、基础环境 #各个机器设置自己的域名 hostnamectl set-hostname xxxx hostnamectl set-hostname k8s-master hostnamectl set-hostname k8s-node1 hostnamectl set-hostname k8s-node2 # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config #关闭swap swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab #允许 iptables 检查桥接流量 cat 2、安装kubelet、kubeadm、kubectl cat 所有节点都需要执行 kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环 使用kubeadm引导集群 1、下载各个机器需要的镜像 sudo tee ./images.sh 2、初始化主节点 #所有机器添加master域名映射，以下需要修改为自己的 echo \"172.31.0.100 cluster-endpoint\" >> /etc/hosts #主节点初始化 kubeadm init \\ --apiserver-advertise-address=172.31.0.100 \\ --control-plane-endpoint=cluster-endpoint \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.20.9 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=192.168.0.0/16 #所有网络范围不重叠 ##初始化完成后的信息，用于添加节点 Your Kubernetes control-plane has initialized successfully! ##主节点执行如下信息 To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join cluster-endpoint:6443 --token srtl5c.618lmpbu0gpxruhx \\ --discovery-token-ca-cert-hash sha256:9fc2ce76173023817677c5788b11bebfd3e12680f00c6632b84a6e9ac6f8e5c2 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join cluster-endpoint:6443 --token srtl5c.618lmpbu0gpxruhx \\ --discovery-token-ca-cert-hash sha256:9fc2ce76173023817677c5788b11bebfd3e12680f00c6632b84a6e9ac6f8e5c2 设置.kube/config 配置命令在初始化信息中 3、 安装网络组件 curl https://docs.projectcalico.org/manifests/calico.yaml -O kubectl apply -f calico.yaml --pod-network-cid值如果修改了，需要修改 calico.yaml value: \"192.168.0.0/16\" 4、加入node节点 kubeadm join cluster-endpoint:6443 --token srtl5c.618lmpbu0gpxruhx \\ --discovery-token-ca-cert-hash sha256:9fc2ce76173023817677c5788b11bebfd3e12680f00c6632b84a6e9ac6f8e5c2 TOKEN24小时内有效，过期可以使用以下命令重新生成新令牌 kubeadm token create --print-join-command 问题处理 1、如果出现镜像拉取失败imagespullbakcoff的情况可以执行以下命令，查看失败原因，在失败的节点上拉取镜像 kubectl describe poDS -n kube-system calico-node-s26rt 5、验证集群 验证集群节点状态 kubectl get nodes 6、部署dashboard 1、部署 kubernetes官方提供的可视化界面 https://github.com/kubernetes/dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 2、设置访问端口 kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard type: ClusterIP 改为 type: NodePort kubectl get svc -A |grep kubernetes-dashboard ## 找到端口，在安全组放行 访问： https://集群任意IP:端口 https://10.0.0.100:30368 3、创建访问账号 #创建访问账号，准备一个yaml文件； vi dash.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard kubectl apply -f dash.yaml 4、令牌访问 kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{ .data.token | base64decode}}\" eyJhbGciOiJSUzI1NiIsImtpZCI6IklLLUtVV2VvMGE2a1hBT3NMU1JXY3FxTm9MRzNDQUZTQXBkQkVGc2ZRNXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWc5Z3A4Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyOGRmNDQ2Ni03YTM1LTRiM2UtYWFkZC0xZWM3NmE2MDBkZWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.BQcX86qMhgRtI6rK8LK5uhb_IJw-oZpiLkeSEjHngPyMIKhcQ8RfhQ0DuZ0ApoJt_59Qrpc6v2GzUZ8P-pDe3BcA0JV8g3QarYnQ458-LZhzIlCsaVvXFZMLGSA0l08FySXnckIEzdEZzuvsa7Q9aoMhe4eb_DpDmZj-jwEo7gBEVLKjuqdvLhak7BAcrsymVKXOioxZMKVJdglEXNDjBcBfnkbRM4pNKnP6Zp6m_qF2ILMlfB48IJFFDSN2EjAKvxro9mAwDJ98Al48w62phL_V6M-O_0KhCEhN4a2lJtuYuWmdQSQ5zUsDSs0NtCJaH6rb0u6Vf0wmUp_NvuonVA Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/k8s/k8s_base.html":{"url":"cloud_learn/k8s/k8s_base.html","title":"01 Kubernetes 基础入门","keywords":"","body":" Kubernetes 基础入门 一、Kubernetes简介 1、背景 1、部署方式的变迁 传统部署时代： 在物理服务器上运行应用程序 无法为应用程序定义资源边界 导致资源分配问题 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。 一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。 虚拟化部署时代： 作为解决方案，引入了虚拟化 虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM） 虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全 一个应用程序的信息 不能被另一应用程序随意访问。 虚拟化技术能够更好地利用物理服务器上的资源 因为可轻松地添加或更新应用程序 ，所以可以实现更好的可伸缩性，降低硬件成本等等。 每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。 缺点：虚拟层冗余导致的资源浪费与性能下降 容器部署时代： 容器类似于 VM，但可以在应用程序之间共享操作系统（OS）。 容器被认为是轻量级的。 容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。 参照【Docker隔离原理- namespace 6项隔离（资源隔离）与 cgroups 8项资源限制（资源限制）】 裸金属：真正的物理服务器 容器优势： 敏捷性：敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 及时性：持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的 容器镜像构建和部署。 解耦性：关注开发与运维的分离：在构建/发布时创建应用程序容器镜像，而不是在部署时。 从而将应用程序与基础架构分离。 可观测性：可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨平台：跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 可移植：跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 简易性：以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 大分布式：松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 隔离性：资源隔离：可预测的应用程序性能。 高效性：资源利用：高效率和高密度 K8S之前： 10台服务器：25+15中间件 K8S之后： 10台服务器：上百个应用了。 k8s管理10几台服务器。资源规划。 2、容器化问题 弹性的容器化应用管理 强大的故障转移能力 高性能的负载均衡访问机制 便捷的扩展 自动化的资源监测 ...... docker swarm：大规模进行容器编排 mesos：apache Kubernetes : google； 竞品： Kubernetes 胜利 3、为什么用 Kubernetes 容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？ 这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。linux之上的一个服务编排框架； Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 为你提供： 服务发现和负载均衡 Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排 Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚 你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自动完成装箱计算 Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 自我修复 Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理 Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥 ....... 为了生产环境的容器化大规模应用编排，必须有一个自动化的框架。系统 4、市场份额 1、容器化 docker swarm 2、服务编排 google --- kubernetes --- 发起cncf --- 众多的项目辅佐 kubernetes ---- kubernetes +cncf其他软件 = 整个大型云平台 2、简介 Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。 名称 Kubernetes 源于希腊语，意为“舵手”或“飞行员”。Google 在 2014 年开源了 Kubernetes 项目。 Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验 的基础上，结合了社区中最好的想法和实践。 1、Kubernetes不是什么 Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。 Kubernetes 在容器级别而不是在硬件级别运行 它提供了 PaaS 产品共有的一些普遍适用的功能， 例如部署、扩展、负载均衡、日志记录和监视。 但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。 Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。 Kubernetes： 不限制支持的应用程序类型。 Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。 不部署源代码，也不构建你的应用程序。 持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。 不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、 数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统 （例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如， 开放服务代理）来访问。 不要求日志记录、监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API， 该声明性 API 可以由任意形式的声明性规范所构成。RESTful；写yaml文件 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 相比之下，Kubernetes 包含一组独立的、可组合的控制过程， 这些过程连续地将当前状态驱动到所提供的所需状态。 如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用 且功能更强大、系统更健壮、更为弹性和可扩展。 容器管家： 安装了很多应用。 ------------------------- qq电脑管家。（自动杀垃圾，自动卸载没用东西....） 机器上有很多容器。 -------------------------- kubernete容器的管家。（容器的启动停止、故障转义、负载均衡等） 二、Kubernetes安装 1、集群原理 集群： 主从： 主从同步/复制 ;mysql 主 -- mysql 从 主管理从 v 分片（数据集群）： 大家都一样 每个人存一部分东西 1、master-node 架构 11000台机器 地主+奴隶 地（机器） 奴隶（在机器上干活） master：主节点（地主）。可能有很多（多人控股公司） node：work节点（工作节点）。 很多。真正干应用的活 master 和 worker怎么交互 master决定worker里面都有什么 worker只是和master （API） 通信； 每一个节点自己干自己的活 程序员使用UI或者CLI操作k8s集群的master，就可以知道整个集群的状况。 2、工作原理 master节点（Control Plane【控制面板】）：master节点控制整个集群 master节点上有一些核心组件： Controller Manager：控制管理器 etcd：键值数据库（redis）【记账本，记事本】 scheduler：调度器 api server：api网关（所有的控制都需要通过api-server） node节点（worker工作节点）： kubelet（监工）：每一个node节点上必须安装的组件。 kube-proxy：代理。代理网络 部署一个应用？ 程序员：调用CLI告诉master，我们现在要部署一个tomcat应用 程序员的所有调用都先去master节点的网关api-server。这是matser的唯一入口（mvc模式中的c层） 收到的请求先交给master的api-server。由api-server交给controller-mannager进行控制 controller-mannager 进行 应用部署 controller-mannager 会生成一次部署信息。 tomcat --image:tomcat6 --port 8080 ,真正不部署应用 部署信息被记录在etcd中 scheduler调度器从etcd数据库中，拿到要部署的应用，开始调度。看哪个节点合适， scheduler把算出来的调度信息再放到etcd中 每一个node节点的监控kubelet，随时和master保持联系的（给api-server发送请求不断获取最新数据），所有节点的kubelet就会从master 假设node2的kubelet最终收到了命令，要部署。 kubelet就自己run一个应用在当前机器上，随时给master汇报当前应用的状态信息，分配ip node和master是通过master的api-server联系的 每一个机器上的kube-proxy能知道集群的所有网络。只要node访问别人或者别人访问node，node上的kube-proxy网络代理自动计算进行流量转发 下图和上图一样的，再理解一下 无论访问哪个机器，都可以访问到真正应用（Service【服务】） 3、原理分解 1、主节点（master） 快速介绍： master也要装kubelet和kubeproxy 前端访问（UI\\CLI）： kube-apiserver： scheduler: controller manager: etcd kubelet+kubeproxy每一个节点的必备+docker（容器运行时环境） 2、工作节点（node） 快速介绍： Pod： docker run 启动的是一个container（容器），容器是docker的基本单位，一个应用是一个容器 kubelet run 启动的一个应用称为一个Pod；Pod是k8s的基本单位。 Pod是容器的一个再封装 atguigu(永远不变) ==slf4j= log4j(类) 应用 ===== ==Pod== ======= docker的容器 一个容器往往代表不了一个基本应用。博客（php+mysql合起来完成） 准备一个Pod 可以包含多个 container；一个Pod代表一个基本的应用。 IPod（看电影、听音乐、玩游戏）【一个基本产品，原子】； Pod（music container、movie container）【一个基本产品，原子的】 Kubelet：监工，负责交互master的api-server以及当前机器的应用启停等，在master机器就是master的小助手。每一台机器真正干活的都是这个 Kubelet Kube-proxy： 其他： 2、组件交互原理 想让k8s部署一个tomcat？ 0、开机默认所有节点的kubelet、master节点的scheduler（调度器）、controller-manager（控制管理器）一直监听master的api-server发来的事件变化（for ::） 1、程序员使用命令行工具： kubectl ； kubectl create deploy tomcat --image=tomcat8（告诉master让集群使用tomcat8镜像，部署一个tomcat应用） 2、kubectl命令行内容发给api-server，api-server保存此次创建信息到etcd 3、etcd给api-server上报事件，说刚才有人给我里面保存一个信息。（部署Tomcat[deploy]） 4、controller-manager监听到api-server的事件，是 （部署Tomcat[deploy]） 5、controller-manager 处理这个 （部署Tomcat[deploy]）的事件。controller-manager会生成Pod的部署信息【pod信息】 6、controller-manager 把Pod的信息交给api-server，再保存到etcd 7、etcd上报事件【pod信息】给api-server。 8、scheduler专门监听 【pod信息】 ，拿到 【pod信息】的内容，计算，看哪个节点合适部署这个Pod【pod调度过后的信息（node: node-02）】， 9、scheduler把 【pod调度过后的信息（node: node-02）】交给api-server保存给etcd 10、etcd上报事件【pod调度过后的信息（node: node-02）】，给api-server 11、其他节点的kubelet专门监听 【pod调度过后的信息（node: node-02）】 事件，集群所有节点kubelet从api-server就拿到了 【pod调度过后的信息（node: node-02）】 事件 12、每个节点的kubelet判断是否属于自己的事情；node-02的kubelet发现是他的事情 13、node-02的kubelet启动这个pod。汇报给master当前启动好的所有信息 3、安装 1、理解 安装方式 二进制方式（建议生产环境使用） MiniKube..... kubeadm引导方式（官方推荐） GA 大致流程 准备N台服务器，内网互通， 安装Docker容器化环境【k8s放弃dockershim】 安装Kubernetes 三台机器安装核心组件（kubeadm(创建集群的引导工具), kubelet，kubectl（程序员用的命令行） ） kubelet可以直接通过容器化的方式创建出之前的核心组件（api-server）【官方把核心组件做成镜像】 由kubeadm引导创建集群 2、执行 1、准备机器 开通三台机器，内网互通，配置公网ip。centos7.8/7.9，基础实验2c4g三台也可以 每台机器的hostname不要用localhost，可用k8s-01，k8s-02，k8s-03之类的【不包含下划线、小数点、大写字母】（这个后续步骤也可以做） 2、安装前置环境（都执行） 1、基础环境 ######################################################################### #关闭防火墙： 如果是云服务器，需要设置安全组策略放行端口 # https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports systemctl stop firewalld systemctl disable firewalld # 修改 hostname hostnamectl set-hostname k8s-01 # 查看修改结果 hostnamectl status # 设置 hostname 解析 echo \"127.0.0.1 $(hostname)\" >> /etc/hosts #关闭 selinux： sed -i 's/enforcing/disabled/' /etc/selinux/config setenforce 0 #关闭 swap： swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab #允许 iptables 检查桥接流量 #https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%85%81%E8%AE%B8-iptables-%E6%A3%80%E6%9F%A5%E6%A1%A5%E6%8E%A5%E6%B5%81%E9%87%8F ## 开启br_netfilter ## sudo modprobe br_netfilter ## 确认下 ## lsmod | grep br_netfilter ## 修改配置 #####这里用这个，不要用课堂上的配置。。。。。。。。。 #将桥接的 IPv4 流量传递到 iptables 的链： # 修改 /etc/sysctl.conf # 如果有配置，则修改 sed -i \"s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward=1#g\" /etc/sysctl.conf sed -i \"s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables=1#g\" /etc/sysctl.conf sed -i \"s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables=1#g\" /etc/sysctl.conf sed -i \"s#^net.ipv6.conf.all.disable_ipv6.*#net.ipv6.conf.all.disable_ipv6=1#g\" /etc/sysctl.conf sed -i \"s#^net.ipv6.conf.default.disable_ipv6.*#net.ipv6.conf.default.disable_ipv6=1#g\" /etc/sysctl.conf sed -i \"s#^net.ipv6.conf.lo.disable_ipv6.*#net.ipv6.conf.lo.disable_ipv6=1#g\" /etc/sysctl.conf sed -i \"s#^net.ipv6.conf.all.forwarding.*#net.ipv6.conf.all.forwarding=1#g\" /etc/sysctl.conf # 可能没有，追加 echo \"net.ipv4.ip_forward = 1\" >> /etc/sysctl.conf echo \"net.bridge.bridge-nf-call-ip6tables = 1\" >> /etc/sysctl.conf echo \"net.bridge.bridge-nf-call-iptables = 1\" >> /etc/sysctl.conf echo \"net.ipv6.conf.all.disable_ipv6 = 1\" >> /etc/sysctl.conf echo \"net.ipv6.conf.default.disable_ipv6 = 1\" >> /etc/sysctl.conf echo \"net.ipv6.conf.lo.disable_ipv6 = 1\" >> /etc/sysctl.conf echo \"net.ipv6.conf.all.forwarding = 1\" >> /etc/sysctl.conf # 执行命令以应用 sysctl -p ################################################################# 2、docker环境 sudo yum remove docker* sudo yum install -y yum-utils #配置docker yum 源 sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #安装docker 19.03.9 yum install -y docker-ce-3:19.03.9-3.el7.x86_64 docker-ce-cli-3:19.03.9-3.el7.x86_64 containerd.io #安装docker 19.03.9 docker-ce 19.03.9 yum install -y docker-ce-19.03.9-3 docker-ce-cli-19.03.9 containerd.io #启动服务 systemctl start docker systemctl enable docker #配置加速 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json 3、安装k8s核心（都执行） # 配置K8S的yum源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 卸载旧版本 yum remove -y kubelet kubeadm kubectl # 查看可以安装的版本 yum list kubelet --showduplicates | sort -r # 安装kubelet、kubeadm、kubectl 指定版本 yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0 # 开机启动kubelet systemctl enable kubelet && systemctl start kubelet 4、初始化master节点（master执行） ############下载核心镜像 kubeadm config images list：查看需要哪些镜像########### ####封装成images.sh文件 #!/bin/bash images=( kube-apiserver:v1.21.0 kube-proxy:v1.21.0 kube-controller-manager:v1.21.0 kube-scheduler:v1.21.0 coredns:v1.8.0 etcd:3.4.13-0 pause:3.4.1 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName done #####封装结束 chmod +x images.sh && ./images.sh # registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns:v1.8.0 ##注意1.21.0版本的k8s coredns镜像比较特殊，结合阿里云需要特殊处理，重新打标签 docker tag registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns:v1.8.0 registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns/coredns:v1.8.0 ########kubeadm init 一个master######################## ########kubeadm join 其他worker######################## kubeadm init \\ --apiserver-advertise-address=10.170.11.8 \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.21.0 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=192.168.0.0/16 ## 注意：pod-cidr与service-cidr # cidr 无类别域间路由（Classless Inter-Domain Routing、CIDR） # 指定一个网络可达范围 pod的子网范围+service负载均衡网络的子网范围+本机ip的子网范围不能有重复域 ######按照提示继续###### ## init完成后第一步：复制相关文件夹 To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ## 导出环境变量 Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf ### 部署一个pod网络 You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ ##############如下：安装calico##################### kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml ### 命令检查 kubectl get pod -A ##获取集群中所有部署好的应用Pod kubectl get nodes ##查看集群所有机器的状态 Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.24.80.222:6443 --token nz9azl.9bl27pyr4exy2wz4 \\ --discovery-token-ca-cert-hash sha256:4bdc81a83b80f6bdd30bb56225f9013006a45ed423f131ac256ffe16bae73a20 5、初始化worker节点（worker执行） ## 用master生成的命令即可 kubeadm join 172.24.80.222:6443 --token nz9azl.9bl27pyr4exy2wz4 \\ --discovery-token-ca-cert-hash sha256:4bdc81a83b80f6bdd30bb56225f9013006a45ed423f131ac256ffe16bae73a20 ##过期怎么办 kubeadm token create --print-join-command kubeadm token create --ttl 0 --print-join-command kubeadm join --token y1eyw5.ylg568kvohfdsfco --discovery-token-ca-cert-hash sha256: 6c35e4f73f72afd89bf1c8c303ee55677d2cdb1342d67bb23c852aba2efc7c73 6、验证集群 #获取所有节点 kubectl get nodes #给节点打标签 ## k8s中万物皆对象。node:机器 Pod：应用容器 ###加标签 《h1》 kubectl label node k8s-02 node-role.kubernetes.io/worker='' ###去标签 kubectl label node k8s-02 node-role.kubernetes.io/worker- ## k8s集群，机器重启了会自动再加入集群，master重启了会自动再加入集群控制中心 7、设置ipvs模式 k8s整个集群为了访问通；默认是用iptables,性能下（kube-proxy在集群之间同步iptables的内容） #1、查看默认kube-proxy 使用的模式 kubectl logs -n kube-system kube-proxy-28xv4 #2、需要修改 kube-proxy 的配置文件,修改mode 为ipvs。默认iptables，但是集群大了以后就很慢 kubectl edit cm kube-proxy -n kube-system 修改如下 ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: \"\" strictARP: false syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \"ipvs\" ###修改了kube-proxy的配置，为了让重新生效，需要杀掉以前的Kube-proxy kubectl get pod -A|grep kube-proxy kubectl delete pod kube-proxy-pqgnt -n kube-system ### 修改完成后可以重启kube-proxy以生效 8、让其他客户端kubelet也能操作集群 #1、master获取管理员配置 cat /etc/kubernetes/admin.conf #2、其他节点创建保存 vi ~/.kube/config #3、重新测试使用 4、急速安装方式 1、三台机器设置自己的hostname（不能是localhost）。云厂商注意三台机器一定要通。 青云需要额外设置组内互信 阿里云默认是通的 虚拟机，关闭所有机器的防火墙 # 修改 hostname; k8s-01要变为自己的hostname hostnamectl set-hostname k8s-01 # 设置 hostname 解析 echo \"127.0.0.1 $(hostname)\" >> /etc/hosts 2、所有机器批量执行如下脚本 #先在所有机器执行 vi k8s.sh # 进入编辑模式（输入i），把如下脚本复制 # 所有机器给脚本权限 chmod +x k8s.sh #执行脚本 ./k8s.sh #/bin/sh #######################开始设置环境##################################### \\n printf \"##################正在配置所有基础环境信息################## \\n\" printf \"##################关闭selinux################## \\n\" sed -i 's/enforcing/disabled/' /etc/selinux/config setenforce 0 printf \"##################关闭swap################## \\n\" swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab printf \"##################配置路由转发################## \\n\" cat > /etc/sysctl.d/k8s.conf ## 必须 ipv6流量桥接 echo 'net.bridge.bridge-nf-call-ip6tables = 1' >> /etc/sysctl.d/k8s.conf ## 必须 ipv4流量桥接 echo 'net.bridge.bridge-nf-call-iptables = 1' >> /etc/sysctl.d/k8s.conf echo \"net.ipv6.conf.all.disable_ipv6 = 1\" >> /etc/sysctl.d/k8s.conf echo \"net.ipv6.conf.default.disable_ipv6 = 1\" >> /etc/sysctl.d/k8s.conf echo \"net.ipv6.conf.lo.disable_ipv6 = 1\" >> /etc/sysctl.d/k8s.conf echo \"net.ipv6.conf.all.forwarding = 1\" >> /etc/sysctl.d/k8s.conf modprobe br_netfilter sudo sysctl --system printf \"##################配置ipvs################## \\n\" cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF ###指定k8s安装版本 yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0 ###要把kubelet立即启动。 systemctl enable kubelet systemctl start kubelet printf \"##################下载api-server等核心镜像################## \\n\" sudo tee ./images.sh 3、使用kubeadm引导集群（参照初始化master继续做） #### --apiserver-advertise-address 的地址一定写成自己master机器的ip地址 #### 虚拟机或者其他云厂商给你的机器ip 10.96 192.168 #### 以下的只在master节点执行 kubeadm init \\ --apiserver-advertise-address=10.170.11.8 \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.21.0 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=192.168.0.0/16 4、master结束以后，按照控制台引导继续往下 ## 第一步 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ##第二步 export KUBECONFIG=/etc/kubernetes/admin.conf ##第三步 部署网络插件 kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml ##第四步，用控制台打印的kubeadm join 去其他node节点执行 kubeadm join 10.170.11.8:6443 --token cnb7x2.lzgz7mfzcjutn0nk \\ --discovery-token-ca-cert-hash sha256:00c9e977ee52632098aadb515c90076603daee94a167728110ef8086d0d5b37d 5、验证集群 #等一会，在master节点执行 kubectl get nodes 6、设置kube-proxy的ipvs模式 ##修改kube-proxy默认的配置 kubectl edit cm kube-proxy -n kube-system ## 修改mode: \"ipvs\" ##改完以后重启kube-proxy ### 查到所有的kube-proxy kubectl get pod -n kube-system |grep kube-proxy ### 删除之前的即可 kubectl delete pod 【用自己查出来的kube-proxy-dw5sf kube-proxy-hsrwp kube-proxy-vqv7n】 -n kube-system ### 三、Kubernetes基础入门 以下的所有都先进行基本理解，我们后来会一一详细讲解 0、基础知识 以上展示了一个master（主节点）和6个worker（工作节点）的k8s集群 # docker run --name hello-pod alpine 是跑一个容器，容器的粒度有点小 kubectl run hello-pod --image=alpine #跑一个Pod。Pod里面其实也是容器 # kubectl get pod #以前的docker ps -a ## 所有kubectl在master节点运行，把命令请求发给api-server。api-server一系列处理 ## master只负责调度，而worker node才是真正部署应用的。 docker是每一个worker节点的运行时环境 kubelet负责控制所有容器的启动停止，保证节点工作正常，已经帮助节点交互master master节点的关键组件： kubelet（监工）：所有节点必备的。控制这个节点所有pod的生命周期以及与api-server交互等工作 kube-api-server：负责接收所有请求。集群内对集群的任何修改都是通过命令行、ui把请求发给api-server才能执行的。api-server是整个集群操作对内、对外的唯一入口。不包含我们后来部署应用暴露端口的方式 kube-proxy：整个节点的网络流量负责 cri：都有容器运行时环境 worker节点： kubelet（监工）：所有节点必备的。控制这个节点所有pod的生命周期以及与api-server交互等工作 kube-proxy：整个节点的网络流量负责 cri：都有容器运行时环境 1、部署一个应用 创建一次部署工作。(自愈机制) kubectl create deploy xxxxxx ：命令行会给api-server发送要部署xxx的请求 api-server把这个请求保存到etcd # kubectl create 帮我们创建k8s集群中的一些对象 kubectl create --help kubectl create deployment 这次部署的名字 --image=应用的镜像 #Create a deployment named my-nginx that runs the nginx image kubectl create deployment my-nginx --image=nginx ##最终在一个机器上有pod、这个pod其实本质里面就是一个容器 k8s_nginx_my-nginx-6b74b79f57-snlr4_default_dbeac79e-1ce9-42c9-bc59-c8ca0412674b_0 ### k8s_镜像(nginx)_pod名(my-nginx-6b74b79f57-snlr4)_容器名(default_dbeac79e-1ce9-42c9-bc59-c8ca0412674b_0) # Create a deployment with command kubectl create deployment my-nginx --image=nginx -- date # Create a deployment named my-nginx that runs the nginx image with 3 replicas. kubectl create deployment my-nginx --image=nginx --replicas=3 # Create a deployment named my-nginx that runs the nginx image and expose port 80. kubectl create deployment my-nginx --image=nginx --port=80 Deployment（部署） 在k8s中，通过发布 Deployment，可以创建应用程序 (docker image) 的实例 (docker container)，这个实例会被包含在称为 Pod 的概念中，Pod 是 k8s 中最小可管理单元。 在 k8s 集群中发布 Deployment 后，Deployment 将指示 k8s 如何创建和更新应用程序的实例，master 节点将应用程序实例调度到集群中的具体的节点上。 创建应用程序实例后，Kubernetes Deployment Controller 会持续监控这些实例。如果运行实例的 worker 节点关机或被删除，则 Kubernetes Deployment Controller 将在群集中资源最优的另一个 worker 节点上重新创建一个新的实例。这提供了一种自我修复机制来解决机器故障或维护问题。 在容器编排之前的时代，各种安装脚本通常用于启动应用程序，但是不能够使应用程序从机器故障中恢复。通过创建应用程序实例并确保它们在集群节点中的运行实例个数，Kubernetes Deployment 提供了一种完全不同的方式来管理应用程序。 Deployment 处于 master 节点上，通过发布 Deployment，master 节点会选择合适的 worker 节点创建 Container（即图中的正方体），Container 会被包含在 Pod （即蓝色圆圈）里。 自愈：针对使用Deployment等部署的应用。 kubectl run ：直接启动一个pod； 不会产生一次部署信息。所以删除就没 kubectl create deploy： 启动一个Pod，以及记录这次部署信息。所以，这个pod即使挂了，这次部署信息有，就会强制同步到这次部署信息期望的最终结果；kubectl get deploy,pod 都有内容 2、应用程序探索 了解Kubernetes Pods（容器组） 了解Kubernetes Nodes（节点） 排查故障 创建 Deployment 后，k8s创建了一个 Pod（容器组） 来放置应用程序实例（container 容器）。 1、了解Pod Pod （容器组） 是一个k8s中一个抽象的概念，用于存放一组 container（可包含一个或多个 container 容器，即图上正方体)，以及这些 container （容器）的一些共享资源。这些资源包括： 共享存储，称为卷(Volumes)，即图上紫色圆柱 网络，每个 Pod（容器组）在集群中有个唯一的 IP，pod（容器组）中的 container（容器）共享该IP地址 container（容器）的基本信息，例如容器的镜像版本，对外暴露的端口等 Pod（容器组）是 k8s 集群上的最基本的单元。当我们在 k8s 上创建 Deployment 时，会在集群上创建包含容器的 Pod (而不是直接创建容器)。每个Pod都与运行它的 worker 节点（Node）绑定，并保持在那里直到终止或被删除。如果节点（Node）发生故障，则会在群集中的其他可用节点（Node）上运行相同的 Pod（从同样的镜像创建 Container，使用同样的配置，IP 地址不同，Pod 名字不同）。 TIP 重要： Pod 是一组容器（可包含一个或多个应用程序容器），以及共享存储（卷 Volumes）、IP 地址和有关如何运行容器的信息。 如果多个容器紧密耦合并且需要共享磁盘等资源，则他们应该被部署在同一个Pod（容器组）中。 2、了解Node Pod（容器组）总是在 Node（节点） 上运行。Node（节点）是 kubernetes 集群中的计算机，可以是虚拟机或物理机。每个 Node（节点）都由 master 管理。一个 Node（节点）可以有多个Pod（容器组），kubernetes master 会根据每个 Node（节点）上可用资源的情况，自动调度 Pod（容器组）到最佳的 Node（节点）上。 每个 Kubernetes Node（节点）至少运行： Kubelet，负责 master 节点和 worker 节点之间通信的进程；管理 Pod（容器组）和 Pod（容器组）内运行的 Container（容器）。 kube-proxy，负责进行流量转发 容器运行环境（如Docker）负责下载镜像、创建和运行容器等。 Kubelet启动的Pod每个都有Ip，全集群任意位置均可访问 kubeadm init \\ --apiserver-advertise-address=10.170.11.8 \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.21.0 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=192.168.0.0/16 --pod-network-cidr=192.168.0.0/16：pod 的ip范围 calico：网络组件: 【扁平化网络】 3、故障排除 kubectl get - 显示资源列表 # kubectl get 资源类型 #获取类型为Deployment的资源列表 kubectl get deployments #获取类型为Pod的资源列表 kubectl get pods #获取类型为Node的资源列表 kubectl get nodes # 查看所有名称空间的 Deployment kubectl get deployments -A kubectl get deployments --all-namespaces # 查看 kube-system 名称空间的 Deployment kubectl get deployments -n kube-system #####并不是所有的对象都在名称空间中 # 在名称空间里 kubectl api-resources --namespaced=true # 不在名称空间里 kubectl api-resources --namespaced=false kubectl describe - 显示有关资源的详细信息 # kubectl describe 资源类型 资源名称 #查看名称为nginx-XXXXXX的Pod的信息 kubectl describe pod nginx-XXXXXX #查看名称为nginx的Deployment的信息 kubectl describe deployment my-nginx kubectl logs - 查看pod中的容器的打印日志（和命令docker logs 类似） # kubectl logs Pod名称 #查看名称为nginx-pod-XXXXXXX的Pod内的容器打印的日志 #本案例中的 nginx-pod 没有输出日志，所以您看到的结果是空的 kubectl logs -f nginx-pod-XXXXXXX kubectl exec - 在pod中的容器环境内执行命令(和命令docker exec 类似) # kubectl exec Pod名称 操作命令 # 在名称为nginx-pod-xxxxxx的Pod中运行bash kubectl exec -it nginx-pod-xxxxxx /bin/bash ### 注意：新版1.21.0 提示这个命令会过期 4、kubectl run 也可以独立跑一个Pod ## kubectl run --help kubectl run nginx --image=nginx 总结： kubectl create 资源 #创建任意资源 kubectl create deploy #创建部署 kubectl run #只创建一个Pod kubectl get 资源名(node/pod/deploy) -n xxx（指定名称空间，默认是default） #获取资源 kubectl describe 资源名(node/pod/deploy) xxx #描述某个资源的详细信息 kubectl logs 资源名 ##查看日志 kubectl exec -it pod名 -- 命令 #进pod并执行命令 kubectl delete 资源名(node/pod/deploy) xxx #删除资源 3、应用外部可见 1、目标 了解 Kubernetes 中的 Service 了解 标签(Label) 和 标签选择器(Label Selector) 对象如何与 Service 关联 在 Kubernetes 集群外用 Service 暴露应用 2、Kubernetes Service 总览 Kubernetes Pod 是转瞬即逝的。 Pod 实际上拥有 生命周期。 当一个工作 Node 挂掉后, 在 Node 上运行的 Pod 也会消亡。 ReplicaSet 会自动地通过创建新的 Pod 驱动集群回到目标状态，以保证应用程序正常运行。 Kubernetes 的 Service 是一个抽象层，它定义了一组 Pod 的逻辑集，并为这些 Pod 支持外部流量暴露、负载平衡和服务发现。 Service 使从属 Pod 之间的松耦合成为可能。 和其他 Kubernetes 对象一样, Service 用 YAML (更推荐) 或者 JSON 来定义. Service 下的一组 Pod 通常由 LabelSelector (请参阅下面的说明为什么您可能想要一个 spec 中不包含selector的服务)来标记。 尽管每个 Pod 都有一个唯一的 IP 地址，但是如果没有 Service ，这些 IP 不会暴露在群集外部。Service 允许您的应用程序接收流量。Service 也可以用在 ServiceSpec 标记type的方式暴露 ClusterIP (默认) - 在集群的内部 IP 上公开 Service 。这种类型使得 Service 只能从集群内访问。 NodePort - 使用 NAT 在集群中每个选定 Node 的相同端口上公开 Service 。使用: 从集群外部访问 Service。是 ClusterIP 的超集。 LoadBalancer - 在当前云中创建一个外部负载均衡器(如果支持的话)，并为 Service 分配一个固定的外部IP。是 NodePort 的超集。 ExternalName - 通过返回带有该名称的 CNAME 记录，使用任意名称(由 spec 中的externalName指定)公开 Service。不使用代理。这种类型需要kube-dns的v1.7或更高版本。 3、Service 和 Label Service 通过一组 Pod 路由通信。Service 是一种抽象，它允许 Pod 死亡并在 Kubernetes 中复制，而不会影响应用程序。在依赖的 Pod (如应用程序中的前端和后端组件)之间进行发现和路由是由Kubernetes Service 处理的。 Service 匹配一组 Pod 是使用 标签(Label)和选择器(Selector), 它们是允许对 Kubernetes 中的对象进行逻辑操作的一种分组原语。标签(Label)是附加在对象上的键/值对，可以以多种方式使用: 指定用于开发，测试和生产的对象 嵌入版本标签 使用 Label 将对象进行分类 4、kubectl expose kubectl expose deployment tomcat6 --port=8912 --target-port=8080 --type=NodePort ## --port：集群内访问service的端口 8912 ## --target-port： pod容器的端口 8080 ## --nodePort： 每个机器开发的端口 30403 ## 进行验证 kubectl get svc curl ip:port kubectl expose #暴露，成一个负载均衡网络 ## kubectl exec 进去pod修改，并测试负载均衡 4、伸缩应用程序-扩缩容 目标 用 kubectl 扩缩应用程序 扩缩一个 Deployment 我们创建了一个 Deployment ，然后通过 服务提供访问 Pod 的方式。我们发布的 Deployment 只创建了一个 Pod 来运行我们的应用程序。当流量增加时，我们需要对应用程序进行伸缩操作以满足系统性能需求。 ## 扩展 ## 扩容的Pod会自动加入到他之前存在的Service（负载均衡网络） kubectl scale --replicas=3 deployment tomcat6 #持续观测效果 watch kubectl get pods -o wide 5、执行滚动升级 目标 使用 kubectl 执行滚动更新 滚动更新允许通过使用新的实例逐步更新 Pod 实例从而实现 Deployments 更新，停机时间为零。 与应用程序扩展类似，如果暴露了 Deployment，服务（Service）将在更新期间仅对可用的 pod 进行负载均衡。可用 Pod 是应用程序用户可用的实例。 滚动更新允许以下操作： 将应用程序从一个环境提升到另一个环境（通过容器镜像更新） 回滚到以前的版本 持续集成和持续交付应用程序，无需停机 #应用升级: tomcat:alpine、tomcat:jre8-alpine # kubectl set image deployment/my-nginx2 nginx=nginx:1.9.1 ##联合jenkins 形成持续集成，灰度发布功能 kubectl set image deployment.apps/tomcat6 tomcat=tomcat:jre8-alpine #可以携带--record参数，记录变更 ##回滚升级 ### 查看历史记录 kubectl rollout history deployment.apps/tomcat6 kubectl rollout history deploy tomcat6 ### 回滚到指定版本 kubectl rollout undo deployment.apps/tomcat6 --to-revision=1 kubectl rollout undo deploy tomcat6 --to-revision=1 命令：记的太多 声明式API； 对象描述文件的方式；Pod --》 yaml ， Deploy--》yaml ， Service --》 yaml kubectl apply -f xxx.yaml .； 用文件固化操作。移植性增加 6、以上用配置文件方式 1、部署一个应用 apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本 kind: Deployment #该配置的类型，我们使用的是 Deployment metadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签 spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: nginx:1.7.9 #使用镜像nginx:1.7.9创建container，该container默认80端口可访问 kubectl apply -f xxx.yaml 2、暴露应用 apiVersion: v1 kind: Service metadata: name: nginx-service #Service 的名称 labels: #Service 自己的标签 app: nginx #为该 Service 设置 key 为 app，value 为 nginx 的标签 spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: nginx #选择包含标签 app:nginx 的 Pod ports: - name: nginx-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 3、扩缩容 修改deployment.yaml 中的 replicas 属性即可 完成后运行 kubectl apply -f xxx.yaml 4、滚动升级 修改deployment.yaml 中的 imageName 属性等 完成后运行 kubectl apply -f xxx.yaml 以上都可以直接 kubectl edit deploy/service 等，修改完成后自动生效 四、其他 1、查看Kubernetes适配的docker版本 https://github.com/kubernetes/kubernetes/releases 查看他的changelog，搜索适配的docker版本即可。 2、弃用dockershim的问题 https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/ 使用containerd： https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#containerd 配置docker：https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker 3、部署dashboard https://github.com/kubernetes/dashboard type: NodePort #访问测试 每次访问都需要令牌 kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') 需要在下载来的文件中改这个 ### 运行这个给个权限 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard 4、master初始化的日志 [root@i-iqrlgkwc ~]# kubeadm init \\ > --apiserver-advertise-address=10.170.11.8 \\ > --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ > --kubernetes-version v1.21.0 \\ > --service-cidr=10.96.0.0/16 \\ > --pod-network-cidr=192.168.0.0/16 [init] Using Kubernetes version: v1.21.0 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.170.11.8] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-01 localhost] and IPs [10.170.11.8 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-01 localhost] and IPs [10.170.11.8 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [kubelet-check] Initial timeout of 40s passed. [apiclient] All control plane components are healthy after 66.504822 seconds [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.21\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-01 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: os234q.tqr5fxmvapgu0b71 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.170.11.8:6443 --token os234q.tqr5fxmvapgu0b71 \\ --discovery-token-ca-cert-hash sha256:68251032e1f77a7356e784bdeb8e1f7f728cb0fb31c258dc7b44befc9f516f85 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/k8s/k8s_gainian.html":{"url":"cloud_learn/k8s/k8s_gainian.html","title":"02 Kubernetes 概念","keywords":"","body":" Kubernetes 概念 一、基础概念理解 集群 master worker Node Pod 应用最终以Pod为一个基本单位部署 Label 很多资源都可以打标签 Deployment 应用部署用它，deployment最终会产生Pod Service 负载均衡机制 二、kubernetes Objects（k8s对象） 1、什么是k8s对象 https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/ k8s里面操作的资源实体，就是k8s的对象，可以使用yaml来声明对象。然后让k8s根据yaml的声明创建出这个对象；kubectl create/run /expose..... 操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 需要使用 Kubernetes API。比如，当使用 kubectl 命令行接口时，CLI 会执行必要的 Kubernetes API 调用 Kubernetes对象指的是Kubernetes系统的持久化实体，所有这些对象合起来，代表了你集群的实际情况。常规的应用里，我们把应用程序的数据存储在数据库中，Kubernetes将其数据以Kubernetes对象的形式通过 api server存储在 etcd 中。具体来说，这些数据（Kubernetes对象）描述了： 集群中运行了哪些容器化应用程序（以及在哪个节点上运行） 集群中对应用程序可用的资源（网络，存储等） 应用程序相关的策略定义，例如，重启策略、升级策略、容错策略 其他Kubernetes管理应用程序时所需要的信息 scheduler先计算应该去哪个节点部署 对象的spec和status 每一个 Kubernetes 对象都包含了两个重要的字段： spec 必须由您来提供，描述了您对该对象所期望的 目标状态 status 只能由 Kubernetes 系统来修改，描述了该对象在 Kubernetes 系统中的 实际状态 Kubernetes通过对应的 控制器，不断地使实际状态趋向于您期望的目标状态 ### kubectl create deployment my-nginx --image=nginx apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" creationTimestamp: \"2021-04-27T11:37:59Z\" generation: 1 labels: app: my-nginx name: my-nginx namespace: default resourceVersion: \"376418\" uid: 5a47e879-e2e9-40d1-8b02-180031903e8a spec: ###期望状态 progressDeadlineSeconds: 600 replicas: 1 ### 副本数量 revisionHistoryLimit: 10 selector: matchLabels: app: my-nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: my-nginx spec: containers: - image: nginx ###使用这个镜像创建容器 imagePullPolicy: Always name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: ###当前状态 availableReplicas: 1 ## 当前集群可用的 conditions: - lastTransitionTime: \"2021-04-27T11:38:17Z\" lastUpdateTime: \"2021-04-27T11:38:17Z\" message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: \"2021-04-27T11:37:59Z\" lastUpdateTime: \"2021-04-27T11:38:17Z\" message: ReplicaSet \"my-nginx-6b74b79f57\" has successfully progressed. reason: NewReplicaSetAvailable status: \"True\" type: Progressing observedGeneration: 1 readyReplicas: 1 replicas: 1 updatedReplicas: 1 ### 最终一致。 ## etcd保存的创建资源期望的状态和最终这个资源的状态要是一致的；spec和status要最终一致 ## 1、kubectl create deployment my-nginx --image=nginx ## 2、api-server保存etcd，controller-manager最终解析数据，知道集群要my-nginx一份，保存到etcd ## 3、kubelet就做一件事情，spec状态和最终状态一致 while(true){ if(my-nginx.replicas != spec.replicas) { kubelet.startPod(); } } ## 2、描述k8s对象 ##自己编写任意资源的yaml都可以创建出他 ###如何会写任意资源的yaml，比如Pod #################编写yaml的黑科技######################### ## kubectl run my-nginx666 --image=nginx #启动一个Pod ## 1、kubectl get pod my-nginx666 -oyaml 集群中挑一个同类资源，获取出他的yaml。 ## 2、kubectl run my-tomcat --image=tomcat --dry-run -oyaml 干跑一遍 kind: Pod #资源类型 kubectl api-resources:可以获取到所有资源 apiVersion: v1 #同一个资源有可能有多个版本。看 kubectl api-resources提示的。 metadata: #每一个资源定义一些元数据信息 labels: run: my-tomcat name: my-tomcat spec: #资源的规格（镜像名、镜像的环境变量信息等等） containers: - image: tomcat name: my-tomcat resources: {} dnsPolicy: ClusterFirst restartPolicy: Always 当您在 Kubernetes 中创建一个对象时，您必须提供 该对象的 spec 字段，通过该字段描述您期望的 目标状态 该对象的一些基本信息，例如名字 可以使用 kubectl 命令行创建对象，业可以编写 .yaml 格式的文件进行创建 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # 运行 2 个容器化应用程序副本 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 #1、部署 kubectl apply -f deployment.yaml #2、移除 kubectl delete -f deployment.yaml 集群中所有的资源，都在那里？ k8s只依赖一个存储就是etcd。 3、k8s对象yaml的结构 必填字段 在上述的 .yaml 文件中，如下字段是必须填写的： apiVersion 用来创建对象时所使用的Kubernetes API版本 kind 被创建对象的类型 metadata 用于唯一确定该对象的元数据：包括 name 和 namespace，如果 namespace 为空，则默认值为 default spec 描述您对该对象的期望状态 不同类型的 Kubernetes，其 spec 对象的格式不同（含有不同的内嵌字段），通过 API 手册 可以查看 Kubernetes 对象的字段和描述。例如，假设您想了解 Pod 的 spec 定义，可以在 这里找到，Deployment 的 spec 定义可以在 这里 找到 https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/ 这就是我们以后要完全参照的文档。 4、管理k8s对象 管理方式 操作对象 推荐的环境 参与编辑的人数 学习曲线 指令性的命令行 Kubernetes对象;kubectl xxxxx。 cka 开发环境 1+ 最低 指令性的对象配置 单个 yaml 文件 生产环境 1 适中 声明式的对象配置 包含多个 yaml 文件的多个目录。kustomize 生产环境 1+ 最高 同一个Kubernetes对象应该只使用一种方式管理，否则可能会出现不可预期的结果 #1、命令式 kubectl run nginx --image nginx kubectl create deployment nginx --image nginx apply -f ： 没有就创建，有就修改 #2、指令性 - 使用指令性的对象配置（imperative object configuration）时，需要向 kubectl 命令指定具体的操作（create,replace,apply,delete等），可选参数以及至少一个配置文件的名字。配置文件中必须包括一个完整的对象的定义，可以是 yaml 格式，也可以是 json 格式。 #创建对象 kubectl create -f nginx.yaml #删除对象 kubectl delete -f nginx.yaml -f redis.yaml #替换对象 kubectl replace -f nginx.yaml #3、声明式 #处理 configs 目录中所有配置文件中的Kubernetes对象，根据情况创建对象、或更新Kubernetes中已经存在的对象。可以先执行 diff 指令查看具体的变更，然后执行 apply 指令执行变更； kubectl diff -f configs/ kubectl apply -f configs/ #递归处理目录中的内容： kubectl diff -R -f configs/ kubectl apply -R -f configs/ #移除 kubectl delete -f configs/ 5、对象名称 Kubernetes REST API 中，所有的对象都是通过 name 和 UID 唯一性确定 可以通过 namespace + name 唯一性地确定一个 RESTFUL 对象，例如： /api/v1/namespaces/{namespace}/pods/{name} Names 同一个名称空间下，同一个类型的对象，可以通过 name 唯一性确定。如果删除该对象之后，可以再重新创建一个同名对象。 依据命名规则，Kubernetes对象的名字应该： 最长不超过 253个字符 必须由小写字母、数字、减号 -、小数点 . 组成 某些资源类型有更具体的要求 例如，下面的配置文件定义了一个 name 为 nginx-demo 的 Pod，该 Pod 包含一个 name 为 nginx 的容器： apiVersion: v1 kind: Pod metadata: name: nginx-demo ##pod的名字 spec: containers: - name: nginx ##容器的名字 image: nginx:1.7.9 ports: - containerPort: 80 UIDs UID UID 是由 Kubernetes 系统生成的，唯一标识某个 Kubernetes 对象的字符串。 Kubernetes集群中，每创建一个对象，都有一个唯一的 UID。用于区分多次创建的同名对象（如前所述，按照名字删除对象后，重新再创建同名对象时，两次创建的对象 name 相同，但是 UID 不同。） 6、名称空间 kubectl get namespaces kubectl describe namespaces #隔离 mysql mapper.xml--》dao. Kubernetes 安装成功后，默认有初始化了三个名称空间： default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes系统创建的对象放在此名称空间下 kube-public 此名称空间自动在安装集群是自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户）。主要是为集群预留的，例如，某些情况下，某些Kubernetes对象应该被所有集群用户看到。 名称空间未来如何隔离 1）、基于环境隔离（prod,test） ​ prod：部署的所有应用 ​ test：部署的所有应用 ​ 2）、基于产品线的名称空间（商城，android，ios，backend）； 3）、基于团队隔离 访问其他名称空间的东西？(名称空间资源隔离，网络不隔离) 1）、配置直接拿来用。不行 2）、网络访问，可以。 Pod-Pod； serviceName来访问，找本名称空间的Service负载均衡 serviceName.名称空间，可以访问别的名称空间的 创建名称空间 apiVersion: v1 kind: Namespace metadata: name: apiVersion: v1 kind: Namespace metadata: creationTimestamp: null name: k8s-03 spec: {} status: {} kubectl create -f ./my-namespace.yaml #直接用命令 kubectl create namespace #删除 kubectl delete namespaces 名称空间的名字必须与 DNS 兼容： 不能带小数点 . 不能带下划线 _ 使用数字、小写字母和减号 - 组成的字符串 默认情况下，安装Kubernetes集群时，会初始化一个 default 名称空间，用来将承载那些未指定名称空间的 Pod、Service、Deployment等对象 为请求设置命名空间 #要为当前请求设置命名空间，请使用 --namespace 参数。 kubectl run nginx --image=nginx --namespace= kubectl get pods --namespace= #在对象yaml中使用命名空间 apiVersion: v1 kind: Pod metadata: name: nginx-demo ##pod的名字 namespace: default #不写就是default spec: containers: - name: nginx ##容器的名字 image: nginx:1.7.9 ports: - containerPort: 80 当您创建一个 Service 时，Kubernetes 会创建一个相应的 DNS 条目。 该条目的形式是 ..svc.cluster.local ，这意味着如果容器只使用 ``，它将被解析到本地命名空间的服务。这对于跨多个命名空间（如开发、分级和生产）使用相同的配置非常有用。如果您希望跨命名空间访问，则需要使用完全限定域名（FQDN）。 # 创建Pod kind:Pod k8s底层最小的部署单元是Pod。Service，Deploy，ReplicaSet # Deploy:直接指定Pod模板（） kind: Deploy 并非所有对象都在命名空间中 大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些命名空间中。但是命名空间资源本身并不在命名空间中。而且底层资源，例如 nodes 和持久化卷不属于任何命名空间。 查看哪些 Kubernetes 资源在命名空间中，哪些不在命名空间中： # In a namespace kubectl api-resources --namespaced=true # Not in a namespace kubectl api-resources --namespaced=false 7 、标签和选择器 标签（Label）是附加在Kubernetes对象上的一组名值对，其意图是按照对用户有意义的方式来标识Kubernetes对象，同时，又不对Kubernetes的核心逻辑产生影响。标签可以用来组织和选择一组Kubernetes对象。您可以在创建Kubernetes对象时为其添加标签，也可以在创建以后再为其添加标签。每个Kubernetes对象可以有多个标签，同一个对象的标签的 Key 必须唯一，例如： metadata: labels: key1: value1 key2: value2 使用标签（Label）可以高效地查询和监听Kubernetes对象，在Kubernetes界面工具（如 Kubenetes Dashboard 或 Kuboard）和 kubectl 中，标签的使用非常普遍。那些非标识性的信息应该记录在 注解（annotation） 为什么要使用标签 使用标签，用户可以按照自己期望的形式组织 Kubernetes 对象之间的结构，而无需对 Kubernetes 有任何修改。 应用程序的部署或者批处理程序的部署通常都是多维度的（例如，多个高可用分区、多个程序版本、多个微服务分层）。管理这些对象时，很多时候要针对某一个维度的条件做整体操作，例如，将某个版本的程序整体删除，这种情况下，如果用户能够事先规划好标签的使用，再通过标签进行选择，就会非常地便捷。 标签的例子有： release: stable、release: canary environment: dev、environment: qa、environment: production tier: frontend、tier: backend、tier: cache partition: customerA、partition: customerB track: daily、track: weekly 上面只是一些使用比较普遍的标签，您可以根据您自己的情况建立合适的使用标签的约定。 句法和字符集 标签是一组名值对（key/value pair）。标签的 key 可以有两个部分：可选的前缀和标签名，通过 / 分隔。 标签名： 标签名部分是必须的 不能多于 63 个字符 必须由字母、数字开始和结尾 可以包含字母、数字、减号-、下划线_、小数点. 标签前缀： 标签前缀部分是可选的 如果指定，必须是一个DNS的子域名，例如：k8s.eip.work 不能多于 253 个字符 使用 / 和标签名分隔 如果省略标签前缀，则标签的 key 将被认为是专属于用户的。Kubernetes的系统组件（例如，kube-scheduler、kube-controller-manager、kube-apiserver、kubectl 或其他第三方组件）向用户的Kubernetes对象添加标签时，必须指定一个前缀。kubernetes.io/ 和 k8s.io/ 这两个前缀是 Kubernetes 核心组件预留的。 标签的 value 必须： 不能多于 63 个字符 可以为空字符串 如果不为空，则 必须由字母、数字开始和结尾 可以包含字母、数字、减号-、下划线_、小数点. apiVersion: v1 kind: Pod metadata: name: label-demo labels: environment: production app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 标签选择器 通常来讲，会有多个Kubernetes对象包含相同的标签。通过使用标签选择器（label selector），用户/客户端可以选择一组对象。标签选择器（label selector）是 Kubernetes 中最主要的分类和筛选手段。 Kubernetes api server支持两种形式的标签选择器，equality-based 基于等式的 和 set-based 基于集合的。标签选择器可以包含多个条件，并使用逗号分隔，此时只有满足所有条件的 Kubernetes 对象才会被选中 使用基于等式的选择方式,可以使用三种操作符 =、==、!=。前两个操作符含义是一样的，都代表相等，后一个操作符代表不相等 # kubectl get pods -l environment=production,tier=frontend # 选择了标签名为 `environment` 且 标签值为 `production` 的Kubernetes对象 environment = production # 选择了标签名为 `tier` 且标签值不等于 `frontend` 的对象，以及不包含标签 `tier` 的对象 tier != frontend 使用基于集合的选择方式 Set-based 标签选择器可以根据标签名的一组值进行筛选。支持的操作符有三种：in、notin、exists。例如 # 选择所有的包含 `environment` 标签且值为 `production` 或 `qa` 的对象 environment in (production, qa) # 选择所有的 `tier` 标签不为 `frontend` 和 `backend`的对象，或不含 `tier` 标签的对象 tier notin (frontend, backend) # 选择所有包含 `partition` 标签的对象 partition # 选择所有不包含 `partition` 标签的对象 !partition # 选择包含 `partition` 标签（不检查标签值）且 `environment` 不是 `qa` 的对象 partition,environment notin (qa) kubectl get pods -l 'environment in (production),tier in (frontend)' #Job、Deployment、ReplicaSet 和 DaemonSet 同时支持基于等式的选择方式和基于集合的选择方式。例如： selector: matchLabels: component: redis matchExpressions: - {key: tier, operator: In, values: [cache]} - {key: environment, operator: NotIn, values: [dev]} # matchLabels 是一个 {key,value} 组成的 map。map 中的一个 {key,value} 条目相当于 matchExpressions 中的一个元素，其 key 为 map 的 key，operator 为 In， values 数组则只包含 value 一个元素。matchExpression 等价于基于集合的选择方式，支持的 operator 有 In、NotIn、Exists 和 DoesNotExist。当 operator 为 In 或 NotIn 时，values 数组不能为空。所有的选择条件都以 AND 的形式合并计算，即所有的条件都满足才可以算是匹配 #添加或者修改标签 kubectl label --help # Update pod 'foo' with the label 'unhealthy' and the value 'true'. kubectl label pods foo unhealthy=true # Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value. kubectl label --overwrite pods foo status=unhealthy # Update all pods in the namespace kubectl label pods --all status=unhealthy # Update a pod identified by the type and name in \"pod.json\" kubectl label -f pod.json status=unhealthy # Update pod 'foo' only if the resource is unchanged from version 1. kubectl label pods foo status=unhealthy --resource-version=1 # Update pod 'foo' by removing a label named 'bar' if it exists. # Does not require the --overwrite flag. kubectl label pods foo bar- 8、注解annotation 注解（annotation）可以用来向 Kubernetes 对象的 metadata.annotations 字段添加任意的信息。Kubernetes 的客户端或者自动化工具可以存取这些信息以实现其自定义的逻辑。 metadata: annotations: key1: value1 key2: value2 9、字段选择器 字段选择器（Field selectors）允许您根据一个或多个资源字段的值筛选 Kubernetes 资源。 下面是一些使用字段选择器查询的例子： metadata.name=my-service metadata.namespace!=default status.phase=Pending kubectl get pods --field-selector status.phase=Running 三个命令玩转所有的yaml写法‘ kubectl get xxx -oyaml kubectl create deploy xxxxx --dry-run-client -oyaml kubectl explain pod.spec.xx 写完yaml kubectl apply -f 即可 10、给vscode安装插件 搜索kubernetes ，安装 yaml和kubernetes template插件即可 idea也有kubernetes插件 11、认识kubectl和kubelet kubeadm安装的集群。二进制后来就是 yum install etcd api-server 认识核心文件夹 /etc/kubernetes . 以Pod方式安装的核心组件。 etcd，api-server，scheduler。（安装k8s的时候，yum kubeadm kubelet kubectl） 回顾集群安装的时候，为什么只有master节点的kubectl可以操作集群 kubelet额外参数配置 /etc/sysconfig/kubelet；kubelet配置位置 /var/lib/kubelet/config.yaml kubectl的所有命令参考： 命令参考：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands pdf命令实战：https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/cheatsheet-kubernetes-A4.pdf Basic Commands (Beginner): 初学者掌握的命令 create Create a resource from a file or from stdin. expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service run Run a particular image on the cluster set Set specific features on objects Basic Commands (Intermediate): 基础命令 explain Documentation of resources get Display one or many resources edit Edit a resource on the server delete Delete resources by filenames, stdin, resources and names, or by resources and label selector Deploy Commands: #部署用的命令 rollout Manage the rollout of a resource scale Set a new size for a Deployment, ReplicaSet or Replication Controller autoscale Auto-scale a Deployment, ReplicaSet, StatefulSet, or ReplicationController Cluster Management Commands: #集群管理的命令 certificate Modify certificate resources. cluster-info Display cluster info top Display Resource (CPU/Memory) usage. cordon Mark node as unschedulable uncordon Mark node as schedulable drain Drain node in preparation for maintenance taint Update the taints on one or more nodes Troubleshooting and Debugging Commands: # debug的命令 describe Show details of a specific resource or group of resources logs Print the logs for a container in a pod attach Attach to a running container exec Execute a command in a container port-forward Forward one or more local ports to a pod proxy Run a proxy to the Kubernetes API server cp Copy files and directories to and from containers. auth Inspect authorization debug Create debugging sessions for troubleshooting workloads and nodes Advanced Commands: # 高阶命令 diff Diff live version against would-be applied version apply Apply a configuration to a resource by filename or stdin patch Update field(s) of a resource replace Replace a resource by filename or stdin wait Experimental: Wait for a specific condition on one or many resources. kustomize Build a kustomization target from a directory or URL. Settings Commands: # 设置 label Update the labels on a resource annotate Update the annotations on a resource completion Output shell completion code for the specified shell (bash or zsh) # Other Commands: #其他 api-resources Print the supported API resources on the server api-versions Print the supported API versions on the server, in the form of \"group/version\" config Modify kubeconfig files plugin Provides utilities for interacting with plugins. version Print the client and server version information 12、自动补全 https://kubernetes.io/zh/docs/tasks/tools/included/optional-kubectl-configs-bash-linux/ # 安装 yum install bash-completion # 自动补全 echo 'source >~/.bashrc kubectl completion bash >/etc/bash_completion.d/kubectl source /usr/share/bash-completion/bash_completion 三、万物基础-容器 思考：我们在k8s里面的容器和docker的容器有什么异同？ k8s的Pod是最小单位，Pod中容器的配置需要注意以下常用的 Pod里面的容器内容可以写的东西 args command Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double {% math %}, ie: {% endmath %}(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env 容器要用的环境变量 envFrom List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. image 写镜像的名字 imagePullPolicy 下载策略： Always：总是去下载： 【默认】 先看网上有没有，有了就下载，（本机也有，docker就相当于不用下载了） Never：总不去下载，一定保证当前Pod所在的机器有这个镜像 ；直接看本机 IfNotPresent：如果本机没有就去下载；先看本机，再看远程 lifecycle 生命周期钩子 livenessProbe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes name -required- 容器的名字 ports 端口： readinessProbe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ securityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ startupProbe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes stdin Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. stdinOnce Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false terminationMessagePath Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. terminationMessagePolicy Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. tty Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. volumeDevices volumeDevices is the list of block devices to be used by the container. volumeMounts Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir 指定进容器的工作目录 1、镜像 在 Kubernetes 的 Pod 中使用容器镜像之前，我们必须将其推送到一个镜像仓库（或者使用仓库中已经有的容器镜像）。在 Kubernetes 的 Pod 定义中定义容器时，必须指定容器所使用的镜像，容器中的 image 字段支持与 docker 命令一样的语法，包括私有镜像仓库和标签。 如果使用 hub.dokcer.com Registry 中的镜像，可以省略 registry 地址和 registry 端口。例如：nginx:latest Kubernetes中，默认的镜像抓取策略是 IfNotPresent，使用此策略，kubelet在发现本机有镜像的情况下，不会向镜像仓库抓取镜像。如果您期望每次启动 Pod 时，都强制从镜像仓库抓取镜像，可以尝试如下方式： 设置 container 中的 imagePullPolicy 为 Always 省略 imagePullPolicy 字段，并使用 :latest tag 的镜像 省略 imagePullPolicy 字段和镜像的 tag 激活 AlwaysPullImages 管理控制器 docker pull redis docker.io/library/redis:latest 下载私有仓库镜像 #这个秘钥默认在default名称空间，不能被hello名称空间共享 kubectl create secret -n hello docker-registry my-aliyun \\ --docker-server=registry.cn-hangzhou.aliyuncs.com \\ --docker-username=forsumlove \\ --docker-password=lfy11223344 # 那个镜像对应哪个仓库没有任何问题 apiVersion: v1 kind: Pod metadata: name: foo spec: containers: - name: foo image: registry.cn-zhangjiakou.aliyuncs.com/atguigudocker/atguigu-java-img:v1.0 imagePullSecrets: - name: mydocker 2、启动命令 3、环境变量 env指定即可 4、生命周期容器钩子 Kubernetes中为容器提供了两个 hook（钩子函数）： PostStart 此钩子函数在容器创建后将立刻执行。但是，并不能保证该钩子函数在容器的 ENTRYPOINT 之前执行。该钩子函数没有输入参数。 PreStop 此钩子函数在容器被 terminate（终止）之前执行，例如： 通过接口调用删除容器所在 Pod 某些管理事件的发生：健康检查失败、资源紧缺等 如果容器已经被关闭或者进入了 completed 状态，preStop 钩子函数的调用将失败。该函数的执行是同步的，即，kubernetes 将在该函数完成执行之后才删除容器。该钩子函数没有输入参数。 apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: alpine command: [\"/bin/sh\", \"-c\", \"echo hello; \"] volumeMounts: - name: mount1 mountPath: /app lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo world;\"] preStop: exec: command: [\"/bin/sh\",\"-c\",\"echo 66666;\"] Kubernetes 在容器启动后立刻发送 postStart 事件，但是并不能确保 postStart 事件处理程序在容器的 EntryPoint 之前执行。postStart 事件处理程序相对于容器中的进程来说是异步的（同时执行），然而，Kubernetes 在管理容器时，将一直等到 postStart 事件处理程序结束之后，才会将容器的状态标记为 Running。 Kubernetes 在决定关闭容器时，立刻发送 preStop 事件，并且，将一直等到 preStop 事件处理程序结束或者 Pod 的 --grace-period 超时，才删除容器 3、资源限制 pods/qos/qos-pod.yaml apiVersion: v1 kind: Pod metadata: name: qos-demo namespace: qos-example spec: containers: - name: qos-demo-ctr image: nginx resources: # limits: # 限制最大大小 -Xmx memory: \"200Mi\" cpu: \"700m\" # 启动默认给分配的大小 -Xms requests: memory: \"200Mi\" cpu: \"700m\" 4、其他 kubectl explain 解析一个资源Pod改怎么编写yaml kubectl describe 用来排错的，看资源的状态 官方文档 = 专家 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/k8s/k8s_svc.html":{"url":"cloud_learn/k8s/k8s_svc.html","title":"03 Kubernetes 工作负载","keywords":"","body":" Kubernetes 工作负载 总：Workloads #获取控制台访问令牌 kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') 什么是工作负载（Workloads） 工作负载是运行在 Kubernetes 上的一个应用程序。 一个应用很复杂，可能由单个组件或者多个组件共同完成。无论怎样我们可以用一组Pod来表示一个应用，也就是一个工作负载 Pod又是一组容器（Containers） 所以关系又像是这样 工作负载（Workloads）控制一组Pod Pod控制一组容器（Containers） 比如Deploy（工作负载） 3个副本的nginx（3个Pod），每个nginx里面是真正的nginx容器（container） 工作负载能让Pod能拥有自恢复能力。 会写Pod。研究不同的工作负载怎么控制Pod的行为 一、Pod 1、什么是Pod Pod是一组（一个或多个） 容器（docker容器）的集合 （就像在豌豆荚中）；这些容器共享存储、网络、以及怎样运行这些容器的声明。 我们一般不直接创建Pod，而是创建一些工作负载由他们来创建Pod Pod的形式 Pod对容器有自恢复能力（Pod自动重启失败的容器） Pod自己不能恢复自己，Pod被删除就真的没了（100，MySQL、Redis、Order）还是希望k8s集群能自己在其他地方再启动这个Pod 单容器Pod 多容器协同Pod。我们可以把另外的容器称为SideCar（为应用赋能） Pod 天生地为其成员容器提供了两种共享资源：网络和 存储。 一个Pod由一个Pause容器设置好整个Pod里面所有容器的网络、名称空间等信息 systemctl status可以观测到。Pod和容器进程关系 kubelet启动一个Pod，准备两个容器，一个是Pod声明的应用容器（nginx），另外一个是Pause。Pause给当前应用容器设置好网络空间各种的。 编写yaml测试：多容器协同 2、Pod使用 可以编写deploy等各种工作负载的yaml文件，最终创建出pod，也可以直接创建 Pod的模板如下 # 这里是 Pod 模版 apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: hello image: busybox command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600'] restartPolicy: OnFailure # 以上为 Pod 模版 3、Pod生命周期 Pod启动，会先依次执行所有初始化容器，有一个失败，则Pod不能启动 接下来启动所有的应用容器（每一个应用容器都必须能一直运行起来），Pod开始正式工作，一个启动失败就会尝试重启Pod内的这个容器，Pod只要是NotReady，Pod就不对外提供服务了 编写yaml测试生命周期 应用容器生命周期钩子 初始化容器（也可以有钩子） 临时容器：线上排错。 有些容器基础镜像。线上没法排错。使用临时容器进入这个Pod。临时容器共享了Pod的所有。临时容器有Debug的一些命令，拍错完成以后，只要exit退出容器，临时容器自动删除 Java：dump， jre 50mb。jdk 150mb jre 50mb。: jdk作为临时容器 临时容器需要开启特性门控 --feature-gates=\"EphemeralContainers=true\" 在所有组件，api-server、kubelet、scheduler、controller-manager都得配置 1.21.0： 生产环境 .5 使用临时容器的步骤： 1、声明一个临时容器。准备好json文件 { \"apiVersion\": \"v1\", \"kind\": \"EphemeralContainers\", \"metadata\": { \"name\": \"my-nginx666\" //指定Pod的名字 }, \"ephemeralContainers\": [{ \"command\": [ \"sh\" ], \"image\": \"busybox\", //jre的需要jdk来调试 \"imagePullPolicy\": \"IfNotPresent\", \"name\": \"debugger\", \"stdin\": true, \"tty\": true, \"terminationMessagePolicy\": \"File\" }] } 2、使用临时容器，应用一下即可 kubectl replace --raw /api/v1/namespaces/default/pods/my-nginx666【pod名】/ephemeralcontainers -f ec.json 4、静态Pod 在 /etc/kubernetes/manifests 位置放的所有Pod.yaml文件，机器启动kubelet自己就把他启动起来。 静态Pod一直守护在他的这个机器上 5、Probe 探针机制（健康检查机制） 每个容器三种探针（Probe） 启动探针**（后来才加的） 一次性成功探针。** 只要启动成功了 kubelet 使用启动探针，来检测应用是否已经启动。如果启动就可以进行后续的探测检查。慢容器一定指定启动探针。一直在等待启动 启动探针 成功以后就不用了，剩下存活探针和就绪探针持续运行 存活探针 kubelet 使用存活探针，来检测容器是否正常存活。（有些容器可能产生死锁【应用程序在运行，但是无法继续执行后面的步骤】），如果检测失败就会**重新启动这个容器** initialDelaySeconds： 3600（长了导致可能应用一段时间不可用） 5（短了陷入无限启动循环） 就绪探针 kubelet 使用就绪探针，来检测容器是否准备好了可以接收流量。当一个 Pod 内的所有容器都准备好了，才能把这个 Pod 看作就绪了。用途就是：Service后端负载均衡多个Pod，如果某个Pod还没就绪，就会从service负载均衡里面剔除 谁利用这些探针探测 kubelet会主动按照配置给Pod里面的所有容器发送响应的探测请求 Probe配置项 initialDelaySeconds：容器启动后要等待多少秒后存活和就绪探测器才被初始化，默认是 0 秒，最小值是 0。这是针对以前没有 periodSeconds：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。 successThreshold：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。 存活和启动探针的这个值必须是 1。最小值是 1。 failureThreshold：当探测失败时，Kubernetes 的重试次数。 存活探测情况下的放弃就意味着重新启动容器。 就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。 timeoutSeconds：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。 https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes exec、httpGet、tcpSocket 【那种方式探测】 failureThreshold initialDelaySeconds periodSeconds successThreshold terminationGracePeriodSeconds timeoutSeconds 编写yaml测试探针机制 apiVersion: v1 kind: Pod metadata: name: \"nginx-start-probe02\" namespace: default labels: app: \"nginx-start-probe02\" spec: volumes: - name: nginx-vol hostPath: path: /app - name: nginx-html hostPath: path: /html containers: - name: nginx image: \"nginx\" ports: - containerPort: 80 startupProbe: exec: command: [\"/bin/sh\",\"-c\",\"cat /app/abc\"] ## 返回不是0，那就是探测失败 # initialDelaySeconds: 20 ## 指定的这个秒以后才执行探测 periodSeconds: 5 ## 每隔几秒来运行这个 timeoutSeconds: 5 ##探测超时，到了超时时间探测还没返回结果说明失败 successThreshold: 1 ## 成功阈值，连续几次成才算成功 failureThreshold: 3 ## 失败阈值，连续几次失败才算真失败 volumeMounts: - name: nginx-vol mountPath: /app - name: nginx-html mountPath: /usr/share/nginx/html livenessProbe: ## nginx容器有没有 /abc.html，就绪探针 # httpGet: # host: 127.0.0.1 # path: /abc.html # port: 80 # scheme: HTTP # periodSeconds: 5 ## 每隔几秒来运行这个 # successThreshold: 1 ## 成功阈值，连续几次成才算成功 # failureThreshold: 5 ## 失败阈值，连续几次失败才算真失败 exec: command: [\"/bin/sh\",\"-c\",\"cat /usr/share/nginx/html/abc.html\"] ## 返回不是0，那就是探测失败 # initialDelaySeconds: 20 ## 指定的这个秒以后才执行探测 periodSeconds: 5 ## 每隔几秒来运行这个 timeoutSeconds: 5 ##探测超时，到了超时时间探测还没返回结果说明失败 successThreshold: 1 ## 成功阈值，连续几次成才算成功 failureThreshold: 3 ## 失败阈值，连续几次失败才算真失败 readinessProbe: ##就绪检测，都是http httpGet: # host: 127.0.0.1 ###不行 path: /abc.html ## 给容器发请求 port: 80 scheme: HTTP ## 返回不是0，那就是探测失败 initialDelaySeconds: 2 ## 指定的这个秒以后才执行探测 periodSeconds: 5 ## 每隔几秒来运行这个 timeoutSeconds: 5 ##探测超时，到了超时时间探测还没返回结果说明失败 successThreshold: 3 ## 成功阈值，连续几次成才算成功 failureThreshold: 5 ## 失败阈值，连续几次失败才算真失败 # livenessProbe: # exec: [\"/bin/sh\",\"-c\",\"sleep 30;abc \"] ## 返回不是0，那就是探测失败 # initialDelaySeconds: 20 ## 指定的这个秒以后才执行探测 # periodSeconds: 5 ## 每隔几秒来运行这个 # timeoutSeconds: 5 ##探测超时，到了超时时间探测还没返回结果说明失败 # successThreshold: 5 ## 成功阈值，连续几次成才算成功 # failureThreshold: 5 ## 失败阈值，连续几次失败才算真失败 微服务。 /health K8S检查当前应用的状态；connection refuse； SpringBoot 优雅停机：gracefulShowdown: true pod.spec.terminationGracePeriodSeconds = 30s 优雅停机；给一个缓冲时间 健康检查+优雅停机 = 0宕机 start完成以后，liveness和readness并存。 liveness失败导致重启。readness失败导致不给Service负载均衡网络中加，不接受流量。 kubectl exec -it 就进不去。Kubectl describe 看看咋了。 二、Deployment 1、什么是Deployment 一个 Deployment 为 Pods 和 ReplicaSets 提供声明式的更新能力。 你负责描述 Deployment 中的 目标状态，而 Deployment 控制器（Controller） 以受控速率更改实际状态， 使其变为期望状态；控制循环。 for(){ xxx controller.spec()} 不要管理 Deployment 所拥有的 ReplicaSet 我们部署一个应用一般不直接写Pod，而是部署一个Deployment Deploy编写规约 https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/#writing-a-deployment-spec 2、Deployment创建 基本格式 .metadata.name指定deploy名字 replicas 指定副本数量 selector 指定匹配的Pod模板。 template 声明一个Pod模板 编写一个Deployment的yaml 赋予Pod自愈和故障转移能力。 在检查集群中的 Deployment 时，所显示的字段有： NAME 列出了集群中 Deployment 的名称。 READY 显示应用程序的可用的 副本 数。显示的模式是“就绪个数/期望个数”。 UP-TO-DATE 显示为了达到期望状态已经更新的副本数。 AVAILABLE 显示应用可供用户使用的副本数。 AGE 显示应用程序运行的时间。 ReplicaSet 输出中包含以下字段： NAME 列出名字空间中 ReplicaSet 的名称； DESIRED 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。 此为期望状态； CURRENT 显示当前运行状态中的副本个数； READY 显示应用中有多少副本可以为用户提供服务； AGE 显示应用已经运行的时间长度。 注意：ReplicaSet 的名称始终被格式化为[Deployment名称]-[随机字符串]。 其中的随机字符串是使用 pod-template-hash 作为种子随机生成的。 一个Deploy产生三个 Deployment资源 replicaset资源 Pod资源 Deployment控制RS，RS控制Pod的副本数 ReplicaSet： 只提供了副本数量的控制功能 Deployment： 每部署一个新版本就会创建一个新的副本集，利用他记录状态，回滚也是直接让指定的rs生效 --- rs1： 4 abc --- rs2: 4 def --- rsN: 4 eee nginx=111 nginx:v1=2222 nginx:v2=3333 3、Deployment 更新机制 仅当 Deployment Pod 模板（即 .spec.template）发生改变时，例如模板的标签或容器镜像被更新， 才会触发 Deployment 上线。 其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。 上线动作 原理： 创建新的rs，准备就绪后，替换旧的rs（此时不会删除，因为revisionHistoryLimit 指定了保留几个版本） 常用的kubectl 命令 利用set image命令更新 ################更新################################# #kubectl set image deployment资源名 容器名=镜像名 kubectl set image deploy mydeploy-01 nginx-01=nginx:alpine --record 编辑文件进行更新 #或者直接修改定义也行 kubectl edit deployment.apps/ #查看状态 kubectl rollout status deployment.v1.apps/nginx-deployment ################查看历史并回滚#################################### #查看更新历史-看看我们设置的历史总记录数是否生效了 kubectl rollout history deployment.v1.apps/mydeploy-01 #回滚 kubectl rollout undo deployment.v1.apps/mydeploy-01 --to-revision=2 ###############累计更新############## #暂停记录版本 kubectl rollout pause deployment.v1.apps/mydeploy-01 #多次更新操作。 ##比如更新了资源限制 kubectl set resources deployment.v1.apps/mydeploy-01 -c=nginx --limits=cpu=200m,memory=512Mi ##比如更新了镜像版本 kubectl set image deployment.apps/mydeploy-01 php-redis=tomcat:8 ##在继续操作多次 ##看看历史版本有没有记录变化 kubectl rollout history deployment.v1.apps/mydeploy-01 #让多次累计生效 kubectl rollout resume deployment.v1.apps/mydeploy-01 1、比例缩放（Proportional Scaling） revisionHistoryLimit : 旧副本集保留的数量，可回滚的数量，默认是10 type : Recreate/RollingUpdate（默认） maxSurge（最大增量）：除当前数量外还要添加多少个实例。 maxUnavailable（最大不可用量）：滚动更新过程中的不可用实例数。 maxSurge 【最大增量】: 2 一次最多新建几个Pod。 百分比和数字都可以 MaxUnavailable：为0 的时候， maxSurge不能为0 maxUnavailable【最大不可用量】: 4 最大不可用的Pod数量 kubectl explain deploy.spec.strategy.rollingUpdate 演示案例： apiVersion: apps/v1 ### kind: Deployment ## metadata: name: mydeploy-05 ### 遵循域名编写规范 namespace: default labels: dep: test-04 ### 期望状态 spec: strategy: # type: Recreate ### 把以前全部杀死，直接新建 #滚动更新 type: RollingUpdate rollingUpdate: maxUnavailable: 2 maxSurge: 20% replicas: 10 selector: ### 选择器 matchLabels: ### 匹配标签 pod-name: aaa55566 ### 和模板template里面的pod的标签必须一样 #### template: metadata: ### pod的metadata labels: pod-name: aaa55566 spec: containers: - name: nginx-01 image: nginx 2、HPA（动态扩缩容） 概念：https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-policies 实战：https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ 需要先安装metrics-server https://github.com/kubernetes-sigs/metrics-server 安装步骤 apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rbac.authorization.k8s.io/aggregate-to-view: \"true\" name: system:aggregated-metrics-reader rules: - apiGroups: - metrics.k8s.io resources: - pods - nodes verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server name: system:metrics-server rules: - apiGroups: - \"\" resources: - pods - nodes - nodes/stats - namespaces - configmaps verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: v1 kind: Service metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: ports: - name: https port: 443 protocol: TCP targetPort: https selector: k8s-app: metrics-server --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: selector: matchLabels: k8s-app: metrics-server strategy: rollingUpdate: maxUnavailable: 0 template: metadata: labels: k8s-app: metrics-server spec: containers: - args: - --cert-dir=/tmp - --kubelet-insecure-tls - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/metrics-server:v0.4.3 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /livez port: https scheme: HTTPS periodSeconds: 10 name: metrics-server ports: - containerPort: 4443 name: https protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /readyz port: https scheme: HTTPS periodSeconds: 10 securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /tmp name: tmp-dir nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: metrics-server volumes: - emptyDir: {} name: tmp-dir --- apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: labels: k8s-app: metrics-server name: v1beta1.metrics.k8s.io spec: group: metrics.k8s.io groupPriorityMinimum: 100 insecureSkipTLSVerify: true service: name: metrics-server namespace: kube-system version: v1beta1 versionPriority: 100 kubectl apply 即可、 全部runnning 用 kubectl top nodes --use-protocol-buffers kubectl top pods --use-protocol-buffers 配置hpa测试 ### 测试镜像 registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/php-hpa:latest ##应用的yaml已经做好 apiVersion: v1 kind: Service metadata: name: php-apache spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: run: php-apache --- apiVersion: apps/v1 kind: Deployment metadata: labels: run: php-apache name: php-apache spec: replicas: 1 selector: matchLabels: run: php-apache template: metadata: creationTimestamp: null labels: run: php-apache spec: containers: - image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/php-hpa:latest name: php-apache ports: - containerPort: 80 resources: requests: cpu: 200m ##hpa配置 hpa.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: maxReplicas: 10 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache targetCPUUtilizationPercentage: 50 #3、进行压力测试 kubectl run -i --tty load-generator --image=busybox /bin/sh #回车然后敲下面的命令 kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\" 3、Canary（金丝雀部署） 1、蓝绿部署VS金丝雀部署 蓝绿部署 金丝雀部署 矿场。 2、金丝雀的简单测试 #### 使用这个镜像测试registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/nginx-test #### 这个镜像docker run 的时候 -e msg=aaaa，访问这个nginx页面就是看到aaaa 步骤原理 准备一个Service，负载均衡Pod 准备版本v1的deploy，准备版本v2的deploy 滚动发布的缺点？（同时存在两个版本都能接受流量） 没法控制流量 ； 6 4， 8 2 ，3 7 滚动发布短时间就直接结束，不能直接控制新老版本的存活时间。 用两个镜像： registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/nginx-test:env-msg 默认输出11111 nginx： 默认输出 默认页； 4、Deployment状态与排错 https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/#deployment-status 三、RC、RS 四、DaemonSet DaemonSet 控制器确保所有（或一部分）的节点都运行了一个指定的 Pod 副本。 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上 当节点从集群中移除时，Pod 也就被垃圾回收了 删除一个 DaemonSet 可以清理所有由其创建的 Pod DaemonSet 的典型使用场景有： 在每个节点上运行集群的存储守护进程，例如 glusterd、ceph 在每个节点上运行日志收集守护进程，例如 fluentd、logstash 在每个节点上运行监控守护进程，例如 Prometheus Node Exporter、Sysdig Agent、collectd、Dynatrace OneAgent、APPDynamics Agent、Datadog agent、New Relic agent、Ganglia gmond、Instana Agent 等 apiVersion: apps/v1 kind: DaemonSet metadata: name: logging labels: app: logging spec: selector: matchLabels: name: logging template: metadata: labels: name: logging spec: containers: - name: logging image: nginx resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi tolerations: #设置容忍master的污点 - key: node-role.kubernetes.io/master effect: NoSchedule #查看效果 kubectl get pod -l name=logging -o wide 五、StatefulSet 有状态副本集；Deployment等属于无状态的应用部署（stateless） StatefulSet 使用场景；对于有如下要求的应用程序，StatefulSet 非常适用： 稳定、唯一的网络标识（dnsname） StatefulSet通过与其相关的无头服务为每个pod提供DNS解析条目。假如无头服务的DNS条目为: \"$(service name).$(namespace).svc.cluster.local\"， 那么pod的解析条目就是\"$(pod name).$(service name).$(namespace).svc.cluster.local\"，每个pod name也是唯一的。 稳定的、持久的存储；【每个Pod始终对应各自的存储路径（PersistantVolumeClaimTemplate）】 有序的、优雅的部署和缩放。【按顺序地增加副本、减少副本，并在减少副本时执行清理】 有序的、自动的滚动更新。【按顺序自动地执行滚动更新】 限制 给定 Pod 的存储必须由 PersistentVolume 驱动 基于所请求的 storage class 来提供，或者由管理员预先提供。 删除或者收缩 StatefulSet 并不会删除它关联的存储卷。 这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。 StatefulSet 当前需要无头服务 来负责 Pod 的网络标识。你需要负责创建此服务。 当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。 为了实现 StatefulSet 中的 Pod 可以有序地且体面地终止，可以在删除之前将 StatefulSet 缩放为 0。 在默认 Pod 管理策略(OrderedReady) 时使用 滚动更新，可能进入需要人工干预 才能修复的损坏状态。 如果一个应用程序不需要稳定的网络标识，或者不需要按顺序部署、删除、增加副本，就应该考虑使用 Deployment 这类无状态（stateless）的控制器 apiVersion: v1 kind: Service #定义一个负载均衡网络 metadata: name: stateful-tomcat labels: app: stateful-tomcat spec: ports: - port: 8123 name: web targetPort: 8080 clusterIP: None #NodePort：任意机器+NodePort都能访问，ClusterIP：集群内能用这个ip、service域名能访问，clusterIP: None；不要分配集群ip。headless；无头服务。稳定的域名 selector: app: stateful-tomcat --- apiVersion: apps/v1 kind: StatefulSet #控制器。 metadata: name: stateful-tomcat spec: selector: matchLabels: app: stateful-tomcat # has to match .spec.template.metadata.labels serviceName: \"stateful-tomcat\" #这里一定注意，必须提前有个service名字叫这个的 replicas: 3 # by default is 1 template: metadata: labels: app: stateful-tomcat # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: tomcat image: tomcat:7 ports: - containerPort: 8080 name: web #观察效果。 删除一个，重启后名字，ip等都是一样的。保证了状态 #细节 kubectl explain StatefulSet.spec podManagementPolicy： OrderedReady（按序）、Parallel（并发） serviceName -required- 设置服务名，就可以用域名访问pod了。 pod-specific-string.serviceName.default.svc.cluster.local #测试 kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh ping stateful-tomcat-0.stateful-tomcat #我们在这里没有加存储卷。如果有的话 kubectl get pvc -l app=stateful-tomcat 我们就能看到即使Pod删了再拉起，卷还是同样的。 六、Job、CronJob 1、Job Kubernetes中的 Job 对象将创建一个或多个 Pod，并确保指定数量的 Pod 可以成功执行到进程正常结束： 当 Job 创建的 Pod 执行成功并正常结束时，Job 将记录成功结束的 Pod 数量 当成功结束的 Pod 达到指定的数量时，Job 将完成执行 删除 Job 对象时，将清理掉由 Job 创建的 Pod apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never #Job情况下，不支持Always backoffLimit: 4 #任务4次都没成，认为失败 activeDeadlineSeconds: 10 #默认这个任务需要成功执行一次。 #查看job情况 kubectl get job #修改下面参数设置再试试 #千万不要用阻塞容器。nginx。job由于Pod一直running状态。下一个永远得不到执行，而且超时了，当前running的Pod还会删掉 kubectl api-resources #参数说明 kubectl explain job.spec activeDeadlineSeconds：10 总共维持10s #该字段限定了 Job 对象在集群中的存活时长，一旦达到 .spec.activeDeadlineSeconds 指定的时长，该 Job 创建的所有的 Pod 都将被终止。但是Job不会删除，Job需要手动删除，或者使用ttl进行清理 backoffLimit： #设定 Job 最大的重试次数。该字段的默认值为 6；一旦重试次数达到了 backoffLimit 中的值，Job 将被标记为失败，且尤其创建的所有 Pod 将被终止； completions： #Job结束需要成功运行的Pods。默认为1 manualSelector： parallelism： #并行运行的Pod个数，默认为1 ttlSecondsAfterFinished： ttlSecondsAfterFinished: 0 #在job执行完时马上删除 ttlSecondsAfterFinished: 100 #在job执行完后，等待100s再删除 #除了 CronJob 之外，TTL 机制是另外一种自动清理已结束Job（Completed 或 Finished）的方式： #TTL 机制由 TTL 控制器 提供，ttlSecondsAfterFinished 字段可激活该特性 #当 TTL 控制器清理 Job 时，TTL 控制器将删除 Job 对象，以及由该 Job 创建的所有 Pod 对象。 # job超时以后 已经完成的不删，正在运行的Pod就删除 #单个Pod时，Pod成功运行，Job就结束了 #如果Job中定义了多个容器，则Job的状态将根据所有容器的执行状态来变化。 #Job任务不建议去运行nginx，tomcat，mysql等阻塞式的，否则这些任务永远完不了。 ##如果Job定义的容器中存在http server、mysql等长期的容器和一些批处理容器，则Job状态不会发生变化（因为长期运行的容器不会主动结束）。此时可以通过Pod的.status.containerStatuses获取指定容器的运行状态。 manualSelector： job同样可以指定selector来关联pod。需要注意的是job目前可以使用两个API组来操作，batch/v1和extensions/v1beta1。当用户需要自定义selector时，使用两种API组时定义的参数有所差异。 使用batch/v1时，用户需要将jod的spec.manualSelector设置为true，才可以定制selector。默认为false。 使用extensions/v1beta1时，用户不需要额外的操作。因为extensions/v1beta1的spec.autoSelector默认为false，该项与batch/v1的spec.manualSelector含义正好相反。换句话说，使用extensions/v1beta1时，用户不想定制selector时，需要手动将spec.autoSelector设置为true。 2、CronJob CronJob 按照预定的时间计划（schedule）创建 Job（注意：启动的是Job不是Deploy，rs）。一个 CronJob 对象类似于 crontab (cron table) 文件中的一行记录。该对象根据 Cron 格式定义的时间计划，周期性地创建 Job 对象。 Schedule 所有 CronJob 的 schedule 中所定义的时间，都是基于 master 所在时区来进行计算的。 一个 CronJob 在时间计划中的每次执行时刻，都创建 大约 一个 Job 对象。这里用到了 大约 ，是因为在少数情况下会创建两个 Job 对象，或者不创建 Job 对象。尽管 K8S 尽最大的可能性避免这种情况的出现，但是并不能完全杜绝此现象的发生。因此，Job 程序必须是 幂等的。 当以下两个条件都满足时，Job 将至少运行一次： startingDeadlineSeconds 被设置为一个较大的值，或者不设置该值（默认值将被采纳） concurrencyPolicy 被设置为 Allow # kubectl explain cronjob.spec concurrencyPolicy：并发策略 \"Allow\" (允许，default): \"Forbid\"(禁止): forbids；前个任务没执行完，要并发下一个的话，下一个会被跳过 \"Replace\"(替换): 新任务，替换当前运行的任务 failedJobsHistoryLimit：记录失败数的上限，Defaults to 1. successfulJobsHistoryLimit： 记录成功任务的上限。 Defaults to 3. #指定了 CronJob 应该保留多少个 completed 和 failed 的 Job 记录。将其设置为 0，则 CronJob 不会保留已经结束的 Job 的记录。 jobTemplate： job怎么定义（与前面我们说的job一样定义法） schedule： cron 表达式； startingDeadlineSeconds： 表示如果Job因为某种原因无法按调度准时启动，在spec.startingDeadlineSeconds时间段之内，CronJob仍然试图重新启动Job，如果在.spec.startingDeadlineSeconds时间之内没有启动成功，则不再试图重新启动。如果spec.startingDeadlineSeconds的值没有设置，则没有按时启动的任务不会被尝试重新启动。 suspend 暂停定时任务，对已经执行了的任务，不会生效； Defaults to false. apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" #分、时、日、月、周 jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 七、GC https://kubernetes.io/zh/docs/concepts/workloads/controllers/ttlafterfinished/ 这是alpha版本 这个特性现在在v1.12版本是alpha阶段，而且默认关闭的，需要手动开启。 需要修改的组件包括apiserver、controller还要scheduler。 apiserver、controller还要scheduler都是以pod的形式运行的，所以直接修改/etc/kubernetes/manifests下面对应的三个.yaml静态文件，加入 - --feature-gates=TTLAfterFinished=true 命令，然后重启对应的pod即可。 例如修改后的kube-scheduler.yaml的spec部分如下，kube-apiserver.yaml和kube-controller-manager.yaml也在spec部分加入- --feature-gates=TTLAfterFinished=true即可。 什么是垃圾回收 Kubernetes garbage collector（垃圾回收器）的作用是删除那些曾经有 owner，后来又不再有 owner 的对象。描述 垃圾收集器如何删除从属对象 当删除某个对象时，可以指定该对象的从属对象是否同时被自动删除，这种操作叫做级联删除（cascading deletion）。级联删除有两种模式：后台（background）和前台（foreground） 如果删除对象时不删除自动删除其从属对象，此时，从属对象被认为是孤儿（或孤立的 orphaned） 通过参数 --cascade，kubectl delete 命令也可以选择不同的级联删除策略： --cascade=true 级联删除 --cascade=false 不级联删除 orphan #删除rs，但不删除级联Pod kubectl delete replicaset my-repset --cascade=false Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/k8s/k8s_network.html":{"url":"cloud_learn/k8s/k8s_network.html","title":"04 Kubernetes网络","keywords":"","body":" Kubernetes 网络和负载均衡 一、Kubernetes网络 Kubernetes 网络解决四方面的问题： 一个 Pod 中的容器之间通过本地回路（loopback）通信。 集群网络在不同 pod 之间提供通信。Pod和Pod之间互通 Service 资源允许你对外暴露 Pods 中运行的应用程序，以支持来自于集群外部的访问。Service和Pod要通 可以使用 Services 来发布仅供集群内部使用的服务。 1、k8s网络架构图 1、架构图 2、访问流程 门面。所有的零散层上再抽取一个聚合层。 2、网络连通原理 1、Container To Container ip netns add ns1 #添加网络名称空间 ls /var/run/netns #查看所有网络名词空间 ip netns #查看所有网络名词空间 # Linux 将所有的进程都分配到 root network namespace，以使得进程可以访问外部网络 # Kubernetes 为每一个 Pod 都创建了一个 network namespace 2、Pod To Pod 1、同节点 2、跨节点 3、Pod-To-Service 1、Pod To Service 2、Service-To-Pod 4、Internet-To-Service 1、Pod-To-Internet 2、Internet-To-Pod（LoadBalancer -- Layer4） 3、Internet-To-Pod（Ingress-- Layer7） 二、Service 负载均衡服务。让一组Pod可以被别人进行服务发现。 Service --- >> 选择一组Pod 别人只需要访问这个Service。Service还会基于Pod的探针机制（ReadinessProbe：就绪探针）完成Pod的自动剔除和上线工作。 Service即使无头服务。别人（Pod）不能用ip访问，但是可以用service名当成域名访问。 Service的名字还能当成域名被Pod解析 1、基础概念 将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。 云原生服务发现 service中的type可选值如下，代表四种不同的服务发现类型 ExternalName ClusterIP: 为当前Service分配或者不分配集群IP。负载均衡一组Pod NodePort： 外界也可以使用机器ip+暴露的NodePort端口 访问。 nodePort端口由kube-proxy开在机器上 机器ip+暴露的NodePort 流量先来到 kube-proxy LoadBalancer. ClusterIP ：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ServiceType。 NodePort：通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到自动创建的 ClusterIP 服务。 通过请求 :，你可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。 ExternalName：通过返回 CNAME 和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。 无需创建任何类型代理。 1、创建简单Service apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: MyApp ## 使用选择器选择所有Pod # type: ClusterIP ##type很重要，不写默认是ClusterIP ports: - protocol: TCP port: 80 targetPort: 9376 Service 创建完成后，会对应一组EndPoint。可以kubectl get ep 进行查看 type有四种，每种对应不同服务发现机制 Servvice可以利用Pod的就绪探针机制，只负载就绪了的Pod。自动剔除没有就绪的Pod 2、创建无Selector的Service 我们可以创建Service不指定Selector 然后手动创建EndPoint，指定一组Pod地址。 此场景用于我们负载均衡其他中间件场景。 # 无selector的svc apiVersion: v1 kind: Service metadata: name: my-service-no-selector spec: ports: - protocol: TCP name: http ###一定注意，name可以不写， ###但是这里如果写了name，那么endpoint里面的ports必须有同名name才能绑定 port: 80 # service 80 targetPort: 80 #目标80 --- apiVersion: v1 kind: Endpoints metadata: name: my-service-no-selector ### ep和svc的绑定规则是：和svc同名同名称空间，port同名或同端口 namespace: default subsets: - addresses: - ip: 220.181.38.148 - ip: 39.156.69.79 - ip: 192.168.169.165 ports: - port: 80 name: http ## svc有name这里一定要有 protocol: TCP 原理：kube-proxy 在负责这个事情 https://kubernetes.io/zh/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies ## 实验 apiVersion: v1 kind: Service metadata: name: cluster-service-no-selector namespace: default spec: ## 不选中Pod而在下面手动定义可以访问的EndPoint type: ClusterIP ports: - name: abc port: 80 ## 访问当前service 的 80 targetPort: 80 ## 派发到Pod的 80 --- apiVersion: v1 kind: Endpoints metadata: name: cluster-service-no-selector ## 和service同名 namespace: default subsets: - addresses: - ip: 192.168.169.184 - ip: 192.168.169.165 - ip: 39.156.69.79 ports: - name: abc ## ep和service要是一样的 port: 80 protocol: TCP 场景：Pod要访问 MySQL。 MySQL单独部署到很多机器，每次记ip麻烦 集群内创建一个Service，实时的可以剔除EP信息。反向代理集群外的东西。 2、ClusterIP type: ClusterIP ClusterIP: 手动指定/None/\"\" 手动指定的ClusterIP必须在合法范围内 None会创建出没有ClusterIP的headless service（无头服务），Pod需要用服务的域名访问 3、NodePort apiVersion: v1 kind: Service metadata: name: my-service namespace: default type: NodePort ports: - protocol: TCP port: 80 # service 80 targetPort: 80 #目标80 nodePort: 32123 #自定义 如果将 type 字段设置为 NodePort，则 Kubernetes 将在 --service-node-port-range 标志指定的范围内分配端口（默认值：30000-32767） k8s集群的所有机器都将打开监听这个端口的数据，访问任何一个机器，都可以访问这个service对应的Pod 使用 nodePort 自定义端口 4、ExternalName apiVersion: v1 kind: Service metadata: name: my-service-05 namespace: default spec: type: ExternalName externalName: baidu.com 其他的Pod可以通过访问这个service而访问其他的域名服务 但是需要注意目标服务的跨域问题 5、LoadBalancer apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app.kubernetes.io/name: load-balancer-example name: my-service spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app.kubernetes.io/name: load-balancer-example type: LoadBalancer 6、扩展 - externalIP 在 Service 的定义中， externalIPs 可以和任何类型的 .spec.type 一通使用。在下面的例子中，客户端可通过 80.11.12.10:80 （externalIP:port） 访问my-service apiVersion: v1 kind: Service metadata: name: my-service-externalip spec: selector: app: canary-nginx ports: - name: http protocol: TCP port: 80 targetPort: 80 externalIPs: ### 定义只有externalIPs指定的地址才可以访问这个service - 10.170.0.111 ### 集群内的ip都不行？ #### - 其他机器的ip 黑名单？？？？ 7、扩展 - Pod的DNS apiVersion: v1 kind: Service metadata: name: default-subdomain spec: selector: name: busybox clusterIP: None ports: - name: foo # 实际上不需要指定端口号 port: 1234 targetPort: 1234 --- apiVersion: v1 kind: Pod metadata: name: busybox1 labels: name: busybox spec: hostname: busybox-1 subdomain: default-subdomain ## 指定必须和svc名称一样，才可以 podName.subdomain.名称空间.svc.cluster.local访问。否则访问不同指定Pod containers: - image: busybox:1.28 command: - sleep - \"3600\" name: busybox --- apiVersion: v1 kind: Pod metadata: name: busybox2 labels: name: busybox spec: hostname: busybox-2 ### 每个Pod指定主机名 subdomain: default-subdomain ## subdomain等于sevrice的名 containers: - image: busybox:1.28 command: - sleep - \"3600\" name: busybox 访问 busybox-1.default-subdomain.default.svc.cluster.local 可以访问到busybox-1。 访问Service 同名称空间 ping service-name 即可 不同名称空间 ping service-name.namespace 即可 访问Pod 同名称空间 ping pod-host-name.service-name 即可 不同名称空间 ping pod-host-name.service-name.namespace 即可 busybox-1.default-subdomain.default** Pod的hostName.service的名.名称空间的名 想要使用域名访问的模式，必须加Service网络的名字 三、Ingress 为什么需要Ingress？ Service可以使用NodePort暴露集群外访问端口，但是性能低下不安全 缺少Layer7的统一访问入口，可以负载均衡、限流等 Ingress 公开了从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。 我们使用Ingress作为整个集群统一的入口，配置Ingress规则转到对应的Service 1、Ingress nginx和nginx ingress 1、nginx ingress 这是nginx官方做的，适配k8s的，分为开源版和nginx plus版（收费）。文档地址 https://www.nginx.com/products/nginx-ingress-controller 2、ingress nginx https://kubernetes.io/zh/docs/concepts/services-networking/ingress/#ingress-%E6%98%AF%E4%BB%80%E4%B9%88 这是k8s官方做的，适配nginx的。这个里面会及时更新一些特性，而且性能很高，也被广泛采用。文档地址 ## 默认安装使用这个镜像 registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0 https://kubernetes.github.io/ingress-nginx/examples/auth/basic/ 文档地址 2、ingress nginx 安装 1、安装 自建集群使用裸金属安装方式 需要如下修改： 修改ingress-nginx-controller镜像为 registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0 修改Deployment为DaemonSet比较好 修改Container使用主机网络，直接在主机上开辟 80,443端口，无需中间解析，速度更快 Container使用主机网络，对应的dnsPolicy策略也需要改为主机网络的 修改Service为ClusterIP，无需NodePort模式了 修改DaemonSet的nodeSelector: ingress-node=true 。这样只需要给node节点打上ingress-node=true 标签，即可快速的加入/剔除 ingress-controller的数量 kubectl label node k8s-node01 node-role=ingress 修改好的yaml如下。大家直接复制使用 apiVersion: v1 kind: Namespace metadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx automountServiceAccountToken: true --- # Source: ingress-nginx/templates/controller-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx data: --- # Source: ingress-nginx/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx rules: - apiGroups: - '' resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - '' resources: - nodes verbs: - get - apiGroups: - '' resources: - services verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses verbs: - get - list - watch - apiGroups: - '' resources: - events verbs: - create - patch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io # k8s 1.14+ resources: - ingressclasses verbs: - get - list - watch --- # Source: ingress-nginx/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - '' resources: - namespaces verbs: - get - apiGroups: - '' resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - '' resources: - services verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io # k8s 1.14+ resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io # k8s 1.14+ resources: - ingressclasses verbs: - get - list - watch - apiGroups: - '' resources: - configmaps resourceNames: - ingress-controller-leader-nginx verbs: - get - update - apiGroups: - '' resources: - configmaps verbs: - create - apiGroups: - '' resources: - events verbs: - create - patch --- # Source: ingress-nginx/templates/controller-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-service-webhook.yaml apiVersion: v1 kind: Service metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller-admission namespace: ingress-nginx spec: type: ClusterIP ports: - name: https-webhook port: 443 targetPort: webhook selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller --- # Source: ingress-nginx/templates/controller-service.yaml：不要 apiVersion: v1 kind: Service metadata: annotations: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx spec: type: ClusterIP ## 改为clusterIP ports: - name: http port: 80 protocol: TCP targetPort: http - name: https port: 443 protocol: TCP targetPort: https selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller --- # Source: ingress-nginx/templates/controller-deployment.yaml apiVersion: apps/v1 kind: DaemonSet metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx spec: selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller revisionHistoryLimit: 10 minReadySeconds: 0 template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller spec: dnsPolicy: ClusterFirstWithHostNet ## dns对应调整为主机网络 hostNetwork: true ## 直接让nginx占用本机80端口和443端口，所以使用主机网络 containers: - name: controller image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE runAsUser: 101 allowPrivilegeEscalation: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so livenessProbe: httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 1 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 1 successThreshold: 1 failureThreshold: 3 ports: - name: http containerPort: 80 protocol: TCP - name: https containerPort: 443 protocol: TCP - name: webhook containerPort: 8443 protocol: TCP volumeMounts: - name: webhook-cert mountPath: /usr/local/certificates/ readOnly: true resources: requests: cpu: 100m memory: 90Mi nodeSelector: ## 节点选择器 node-role: ingress #以后只需要给某个node打上这个标签就可以部署ingress-nginx到这个节点上了 #kubernetes.io/os: linux ## 修改节点选择 serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- # Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml # before changing this value, check the required kubernetes version # https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisites apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook name: ingress-nginx-admission webhooks: - name: validate.nginx.ingress.kubernetes.io matchPolicy: Equivalent rules: - apiGroups: - networking.k8s.io apiVersions: - v1beta1 operations: - CREATE - UPDATE resources: - ingresses failurePolicy: Fail sideEffects: None admissionReviewVersions: - v1 - v1beta1 clientConfig: service: namespace: ingress-nginx name: ingress-nginx-controller-admission path: /networking/v1beta1/ingresses --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginx rules: - apiGroups: - '' resources: - secrets verbs: - get - create --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-create annotations: helm.sh/hook: pre-install,pre-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginx spec: template: metadata: name: ingress-nginx-admission-create labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: create image: docker.io/jettech/kube-webhook-certgen:v1.5.1 imagePullPolicy: IfNotPresent args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission securityContext: runAsNonRoot: true runAsUser: 2000 --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-patch annotations: helm.sh/hook: post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook namespace: ingress-nginx spec: template: metadata: name: ingress-nginx-admission-patch labels: helm.sh/chart: ingress-nginx-3.30.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.46.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: patch image: docker.io/jettech/kube-webhook-certgen:v1.5.1 imagePullPolicy: IfNotPresent args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission securityContext: runAsNonRoot: true runAsUser: 2000 2、验证 访问部署了ingress-nginx主机的80端口，有nginx响应即可。 2、卸载 kubectl delete -f ingress-controller.yaml 即可 3、案例实战 1、基本配置 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: itdachang-ingress namespace: default spec: rules: - host: itdachang.com http: paths: - path: / pathType: Prefix backend: ## 指定需要响应的后端服务 service: name: my-nginx-svc ## kubernetes集群的svc名称 port: number: 80 ## service的端口号 pathType 详细： Prefix：基于以 / 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 / 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。 Exact：精确匹配 URL 路径，且区分大小写。 ImplementationSpecific：对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。 ingress规则会生效到所有按照了IngressController的机器的nginx配置。 2、默认后端 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: itdachang-ingress namespace: default spec: defaultBackend: ## 指定所有未匹配的默认后端 service: name: php-apache port: number: 80 rules: - host: itdachang.com http: paths: - path: /abc pathType: Prefix backend: service: name: my-nginx-svc port: number: 80 效果 itdachang.com 下的 非 /abc 开头的所有请求，都会到defaultBackend 非itdachang.com 域名下的所有请求，也会到defaultBackend nginx的全局配置 kubectl edit cm ingress-nginx-controller -n ingress-nginx 编辑配置加上 data: 配置项: 配置值 所有配置项参考 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/ 基于环境变量带去的 3、路径重写 https://kubernetes.github.io/ingress-nginx/examples/rewrite/ Rewrite 功能，经常被用于前后分离的场景 前端给服务器发送 / 请求映射前端地址。 后端给服务器发送 /api 请求来到对应的服务。但是后端服务没有 /api的起始路径，所以需要ingress-controller自动截串 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: ## 写好annotion #https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/ nginx.ingress.kubernetes.io/rewrite-target: /$2 ### 只保留哪一部分 name: rewrite-ingress-02 namespace: default spec: rules: ## 写好规则 - host: itzongchang.com http: paths: - backend: service: name: php-apache port: number: 80 path: /api(/|$)(.*) pathType: Prefix 4、配置SSL https://kubernetes.github.io/ingress-nginx/user-guide/tls/ 生成证书：（也可以去青云申请免费证书进行配置） $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ${KEY_FILE:tls.key} -out ${CERT_FILE:tls.cert} -subj \"/CN=${HOST:itdachang.com}/O=${HOST:itdachang.com}\" kubectl create secret tls ${CERT_NAME:itdachang-tls} --key ${KEY_FILE:tls.key} --cert ${CERT_FILE:tls.cert} ## 示例命令如下 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.cert -subj \"/CN=it666.com/O=it666.com\" kubectl create secret tls it666-tls --key tls.key --cert tls.cert apiVersion: v1 data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJekNDQWd1Z0F3SUJBZ0lKQVB6YXVMQ1ZjdlVKTUEwR0NTcUdTSWIzRFFFQkN3VUFNQ2d4RWpBUUJnTlYKQkFNTUNXbDBOalkyTG1OdmJURVNNQkFHQTFVRUNnd0phWFEyTmpZdVkyOXRNQjRYRFRJeE1EVXhNREV5TURZdwpNRm9YRFRJeU1EVXhNREV5TURZd01Gb3dLREVTTUJBR0ExVUVBd3dKYVhRMk5qWXVZMjl0TVJJd0VBWURWUVFLCkRBbHBkRFkyTmk1amIyMHdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDbkNYa0wKNjdlYzNjYW5IU1V2VDR6YXZmMGpsOEFPWlBtUERhdUFRTElEby80LzlhV2JPSy9yZm5OelVXV3lTRFBqb3pZVApWa2xmQTZYRG1xRU5FSWRHRlhjdExTSlRNRkM5Y2pMeTlwYVFaaDVYemZId0ZoZXZCR1J3MmlJNXdVdk5iTGdWCmNzcmRlNXlKMEZYOFlMZFRhdjhibzhjTXpxN2FqZXhXMWc1dkxmTWZhczAvd2VyVk9Qc0ZmS3RwZ1dwSWMxMXEKekx6RnlmWHNjcVNhVTV2NFo5WHFqQjRtQjhZZ043U2FSa2pzU0VsSFU4SXhENEdTOUtTNGtkR2xZak45V2hOcAp6aG5MdllpSDIrZThQWE9LdU8wK2Jla1MrS3lUS2hnNnFWK21kWTN0MWJGenpCdjFONTVobTNQTldjNk9ROTh3CkYrQk9uUUNhWExKVmRRcS9BZ01CQUFHalVEQk9NQjBHQTFVZERnUVdCQlNzSUFvMHZ4RFZjVWtIZ1V1TFlwY0wKdjBFSERqQWZCZ05WSFNNRUdEQVdnQlNzSUFvMHZ4RFZjVWtIZ1V1TFlwY0x2MEVIRGpBTUJnTlZIUk1FQlRBRApBUUgvTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFDSjFEdGJoQnBacTE1ODVEMGlYV1RTdmU3Q2YvQ3VnakxZCjNYb2gwSU9sNy9mVmNndFJkWXlmRFBmRDFLN0l4bElETWtUbTVEVWEyQzBXaFY5UlZLU0poSTUzMmIyeVRGcm8Kc053eGhkcUZpOC9CU1lsQTl0Tk5HeXhKT1RKZWNtSUhsaFhjRlEvUzFaK3FjVWNrTVh6UHlIcFl0VjRaU0hheQpFWVF2bUVBZTFMNmlnRk8wc2xhbUllTFBCTWhlTDNnSDZQNlV3TVpQbTRqdFR1d2FGSmZGRlRIakQydmhSQkJKCmZjTGY5QjN3U3k2cjBDaXF2VXQxQUNQVnpSdFZrcWJJV1d5VTBDdkdjVDVIUUxPLzdhTE4vQkxpNGdYV2o1MUwKVXdTQzhoY2xodVp3SmRzckNkRlltcjhTMnk0UDhsaDdBc0ZNOGorNjh1ZHJlYXovWmFNbwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== tls.key: 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 kind: Secret metadata: creationTimestamp: \"2021-05-10T12:06:22Z\" name: it666-tls namespace: default resourceVersion: \"2164722\" uid: 16f8a4b6-1600-4ded-8458-b0480ce075ba type: kubernetes.io/tls 配置域名使用证书； apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: itdachang-ingress namespace: default spec: tls: - hosts: - itdachang.com secretName: itdachang-tls rules: - host: itdachang.com http: paths: - path: / pathType: Prefix backend: service: name: my-nginx-svc port: number: 80 配置好证书，访问域名，就会默认跳转到https； 5、限速 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#rate-limiting apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-222333 namespace: default annotations: ##注解 nginx.ingress.kubernetes.io/limit-rps: \"1\" ### 限流的配置 spec: defaultBackend: ## 只要未指定的映射路径 service: name: php-apache port: number: 80 rules: - host: it666.com http: paths: - path: /bbbbb pathType: Prefix backend: service: name: cluster-service-222 port: number: 80 6、灰度发布-Canary 以前可以使用k8s的Service配合Deployment进行金丝雀部署。原理如下 缺点： 不能自定义灰度逻辑，比如指定用户进行灰度 现在可以使用Ingress进行灰度。原理如下 ## 使用如下文件部署两个service版本。v1版本返回nginx默认页，v2版本返回 11111 apiVersion: v1 kind: Service metadata: name: v1-service namespace: default spec: selector: app: v1-pod type: ClusterIP ports: - name: http port: 80 targetPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: Deployment metadata: name: v1-deploy namespace: default labels: app: v1-deploy spec: selector: matchLabels: app: v1-pod replicas: 1 template: metadata: labels: app: v1-pod spec: containers: - name: nginx image: nginx --- apiVersion: v1 kind: Service metadata: name: canary-v2-service namespace: default spec: selector: app: canary-v2-pod type: ClusterIP ports: - name: http port: 80 targetPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: Deployment metadata: name: canary-v2-deploy namespace: default labels: app: canary-v2-deploy spec: selector: matchLabels: app: canary-v2-pod replicas: 1 template: metadata: labels: app: canary-v2-pod spec: containers: - name: nginx image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/nginx-test:env-msg 7、会话保持-Session亲和性 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#session-affinity 第一次访问，ingress-nginx会返回给浏览器一个Cookie，以后浏览器带着这个Cookie，保证访问总是抵达之前的Pod； ## 部署一个三个Pod的Deployment并设置Service apiVersion: v1 kind: Service metadata: name: session-affinity namespace: default spec: selector: app: session-affinity type: ClusterIP ports: - name: session-affinity port: 80 targetPort: 80 protocol: TCP --- apiVersion: apps/v1 kind: Deployment metadata: name: session-affinity namespace: default labels: app: session-affinity spec: selector: matchLabels: app: session-affinity replicas: 3 template: metadata: labels: app: session-affinity spec: containers: - name: session-affinity image: nginx 编写具有会话亲和的ingress ### 利用每次请求携带同样的cookie，来标识是否是同一个会话 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: session-test namespace: default annotations: nginx.ingress.kubernetes.io/affinity: \"cookie\" nginx.ingress.kubernetes.io/session-cookie-name: \"itdachang-session\" spec: rules: - host: it666.com http: paths: - path: / ### 如果以前这个域名下的这个路径相同的功能有配置过，以最后一次生效 pathType: Prefix backend: service: name: session-affinity ### port: number: 80 四、NetworkPolicy 网络策略（网络隔离策略） https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/ 指定Pod间的网络隔离策略，默认是所有互通。 Pod 之间互通，是通过如下三个标识符的组合来辩识的： 其他被允许的 Pods（例外：Pod 无法阻塞对自身的访问） 被允许的名称空间 IP 组块（例外：与 Pod 运行所在的节点的通信总是被允许的， 无论 Pod 或节点的 IP 地址） 1、Pod隔离与非隔离 默认情况下，Pod网络都是非隔离的（non-isolated），可以接受来自任何请求方的网络请求。 如果一个 NetworkPolicy 的标签选择器选中了某个 Pod，则该 Pod 将变成隔离的（isolated），并将拒绝任何不被 NetworkPolicy 许可的网络连接。 2、规约 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: ## 选中指定Pod matchLabels: role: db policyTypes: ## 定义上面Pod的入站出站规则 - Ingress - Egress ingress: ## 定义入站白名单 - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: ## 定义出站白名单 - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 基本信息： 同其他的 Kubernetes 对象一样，NetworkPolicy 需要 apiVersion、kind、metadata 字段 spec：NetworkPolicy的spec字段包含了定义网络策略的主要信息： podSelector： 同名称空间中，符合此标签选择器 .spec.podSelector 的 Pod 都将应用这个 NetworkPolicy。上面的 Example中的 podSelector 选择了 role=db 的 Pod。如果该字段为空，则将对名称空间中所有的 Pod 应用这个 NetworkPolicy policyTypes： .spec.policyTypes 是一个数组类型的字段，该数组中可以包含 Ingress、Egress 中的一个，也可能两个都包含。该字段标识了此 NetworkPolicy 是否应用到 入方向的网络流量、出方向的网络流量、或者两者都有。如果不指定 policyTypes 字段，该字段默认将始终包含 Ingress，当 NetworkPolicy 中包含出方向的规则时，Egress 也将被添加到默认值。 ingress：ingress是一个数组，代表入方向的白名单规则。每一条规则都将允许与from和ports匹配的入方向的网络流量发生。例子中的ingress包含了一条规则，允许的入方向网络流量必须符合如下条件： Pod 的监听端口为 6379 请求方可以是如下三种来源当中的任意一种： ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段 namespaceSelector 标签选择器，匹配标签为 project=myproject podSelector 标签选择器，匹配标签为 role=frontend egress：egress是一个数组，代表出方向的白名单规则。每一条规则都将允许与to和ports匹配的出方向的网络流量发生。例子中的egress允许的出方向网络流量必须符合如下条件： 目标端口为 5978 目标 ipBlock 为 10.0.0.0/24 网段 因此，例子中的 NetworkPolicy 对网络流量做了如下限制： 隔离了 default 名称空间中带有 role=db 标签的所有 Pod 的入方向网络流量和出方向网络流量 Ingress规则（入方向白名单规则）： 当请求方是如下三种来源当中的任意一种时，允许访问default名称空间中所有带role=db标签的 Pod 的6379端口： ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段 namespaceSelector 标签选择器，匹配标签为 project=myproject podSelector 标签选择器，匹配标签为 role=frontend Egress规则（出方向白名单规则）： 当如下条件满足时，允许出方向的网络流量： 目标端口为 5978 目标 ipBlock 为 10.0.0.0/24 网段 3、to和from选择器的行为 NetworkPolicy 的 .spec.ingress.from 和 .spec.egress.to 字段中，可以指定 4 种类型的标签选择器： podSelector 选择与 NetworkPolicy 同名称空间中的 Pod 作为入方向访问控制规则的源或者出方向访问控制规则的目标 namespaceSelector 选择某个名称空间（其中所有的Pod）作为入方向访问控制规则的源或者出方向访问控制规则的目标 namespaceSelector 和 podSelector 在一个 to / from 条目中同时包含 namespaceSelector 和 podSelector 将选中指定名称空间中的指定 Pod。此时请特别留意 YAML 的写法，如下所示： ... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... 该例子中，podSelector 前面没有 - 减号，namespaceSelector 和 podSelector 是同一个 from 元素的两个字段，将选中带 user=alice 标签的名称空间中所有带 role=client 标签的 Pod。但是，下面的这个 NetworkPolicy 含义是不一样的： ... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ... 后者，podSelector 前面带 - 减号，说明 namespaceSelector 和 podSelector 是 from 数组中的两个元素，他们将选中 NetworkPolicy 同名称空间中带 role=client 标签的对象，以及带 user=alice 标签的名称空间的所有 Pod。 前者是交集关系（且），后者是并集关系（或） ipBlock 可选择 IP CIDR 范围作为入方向访问控制规则的源或者出方向访问控制规则的目标。这里应该指定的是集群外部的 IP，因为集群内部 Pod 的 IP 地址是临时分配的，且不可预测。 集群的入方向和出方向网络机制通常需要重写网络报文的 source 或者 destination IP。kubernetes 并未定义应该在处理 NetworkPolicy 之前还是之后再修改 source / destination IP，因此，在不同的云供应商、使用不同的网络插件时，最终的行为都可能不一样。这意味着： 对于入方向的网络流量，某些情况下，你可以基于实际的源 IP 地址过滤流入的报文；在另外一些情况下，NetworkPolicy 所处理的 \"source IP\" 可能是 LoadBalancer 的 IP 地址，或者其他地址 对于出方向的网络流量，基于 ipBlock 的策略可能有效，也可能无效 4、场景 https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/#default-policies Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/kubesphare/k8s_commond.html":{"url":"cloud_learn/kubesphare/k8s_commond.html","title":"Kubernetes核心实战","keywords":"","body":"Kubernetes核心实战 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/kubesphare/kubesphere_install.html":{"url":"cloud_learn/kubesphare/kubesphere_install.html","title":"kubesphere安装","keywords":"","body":"kubesphere安装 安装步骤 选择4核8G（master）、8核16G（node1）、8核16G（node2） 三台机器，按量付费进行实验，CentOS7.9 安装Docker 安装Kubernetes 安装KubeSphere前置环境 安装KubeSphere 1、安装Docker sudo yum remove docker* sudo yum install -y yum-utils #配置docker的yum地址 sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #安装指定版本 sudo yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6 # 启动&开机启动docker systemctl enable docker --now # docker加速配置 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json 2、安装Kubernetes 1、基本环境 每个机器使用内网ip互通 每个机器配置自己的hostname，不能用localhost #设置每个机器自己的hostname hostnamectl set-hostname xxx # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config #关闭swap swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab #允许 iptables 检查桥接流量 cat 2、安装kubelet、kubeadm、kubectl #配置k8s的yum源地址 cat #### 所有机器配置master域名 echo \"172.31.0.100 k8s-master\" >> /etc/hosts 3、初始化master节点 1、初始化 kubeadm init \\ --apiserver-advertise-address=172.31.0.100 \\ --control-plane-endpoint=k8s-master \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.20.9 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=192.168.0.0/16 2、记录关键信息 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join k8s-master:6443 --token 0ns2n4.crdjrcdlt7hyfu3s \\ --discovery-token-ca-cert-hash sha256:d680191c7404c926f5a7b3478524a8a3c2c9a69370b7fff1c8fb8e4f76d85118 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join k8s-master:6443 --token 0ns2n4.crdjrcdlt7hyfu3s \\ --discovery-token-ca-cert-hash sha256:d680191c7404c926f5a7b3478524a8a3c2c9a69370b7fff1c8fb8e4f76d85118 3、安装Calico网络插件 配置管理信息 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config [root@master /opt/soft]# curl https://docs.projectcalico.org/manifests/calico.yaml -O % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 212k 100 212k 0 0 259k 0 --:--:-- --:--:-- --:--:-- 258k [root@master /opt/soft]# kubectl apply -f calico.yaml 4、加入worker节点 kubeadm join k8s-master:6443 --token 24qoxf.nhkjv248ihu3zh4c \\ --discovery-token-ca-cert-hash sha256:0b5cd2a4311042e68670e6a2138edf930c56228fc9978b17f6df6db9b1372855 3、安装KubeSphere前置环境 1、nfs文件系统 1、安装nfs-server 在每个机器。 yum install -y nfs-utils # 在master 执行以下命令 echo \"/nfs/data/ *(insecure,rw,sync,no_root_squash)\" > /etc/exports # 执行以下命令，启动 nfs 服务;创建共享目录 mkdir -p /nfs/data # 在master执行 systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server # 使配置生效 exportfs -r #检查配置是否生效 exportfs 2、配置nfs-client（选做） showmount -e 172.31.0.100 mkdir -p /nfs/data mount -t nfs 172.31.0.100:/nfs/data /nfs/data 3、配置默认存储 配置动态供应的默认存储类 ## 创建了一个存储类 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: k8s-sigs.io/nfs-subdir-external-provisioner parameters: archiveOnDelete: \"true\" ## 删除pv的时候，pv的内容是否要备份 --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/nfs-subdir-external-provisioner:v4.0.2 # resources: # limits: # cpu: 10m # requests: # cpu: 10m volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 172.31.0.100 ## 指定自己nfs服务器地址 - name: NFS_PATH value: /nfs/data ## nfs服务器共享的目录 volumes: - name: nfs-client-root nfs: server: 172.31.0.100 path: /nfs/data --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 确认配置是否生效 kubectl get sc 2、metrics-server apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rbac.authorization.k8s.io/aggregate-to-view: \"true\" name: system:aggregated-metrics-reader rules: - apiGroups: - metrics.k8s.io resources: - pods - nodes verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server name: system:metrics-server rules: - apiGroups: - \"\" resources: - pods - nodes - nodes/stats - namespaces - configmaps verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: v1 kind: Service metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: ports: - name: https port: 443 protocol: TCP targetPort: https selector: k8s-app: metrics-server --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: selector: matchLabels: k8s-app: metrics-server strategy: rollingUpdate: maxUnavailable: 0 template: metadata: labels: k8s-app: metrics-server spec: containers: - args: - --cert-dir=/tmp - --kubelet-insecure-tls - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/metrics-server:v0.4.3 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /livez port: https scheme: HTTPS periodSeconds: 10 name: metrics-server ports: - containerPort: 4443 name: https protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /readyz port: https scheme: HTTPS periodSeconds: 10 securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /tmp name: tmp-dir nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: metrics-server volumes: - emptyDir: {} name: tmp-dir --- apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: labels: k8s-app: metrics-server name: v1beta1.metrics.k8s.io spec: group: metrics.k8s.io groupPriorityMinimum: 100 insecureSkipTLSVerify: true service: name: metrics-server namespace: kube-system version: v1beta1 versionPriority: 100 4 、安装KubeSphere https://kubesphere.com.cn/ 1、下载核心文件 如果下载不到，请复制附录的内容 wget https://github.com/kubesphere/ks-installer/releases/download/v3.1.1/kubesphere-installer.yaml wget https://github.com/kubesphere/ks-installer/releases/download/v3.1.1/cluster-configuration.yaml 2、修改cluster-configuration 在 cluster-configuration.yaml中指定我们需要开启的功能 参照官网“启用可插拔组件” https://kubesphere.com.cn/docs/pluggable-components/overview/ 3、执行安装 kubectl apply -f kubesphere-installer.yaml kubectl apply -f cluster-configuration.yaml 4、查看安装进度 kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/kubesphare/kubesphere_set_mysql.html":{"url":"cloud_learn/kubesphare/kubesphere_set_mysql.html","title":"kubesphere安装mysql","keywords":"","body":"部署mysql 1、进入配置中心，添加配置 2、创建配置 key：配置文件名称 value:配置文件内容 确定后点右下角确定，完成后点创建 3、创建pvc 4、创建 5、创建服务 不暴露外网访问 暴露外网访问 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/kubesphare/kubesphere_set_es.html":{"url":"cloud_learn/kubesphare/kubesphere_set_es.html","title":"kubesphere安装es","keywords":"","body":" cluster.name: \"docker-cluster\" network.host: 0.0.0.0 ################################################################ ## ## JVM configuration ## ################################################################ ## ## WARNING: DO NOT EDIT THIS FILE. If you want to override the ## JVM options in this file, or set any additional options, you ## should create one or more files in the jvm.options.d ## directory containing your adjustments. ## ## See https://www.elastic.co/guide/en/elasticsearch/reference/current/jvm-options.html ## for more information. ## ################################################################ ################################################################ ## IMPORTANT: JVM heap size ################################################################ ## ## The heap size is automatically configured by Elasticsearch ## based on the available memory in your system and the roles ## each node is configured to fulfill. If specifying heap is ## required, it should be done through a file in jvm.options.d, ## and the min and max should be set to the same value. For ## example, to set the heap to 4 GB, create a new file in the ## jvm.options.d directory containing these lines: ## ## -Xms4g ## -Xmx4g ## ## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html ## for more information ## ################################################################ ################################################################ ## Expert settings ################################################################ ## ## All settings below here are considered expert settings. Do ## not adjust them unless you understand what you are doing. Do ## not edit them in this file; instead, create a new file in the ## jvm.options.d directory containing your adjustments. ## ################################################################ ## GC configuration 8-13:-XX:+UseConcMarkSweepGC 8-13:-XX:CMSInitiatingOccupancyFraction=75 8-13:-XX:+UseCMSInitiatingOccupancyOnly ## G1GC Configuration # NOTE: G1 GC is only supported on JDK version 10 or later # to use G1GC, uncomment the next two lines and update the version on the # following three lines to your version of the JDK # 10-13:-XX:-UseConcMarkSweepGC # 10-13:-XX:-UseCMSInitiatingOccupancyOnly 14-:-XX:+UseG1GC ## JVM temporary directory -Djava.io.tmpdir=${ES_TMPDIR} ## heap dumps # generate a heap dump when an allocation from the Java heap fails; heap dumps # are created in the working directory of the JVM unless an alternative path is # specified -XX:+HeapDumpOnOutOfMemoryError # specify an alternative path for heap dumps; ensure the directory exists and # has sufficient space -XX:HeapDumpPath=data # specify an alternative path for JVM fatal error logs -XX:ErrorFile=logs/hs_err_pid%p.log ## JDK 8 GC logging 8:-XX:+PrintGCDetails 8:-XX:+PrintGCDateStamps 8:-XX:+PrintTenuringDistribution 8:-XX:+PrintGCApplicationStoppedTime 8:-Xloggc:logs/gc.log 8:-XX:+UseGCLogFileRotation 8:-XX:NumberOfGCLogFiles=32 8:-XX:GCLogFileSize=64m # JDK 9+ GC logging 9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/kubesphare/kubesphere_set_redis.html":{"url":"cloud_learn/kubesphare/kubesphere_set_redis.html","title":"kubesphere安装redis","keywords":"","body":" Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/kubesphare/kubesphere_set_store.html":{"url":"cloud_learn/kubesphare/kubesphere_set_store.html","title":"kubesphere应用商店安装组件","keywords":"","body":"kubesphere应用商店安装组件 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/kubesphare/kubesphere_ruoyi.html":{"url":"cloud_learn/kubesphare/kubesphere_ruoyi.html","title":"kubesphere手动上云","keywords":"","body":"1、基础环境构建 2、打包jar dockerfile FROM openjdk:8-jdk LABEL maintainer=leifengyang #docker run -e PARAMS=\"--server.port 9090\" ENV PARAMS=\"--server.port=8080 --spring.profiles.active=prod --spring.cloud.nacos.discovery.server-addr=his-nacos.his:8848 --spring.cloud.nacos.config.server-addr=his-nacos.his:8848 --spring.cloud.nacos.config.namespace=prod --spring.cloud.nacos.config.file-extension=yml\" #修改镜像的时区 RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && echo 'Asia/Shanghai' >/etc/timezone #拷贝jar文件 COPY target/*.jar /app.jar #指定启动端口为8080 EXPOSE 8080 # ENTRYPOINT [\"/bin/sh\",\"-c\",\"java -Dfile.encoding=utf8 -Djava.security.egd=file:/dev/./urandom -jar app.jar ${PARAMS}\"] env 环境变量(key =v) ,params由ENTRYPOINT引用 --spring.profiles.active=prod 指定运行环境为生产 --spring.cloud.nacos.discovery.server-addr=his-nacos.his:8848 指定nacos地址 制作镜像 [root@k8s-master /opt/soft/docker/ruoyi-auth]# docker build -t ruoyi-auth:v1.0 -f Dockerfile . Sending build context to Docker daemon 88.26MB Step 1/7 : FROM openjdk:8-jdk 8-jdk: Pulling from library/openjdk 0e29546d541c: Pull complete 9b829c73b52b: Pull complete cb5b7ae36172: Pull complete 6494e4811622: Pull complete 668f6fcc5fa5: Pull complete c0879393b07e: Pull complete bef50c41a74d: Pull complete Digest: sha256:8a9d5c43f540e8d0c003c723a2c8bd20ae350a2efed6fb5719cae33b026f8e7c Status: Downloaded newer image for openjdk:8-jdk ---> e24ac15e052e Step 2/7 : LABEL maintainer=leifengyang ---> Running in 10c459ae4c18 Removing intermediate container 10c459ae4c18 ---> 7542de6ebb68 Step 3/7 : ENV PARAMS=\"--server.port=8080 --spring.profiles.active=prod --spring.cloud.nacos.discovery.server-addr=his-nacos.his:8848 --spring.cloud.nacos.config.server-addr=his-nacos.his:8848 --spring.cloud.nacos.config.namespace=prod --spring.cloud.nacos.config.file-extension=yml\" ---> Running in c8775ab87014 Removing intermediate container c8775ab87014 ---> 83411e1271e0 Step 4/7 : RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && echo 'Asia/Shanghai' >/etc/timezone ---> Running in 1eb3c41fe1bb Removing intermediate container 1eb3c41fe1bb ---> dda389346a2f Step 5/7 : COPY target/*.jar /app.jar ---> f76ce70d0dc9 Step 6/7 : EXPOSE 8080 ---> Running in 60a64890c073 Removing intermediate container 60a64890c073 ---> f660e42afce2 Step 7/7 : ENTRYPOINT [\"/bin/sh\",\"-c\",\"java -Dfile.encoding=utf8 -Djava.security.egd=file:/dev/./urandom -jar app.jar ${PARAMS}\"] ---> Running in 9aa06e6d2d22 Removing intermediate container 9aa06e6d2d22 ---> ef23430dc684 Successfully built ef23430dc684 Successfully tagged ruoyi-auth:v1.0 推送镜像到阿里云 开通阿里云容器服务 ​ 创建一个名称空间 推送镜像到阿里云 #登录阿里云 sudo docker login --username=1355997****@139.com registry.cn-hangzhou.aliyuncs.com #把本地镜像，改名成符合阿里云名字规范的镜像 # $ docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/镜像:[镜像版本号] docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/ $ docker push registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/镜像:[镜像版本号] 若依所有的镜像 $ docker pull registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/ruoyi-visual-monitor:V1.0 $ docker pull registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/ruoyi-job:V1.0 $ docker pull registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/ruoyi-auth:V1.0 $ docker pull registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/ruoyi-gateway:V1.0 $ docker pull registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/ruoyi-file:V1.0 $ docker pull registry.cn-hangzhou.aliyuncs.com/wjh_ruoyi/ruoyi-system:V1.0 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/devops/set_jenkins.html":{"url":"cloud_learn/devops/set_jenkins.html","title":"devops部署","keywords":"","body":"前置准备工作 中间件部署 1、部署Sentinel a、创建优状态服务 b、指定容器镜像 C、默认下一步创建 2、MongoDB创建 选择应用模板进行创建 配置账户密码 密钥配置 配置案例云镜像仓库信息 1、创建名称 2、配置案例云仓库地址（或其他仓库） 网络环境优化 配置maven 使用admin登陆ks 进入集群管理 进入配置中心 找到配置 ks-devops-agent 修改这个配置。加入maven阿里云镜像加速地址 nexus-aliyun *,!jeecg,!jeecg-snapshots Nexus aliyun http://maven.aliyun.com/nexus/content/groups/public 创建devops工程 1、创建devops 进入分公司，找到devops工程，点击创建 2、创建凭证 1、创建git凭证 2、创建案例镜像登录凭证 3、进入工程创建流水线 方式1：手动配置Jenkins 1、配置流水线名称 2、编辑流水线 选择持续集成模板 1、拉取代码 a) 指定容器 b) 添加嵌套步骤 添加git 拉取代码 完整jenkins stages { stage('拉取代码') { agent none steps { container('maven') { git(url: 'https://gitee.com/leifengyang/yygh-parent.git', credentialsId: 'gitee-id', branch: 'master', changelog: true, poll: false) sh 'ls -al' } } } 2、项目编译 a) 指定容器 b)添加嵌套步骤 1、确定文件是否存在，添加shell 2、编译代码，添加shell mvn clean package -Dmaven.test.skip=true 3、确定是否编译成功,添加shell ls hospital-manage/target 完整jenkins stage('项目编译') { agent none steps { container('maven') { sh 'ls' sh 'mvn clean package -Dmaven.test.skip=true' sh 'ls hospital-manage/target' } } } 3、构建镜像 a) 指定容器 b)添加嵌套步骤 ​ 1、确定文件是否存在 ​ 2、构建镜像 docker build -t hospital-manage:latest -f hospital-manage/Dockerfile ./hospital-manage/ 完整jenkins stage('default-2') { parallel { stage('构建hospital-manage镜像') { agent none steps { container('maven') { sh 'ls hospital-manage/target' sh 'docker build -t hospital-manage:latest -f hospital-manage/Dockerfile ./hospital-manage/' } } } } 4、推送镜像到仓库 a、指定容器 b、添加凭证 c、登录私有容器库 echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin d、修改镜像tag docker tag hospital-manage:latest $REGISTRY/$DOCKERHUB_NAMESPACE/hospital-manage:SNAPSHOT-$BUILD_NUMBER e、推送镜像 docker push $REGISTRY/$DOCKERHUB_NAMESPACE/hospital-manage:SNAPSHOT-$BUILD_NUMBER> 完整jenkins stage('default-3') { parallel { stage('推送hospital-manage镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag hospital-manage:latest $REGISTRY/$DOCKERHUB_NAMESPACE/hospital-manage:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/hospital-manage:SNAPSHOT-$BUILD_NUMBER' } } } } } } 5、部署到dev环境 1、添加任务 1、指定凭证 2、指定配置文件路径 完整jenkins stage('default-4') { parallel { stage('hospital-manage - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'hospital-manage/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } } 流水线jenkins pipeline { agent { node { label 'maven' } } stages { stage('拉取代码') { agent none steps { container('maven') { git(url: 'https://gitee.com/leifengyang/yygh-parent.git', credentialsId: 'gitee-id', branch: 'master', changelog: true, poll: false) sh 'ls -al' } } } stage('项目编译') { agent none steps { container('maven') { sh 'ls' sh 'mvn clean package -Dmaven.test.skip=true' sh 'ls hospital-manage/target' } } } stage('default-2') { parallel { stage('构建hospital-manage镜像') { agent none steps { container('maven') { sh 'ls hospital-manage/target' sh 'docker build -t hospital-manage:latest -f hospital-manage/Dockerfile ./hospital-manage/' } } } stage('构建server-gateway镜像') { agent none steps { container('maven') { sh 'ls server-gateway/target' sh 'docker build -t server-gateway:latest -f server-gateway/Dockerfile ./server-gateway/' } } } stage('构建service-cmn镜像') { agent none steps { container('maven') { sh 'ls service/service-cmn/target' sh 'docker build -t service-cmn:latest -f service/service-cmn/Dockerfile ./service/service-cmn/' } } } stage('构建service-hosp镜像') { agent none steps { container('maven') { sh 'ls service/service-hosp/target' sh 'docker build -t service-hosp:latest -f service/service-hosp/Dockerfile ./service/service-hosp/' } } } stage('构建service-order镜像') { agent none steps { container('maven') { sh 'ls service/service-order/target' sh 'docker build -t service-order:latest -f service/service-order/Dockerfile ./service/service-order/' } } } stage('构建service-oss镜像') { agent none steps { container('maven') { sh 'ls service/service-oss/target' sh 'docker build -t service-oss:latest -f service/service-oss/Dockerfile ./service/service-oss/' } } } stage('构建service-sms镜像') { agent none steps { container('maven') { sh 'ls service/service-sms/target' sh 'docker build -t service-sms:latest -f service/service-sms/Dockerfile ./service/service-sms/' } } } stage('构建service-statistics镜像') { agent none steps { container('maven') { sh 'ls service/service-statistics/target' sh 'docker build -t service-statistics:latest -f service/service-statistics/Dockerfile ./service/service-statistics/' } } } stage('构建service-task镜像') { agent none steps { container('maven') { sh 'ls service/service-task/target' sh 'docker build -t service-task:latest -f service/service-task/Dockerfile ./service/service-task/' } } } stage('构建service-user镜像') { agent none steps { container('maven') { sh 'ls service/service-user/target' sh 'docker build -t service-user:latest -f service/service-user/Dockerfile ./service/service-user/' } } } } } stage('default-3') { parallel { stage('推送hospital-manage镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag hospital-manage:latest $REGISTRY/$DOCKERHUB_NAMESPACE/hospital-manage:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/hospital-manage:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送server-gateway镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag server-gateway:latest $REGISTRY/$DOCKERHUB_NAMESPACE/server-gateway:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/server-gateway:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-cmn镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-cmn:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-cmn:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-cmn:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-hosp镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-hosp:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-hosp:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-hosp:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-order镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-order:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-order:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-order:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-oss镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-oss:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-oss:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-oss:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-sms镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-sms:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-sms:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-sms:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-statistics镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-statistics:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-statistics:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-statistics:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-task镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-task:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-task:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-task:SNAPSHOT-$BUILD_NUMBER' } } } } stage('推送service-user镜像') { agent none steps { container('maven') { withCredentials([usernamePassword(credentialsId : 'aliyun-docker-registry' ,usernameVariable : 'DOCKER_USER_VAR' ,passwordVariable : 'DOCKER_PWD_VAR' ,)]) { sh 'echo \"$DOCKER_PWD_VAR\" | docker login $REGISTRY -u \"$DOCKER_USER_VAR\" --password-stdin' sh 'docker tag service-user:latest $REGISTRY/$DOCKERHUB_NAMESPACE/service-user:SNAPSHOT-$BUILD_NUMBER' sh 'docker push $REGISTRY/$DOCKERHUB_NAMESPACE/service-user:SNAPSHOT-$BUILD_NUMBER' } } } } } } stage('default-4') { parallel { stage('hospital-manage - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'hospital-manage/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('server-gateway - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'server-gateway/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-cmn - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-cmn/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-hosp - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-hosp/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-order - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-order/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-oss - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-oss/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-sms - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-sms/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-statistics - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-statistics/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-task - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-task/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } stage('service-user - 部署到dev环境') { agent none steps { kubernetesDeploy(configs: 'service/service-user/deploy/**', enableConfigSubstitution: true, kubeconfigId: \"$KUBECONFIG_CREDENTIAL_ID\") } } } } stage('发送确认邮件') { agent none steps { mail(to: '849109312@qq.com', subject: '构建结果', body: '\"构建成功了 $BUILD_NUMBER\"') } } } environment { DOCKER_CREDENTIAL_ID = 'dockerhub-id' GITHUB_CREDENTIAL_ID = 'github-id' KUBECONFIG_CREDENTIAL_ID = 'demo-kubeconfig' REGISTRY = 'registry.cn-hangzhou.aliyuncs.com' DOCKERHUB_NAMESPACE = 'wjh_ruoyi' GITHUB_ACCOUNT = 'kubesphere' APP_NAME = 'devops-java-sample' ALIYUNHUB_NAMESPACE = 'wjh_ruoyi' } parameters { string(name: 'TAG_NAME', defaultValue: '', description: '') } } 方式二：读取项目自带jenkins 1、指定git路径 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"cloud_learn/devops/baotao_webhook.html":{"url":"cloud_learn/devops/baotao_webhook.html","title":"宝塔webhook","keywords":"","body":"宝塔webhook #!/bin/bash echo \"\" #输出当前时间 date --date='0 days ago' \"+%Y-%m-%d %H:%M:%S\" echo \"-------开始-------\" #判断宝塔WebHook参数是否存在 echo \"$1\" if [ ! -n \"$1\" ]; then echo \"param参数错误\" echo \"End\" exit fi #服务器 git 项目路径 gitPath=\"/www/wwwroot/$1\" #码云项目 git 网址 gitHttp=\"git@github.com:fhwlnetwork/learnote.git\" echo \"路径：$gitPath\" cd $gitPath && pwd && echo \"我来了\" pwd echo \"我进来了\" git pull echo \"拉取完成\" #判断项目路径是否存在 if [ -d \"$gitPath\" ]; then cd $gitPath #判断是否存在git目录 if [ ! -d \".git\" ]; then echo \"在该目录下克隆 git\" git clone $gitHttp gittemp mv gittemp/.git . rm -rf gittemp fi #拉取最新的项目文件 git reset --hard origin/master #git clean -f git pull origin master echo \"拉取完成\" #执行npm #执行编译 #npm run build #设置目录权限 chown -R www:www $gitPath echo \"-------结束--------\" exit else echo \"该项目路径不存在\" echo \"End\" exit Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 15:00:33 "},"es/install.html":{"url":"es/install.html","title":"安装","keywords":"","body":"Elasticsearch安装部署-rpm安装 ### 安装java [root@db01 ~]# yum install -y java-1.8.0-openjdk.x86_64 [root@db01 ~]# mkdir -p /data/es_soft/ [root@db01 ~]# cd /data/es_soft/ [root@db01 /data/es_soft]# rpm -ivh elasticsearch-6.6.0.rpm [root@db01 /data/es_soft]# systemctl daemon-reload [root@db01 /data/es_soft]# systemctl enable elasticsearch.service Created symlink from /etc/systemd/system/multi-user.target.wants/elasticsearch.service to /usr/lib/systemd/system/elasticsearch.service. [root@db01 /data/es_soft]# systemctl start elasticsearch.service [root@db01 /data/es_soft]# systemctl status elasticsearch.service ● elasticsearch.service - Elasticsearch Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-04-22 21:38:35 CST; 10s ago Docs: http://www.elastic.co Main PID: 6738 (java) CGroup: /system.slice/elasticsearch.service └─6738 /bin/java -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC... Apr 22 21:38:35 db01 systemd[1]: Started Elasticsearch. Apr 22 21:38:35 db01 systemd[1]: Starting Elasticsearch... Apr 22 21:38:36 db01 elasticsearch[6738]: OpenJDK 64-Bit Server V... Hint: Some lines were ellipsized, use -l to show in full. #文件目录说明 rpm -qc elasticsearch #查看elasticsearch的所有配置文件 /etc/elasticsearch/elasticsearch.yml #配置文件 /etc/elasticsearch/jvm.options. #jvm虚拟机配置文件 /etc/init.d/elasticsearch #init启动文件 /etc/sysconfig/elasticsearch #环境变量配置文件 /usr/lib/sysctl.d/elasticsearch.conf #sysctl变量文件，修改最大描述符 /usr/lib/systemd/system/elasticsearch.service #systemd启动文件 /var/lib/elasticsearch # 数据目录 /var/log/elasticsearch #日志目录 /var/run/elasticsearch #pid目录 #修改配置 [root@db01 /data/es_soft]# vim /etc/elasticsearch/elasticsearch.yml network.host: 10.0.0.51 # # Set a custom port for HTTP: # http.port: 9200 修改完配置文件后我们需要重启一下 [root@db01 /data/es_soft]# grep \"^[a-Z]\" /etc/elasticsearch/elasticsearch.yml node.name: node-1 path.data: /data/elasticsearch path.logs: /var/log/elasticsearch network.host: 10.0.0.51 http.port: 9200 bootstrap.memory_lock: true #JVM 配置 # 不要超过32g # 最大最小内存设置为一样 #配置文件设置锁定内存 #至少给服务器本身空余50%的内存 [root@db01 /etc/elasticsearch]# vim jvm.options -Xms512m -Xmx512m # 创建目录 [root@db01 /data/es_soft]# mkdir -p /data/elasticsearch [root@db01 /data/es_soft]# chown -R elasticsearch:elasticsearch /data/elasticsearch/ [root@db01 /data/es_soft]# systemctl restart elasticsearch [root@db01 /data/es_soft]# systemctl status elasticsearch 这个时候可能会启动失败，查看日志可能会发现是锁定内存失败 官方解决方案 https://www.elastic.co/guide/en/elasticsearch/reference/6.6/setup-configuration-memory.html https://www.elastic.co/guide/en/elasticsearch/reference/6.6/setting-system-settings.html#sysconfig ### 修改启动配置文件或创建新配置文件 方法1: systemctl edit elasticsearch 方法2: vim /usr/lib/systemd/system/elasticsearch.service ### 增加如下参数 [Service] LimitMEMLOCK=infinity ### 重新启动 systemctl daemon-reload systemctl restart elasticsearch 可能遇到的错误 initial heap size [16777216] not equal to maximum heap size [536870912]; this can cause resize pauses and prevents mlockall from locking the entire heap 说明此时处于生产模式 修改elasticsearch.yml discvery.type： single-node Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"es/head插件交互.html":{"url":"es/head插件交互.html","title":"交互","keywords":"","body":"交互 Head插件在5.0以后安装方式发生了改变，需要nodejs环境支持，或者直接使用别人封装好的docker镜像 插件官方地址 https://github.com/mobz/elasticsearch-head 使用docker部署elasticsearch-head docker pull alivv/elasticsearch-head docker run --name es-head -p 9100:9100 -dit elivv/elasticsearch-head 使用nodejs编译安装elasticsearch-head yum install nodejs npm openssl screen -y node -v npm -v npm install -g cnpm --registry=https://registry.npm.taobao.org cd /opt/ git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head/ cnpm install screen -S es-head cnpm run start Ctrl+A+D 修改ES配置文件支持跨域 http.cors.enabled: true http.cors.allow-origin: \"*\" Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"es/dml.html":{"url":"es/dml.html","title":"操作语言","keywords":"","body":"增删改查 #创建索引 [root@db01 /etc/elasticsearch]#curl -XPUT '10.0.0.51:9200/vipinfo?pretty' { \"acknowledged\" : true, \"shards_acknowledged\" : true, \"index\" : \"vipinfo\" } #插入文档数据 curl -XPUT '10.0.0.51:9200/vipinfo/user/1?pretty' -H 'Content-Type: application/json' -d' { \"first_name\" : \"John\", \"last_name\": \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] }' curl -XPUT 'localhost:9200/vipinfo/user/2?pretty' -H 'Content-Type: application/json' -d' { \"first_name\": \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ] }' curl -XPUT 'localhost:9200/vipinfo/user/3?pretty' -H 'Content-Type: application/json' -d' { \"first_name\": \"Douglas\", \"last_name\" : \"Fir\", \"age\" : 35, \"about\": \"I like to build cabinets\", \"interests\": [ \"forestry\" ] }' #说明：创建数据时，使用默认的随机id,如果需要与mysql建立联系可以新增一个sid列，填入mysql中数据列的id Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"es/集群.html":{"url":"es/集群.html","title":"集群配置","keywords":"","body":"集群部署安装配置 部署三台服务器节点，10.0.0.51,10.0.0.52，10.0.0.53 #1、装java环境 [root@db02 /data/soft]# yum -y install java-1.8.0-openjdk.x86_64 #2、下载es软件 [root@db02 /data/soft]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.0.rpm [root@db02 /data/soft]# rpm -ivh elasticsearch-6.6.0.rpm #3、修改配置文件 [root@db02 /data/soft]#cat > /etc/elasticsearch/elasticsearch.yml 其他集群配置 重复以上内容 2个节点,master设置为2的时候,一台出现故障导致集群不可用 解决方案: 把还存活的节点的配置文件集群选举相关的选项注释掉或者改成1 discovery.zen.minimum_master_nodes: 1 重启服务 结论: 两个节点数据不一致会导致查询结果不一致 找出不一致的数据,清空一个节点,以另一个节点的数据为准 然后手动插入修改后的数据 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/安装.html":{"url":"elk/安装.html","title":"安装","keywords":"","body":"安装 es安装配置 参考es笔记es安装配置 配置参考如下： 安装kibana [root@db01 /data/soft]#rpm -ich kibana-6.6.0-x86_64.rpm warning: kibana-6.6.0-x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY ################################# [100%] Updating / installing... ################################# [100%] # 修改配置文件 修改kibana配置 # 修改配置文件 [root@db01 /data/soft]#vim /etc/kibana/kibana.yml [root@db01 /data/soft]#grep \"^[a-z]\" /etc/kibana/kibana.yml server.port: 5601 server.host: \"10.0.0.51\" elasticsearch.hosts: [\"http://localhost:9200\"] kibana.index: \".kibana\" 启动服务 [root@db01 /data/soft]#systemctl start kibana # 查看状态 [root@db01 /data/soft]#systemctl status kibana ● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; disabled; vendor preset: disabled) Active: active (running) since Mon 2021-04-26 13:44:44 CST; 10s ago Main PID: 2105 (node) CGroup: /system.slice/kibana.service └─2105 /usr/share/kibana/bin/../node/bin/node --no-warnings /usr/share/kib... Apr 26 13:44:44 db01 systemd[1]: [/etc/systemd/system/kibana.service:3] Unknown lv...it' Apr 26 13:44:44 db01 systemd[1]: [/etc/systemd/system/kibana.service:4] Unknown lv...it' Apr 26 13:44:44 db01 systemd[1]: Started Kibana. Apr 26 13:44:44 db01 systemd[1]: Starting Kibana... Hint: Some lines were ellipsized, use -l to show in full. # 查看端口 [root@db01 /data/soft]#netstat -lntup|grep 5601 tcp 0 0 10.0.0.51:5601 0.0.0.0:* LISTEN 2105/node 查看图形界面效果 浏览器输入ip:6501 注意事项： Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/nginx_log_json.html":{"url":"elk/nginx_log_json.html","title":"nginxjson日志采集","keywords":"","body":"nginxjson日志采集 ## 安装nginx [root@db01 /data/soft]#yum -y install nginx Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com # 启动服务 [root@db01 /data/soft]#systemctl restart nginx # 安装压测工具 [root@db01 /data/soft]#yum -y install httpd-tools 配置nginx 日志格式 #在nging.conf文件 http中添加以下内容 http { log_format json '{ \"time_local\": \"$time_local\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"upstream_time\": \"$upstream_response_time\",' '\"request_time\": \"$request_time\"' '}'; } # 验证ngingx配置 [root@db01 /data/soft]#nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful # 重启ngingx 服务 [root@db01 /data/soft]#systemctl restart nginx # 清空数据日志 [root@db01 /data/soft]#> /var/log/nginx/access.log ## 创建测试数据 [root@db01 /data/soft]#ab -n 100 -c 100 http://10.0.0.51/ [root@db01 /data/soft]#tail -f /var/log/nginx/access.log 10.0.0.51 - - [26/Apr/2021:14:08:59 +0800] \"GET / HTTP/1.0\" 200 4833 \"-\" \"ApacheBench/2.3\" \"-\" 10.0.0.51 - - [26/Apr/2021:14:08:59 +0800] \"GET / HTTP/1.0\" 200 4833 \"-\" \"ApacheBench/2.3\" \"-\" 10.0.0.51 - - [26/Apr/2021:14:08:59 +0800] \"GET / HTTP/1.0\" 200 4833 \"-\" \"ApacheBench/2.3\" \"-\" 10.0.0.51 - - [26/Apr/2021:14:08:59 +0800] \"GET / HTTP/1.0\" 200 4833 \"-\" \"ApacheBench/2.3\" \"-\" # 验证查看日志数据格式 [root@db01 /data/soft]#tail -1 /var/log/nginx/access.log { \"time_local\": \"26/Apr/2021:14:30:52 +0800\", \"remote_addr\": \"10.0.0.51\", \"referer\": \"-\", \"request\": \"GET / HTTP/1.0\", \"status\": 200, \"bytes\": 4833, \"agent\": \"ApacheBench/2.3\", \"x_forwarded\": \"-\", \"up_addr\": \"-\",\"up_host\": \"-\",\"upstream_time\": \"-\",\"request_time\": \"0.000\"} 安装filebeat [root@db01 /data/soft]# rpm -ivh filebeat-6.6.0-x86_64.rpm warning: filebeat-6.6.0-x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY Preparing... ################################# [100%] Updating / installing... 1:filebeat-6.6.0-1 ################################# [100%] ## 修改配置文件 [root@db01 /data/soft]#cp /etc/filebeat/filebeat.yml /tmp/ [root@db01 /data/soft]#cat > /etc/filebeat/filebeat.yml 添加kibana监控项目 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/nginx_success_error_log.html":{"url":"elk/nginx_success_error_log.html","title":"nginix正常日志和错误日志","keywords":"","body":"ELk 收集Nginx的正常日志和错误日志 收集多台nginx服务日志信息 #n台服务的配置文件的日志格式为一样 http { log_format json '{ \"time_local\": \"$time_local\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"upstream_time\": \"$upstream_response_time\",' '\"request_time\": \"$request_time\"' '}'; } 正常日志，错误日志拆分 #修改配置信息 [root@db01 ~]# cat >/etc/filebeat/filebeat.yml 说明 特别说明:如果之前已产生日志数据，需将旧日志信息移除或移动到其他目录 删除添加的好的management Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/tomcat_log_cat.html":{"url":"elk/tomcat_log_cat.html","title":"tomcat日志收集","keywords":"","body":"tomcat日志收集 安装tomcat [root@db01 ~]# yum install tomcat tomcat-webapps tomcat-admin-webapps tomcat-docs-webapp tomcat-javadoc -y 启动服务 [root@db01 ~]# systemctl start tomcat 验证服务 配置tomacat日志格式为json [root@db01 ~]# vim /etc/tomcat/server.xml [root@db01 ~]# cat -n /etc/tomcat/server.xml ---------------- 137 ---------------- 重启确认日志是否为json格式 [root@db01 ~]# systemctl restart tomcat [root@db01 ~]# tail -f /var/log/tomcat/localhost_access_log.2021-04-26.txt {\"clientip\":\"10.0.0.1\",\"ClientUser\":\"-\",\"authenticated\":\"-\",\"AccessTime\":\"[26/Apr/2021:23:35:07 +0800]\",\"method\":\"GET / HTTP/1.1\",\"status\":\"200\",\"SendBytes\":\"11217\",\"Query?string\":\"\",\"partner\":\"-\",\"AgentVersion\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36\"} {\"clientip\":\"10.0.0.1\",\"ClientUser\":\"-\",\"authenticated\":\"-\",\"AccessTime\":\"[26/Apr/2021:23:35:07 +0800]\",\"method\":\"GET /favicon.ico HTTP/1.1\",\"status\":\"200\",\"SendBytes\":\"21630\",\"Query?string\":\"\",\"partner\":\"http://10.0.0.51:8080/\",\"AgentVersion\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36\"} {\"clientip\":\"10.0.0.1\",\"ClientUser\":\"-\",\"authenticated\":\"-\",\"AccessTime\":\"[26/Apr/2021:23:35:07 +0800]\",\"method\":\"GET / HTTP/1.1\",\"status\":\"200\",\"SendBytes\":\"11217\",\"Query?string\":\"\",\"partner\":\"-\",\"AgentVersion\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36\"} 修改filebeat配置文件 [root@db01 ~]# cat > /etc/filebeat/filebeat.yml 重启服务&c查看状态 [root@db01 ~]# systemctl restart filebeat.service [root@db01 ~]# tail -f /var/log/filebeat/filebeat 2021-04-26T23:51:54.518+0800 INFO [monitoring] log/log.go:144 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":70,\"time\":{\"ms\":2}},\"total\":{\"ticks\":150,\"time\":{\"ms\":13},\"value\":150},\"user\":{\"ticks\":80,\"time\":{\"ms\":11}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":8},\"info\":{\"ephemeral_id\":\"18f558bb-c001-4fdf-9e1c-1e9ef28bfbd7\",\"uptime\":{\"ms\":240047}},\"memstats\":{\"gc_next\":4194304,\"memory_alloc\":1903176,\"memory_total\":7411504}},\"filebeat\":{\"harvester\":{\"open_files\":1,\"running\":1}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":4,\"events\":{\"active\":0}}},\"registrar\":{\"states\":{\"current\":3}},\"system\":{\"load\":{\"1\":0.05,\"15\":0.22,\"5\":0.2,\"norm\":{\"1\":0.05,\"15\":0.22,\"5\":0.2}}}}}} 添加mangement Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/java_log.html":{"url":"elk/java_log.html","title":"java多行日志收集","keywords":"","body":"java多行日志收集 # 编辑修改配置文件 [root@db01 ~]# vim /etc/filebeat/filebeat.yml - /var/log/elasticsearch/elasticsearch.log tags: [\"es\"] multiline.pattern: '^\\[' multiline.negate: true multiline.match: after #####################output_messages############## setup.kibana: host: \"10.0.0.51:5601\" #自定义配置输出格式 output.elasticsearch: hosts: [\"10.0.0.51:9200\"] # 判断条件可以为其他属性 indices: - index: \"nginx-access-%{[beat.version]}-%{+yyyy.MM}\" when.contains: tags: \"access\" - index: \"nginx-error-%{[beat.version]}-%{+yyyy.MM}\" when.contains: tags: \"error\" - index: \"tomact-access-%{[beat.version]}-%{+yyyy.MM}\" when.contains: tags: \"tomact\" - index: \"es-java-%{[beat.version]}-%{+yyyy.MM}\" when.contains: tags: \"es\" #重新命名模板名称为ngingx setup.template.name: \"nginx\" #匹配格式，以nginx-开头的模板都使用nginx的模板 setup.template.pattern: \"nginx-*\" #不使用系统自自带的模板 setup.template.enabled: false Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/docker_log.html":{"url":"elk/docker_log.html","title":"收集docker日志","keywords":"","body":"收集docker日志 docker安装:docker安装过程 #配置docker cat >docker-compose.yml/etc/filebeat/filebeat.yml Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/filebeat_modules_get_ngingx_simple_log.html":{"url":"elk/filebeat_modules_get_ngingx_simple_log.html","title":"fibet收集ngingx日志","keywords":"","body":"filebeat模块收集ngingx普通日志 第一步 #查看filebeat文件 [root@db01 /data/soft]# rpm -qc filebeat /etc/filebeat/filebeat.yml /etc/filebeat/modules.d/apache2.yml.disabled /etc/filebeat/modules.d/auditd.yml.disabled /etc/filebeat/modules.d/elasticsearch.yml.disabled /etc/filebeat/modules.d/haproxy.yml.disabled /etc/filebeat/modules.d/icinga.yml.disabled /etc/filebeat/modules.d/iis.yml.disabled /etc/filebeat/modules.d/kafka.yml.disabled /etc/filebeat/modules.d/kibana.yml.disabled /etc/filebeat/modules.d/logstash.yml.disabled /etc/filebeat/modules.d/mongodb.yml.disabled /etc/filebeat/modules.d/mysql.yml.disabled /etc/filebeat/modules.d/nginx.yml.disabled /etc/filebeat/modules.d/osquery.yml.disabled /etc/filebeat/modules.d/postgresql.yml.disabled /etc/filebeat/modules.d/redis.yml.disabled /etc/filebeat/modules.d/suricata.yml.disabled /etc/filebeat/modules.d/system.yml.disabled /etc/filebeat/modules.d/traefik.yml.disabled #查询模板 filebeat modules list #激活模块 filebeat moudles enable nginx #配置nginx.yml文件配置 [root@db01 /data/soft]# vim /etc/filebeat/modules.d/nginx.yml - module: nginx # Access logs access: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\"/var/log/nginx/access.log\"] # Error logs error: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths: var.paths: [\"/var/log/nginx/error.log\"] #配置filebeat modules #============================= Filebeat modules =============================== # filebeat.config.modules: # # Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml reload.enabled: false reload.period: 10s output.elasticsearch: hosts: [\"10.0.0.51:9200\"] ~ ~ 第二步 #重启服务 [root@db01 /data/soft]# systemctl restart filebeat.service #查看日志报错 2021-04-29T21:49:12.254+0800 ERROR fileset/factory.go:142 Error loading pipeline: Error loading pipeline for fileset nginx/access: This module requires the following Elasticsearch plugins: ingest-user-agent, ingest-geoip. You can install them by running the following commands on all the Elasticsearch nodes: sudo bin/elasticsearch-plugin install ingest-user-agent sudo bin/elasticsearch-plugin install ingest-geoip #安装插件 [root@db01 /data/soft]# /usr/share/elasticsearch/bin/elasticsearch-plugin install ingest-user-agent -> Downloading ingest-user-agent from elastic [=================================================] 100% -> Installed ingest-user-agent [root@db01 /data/soft]# /usr/share/elasticsearch/bin/elasticsearch-plugin install ingest-geoip -> Downloading ingest-geoip from elastic [=================================================] 100% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: plugin requires additional permissions @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessDeclaredMembers * java.lang.reflect.ReflectPermission suppressAccessChecks See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks. Continue with installation? [y/N]y -> Installed ingest-geoip #查看elasticsearch-plugin命令目录 [root@db01 /data/soft]# rpm -ql elasticsearch |grep elasticsearch-plugin /usr/share/elasticsearch/bin/elasticsearch-plugin /usr/share/elasticsearch/lib/tools/plugin-cli/elasticsearch-plugin-cli-6.6.0.jar 第三步 ## 配置etc/filebeat/filebeat.yml [root@db01 /var/log/nginx]# cat >/etc/filebeat/filebeat.yml Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/kibana_draw_dashboard.html":{"url":"elk/kibana_draw_dashboard.html","title":"kibana画图","keywords":"","body":"kibana画图 第一步 第二步 第三步 第四步 第五步 第六步 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/redis_cat_log.html":{"url":"elk/redis_cat_log.html","title":"redis作为缓存收集日志","keywords":"","body":"redis作为缓存收集日志 方式一 安装logstash [root@db01 /data/soft]#rpm -ivh logstash-6.6.0.rpm warning: logstash-6.6.0.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY Preparing... ################################# [100%] Updating / installing... 1:logstash-1:6.6.0-1 ################################# [100%] Using provided startup.options file: /etc/logstash/startup.options OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N Successfully created system startup script for Logstash 配置filebeat写入到不同的key中 [root@db01 /data/soft]#cat >/etc/filebeat/filebeat.yml 6.1.6 logstash根据tag区分一个key里的不同日志 [root@db01 /data/soft]#cat >/etc/logstash/conf.d/redis.conf \"127.0.0.1\" port => \"6379\" db => \"0\" key => \"nginx_access\" data_type => \"list\" } redis { host => \"127.0.0.1\" port => \"6379\" db => \"0\" key => \"nginx_error\" data_type => \"list\" } } filter { mutate { convert => [\"upstream_time\", \"float\"] convert => [\"request_time\", \"float\"] } } output { stdout {} if \"access\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_access-%{+yyyy.MM.dd}\" } } if \"error\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_error-%{+yyyy.MM.dd}\" } } } EOF #启动服务 [root@db01 /data/soft]#/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/redis.conf 方式二 filebeat收集日志写入到一个key中 [root@db01 /data/soft]# cat >/etc/logstash/conf.d/redis.conf \"127.0.0.1\" port => \"6379\" db => \"0\" key => \"filebeat\" data_type => \"list\" } } filter { mutate { convert => [\"upstream_time\", \"float\"] convert => [\"request_time\", \"float\"] } } output { stdout {} if \"access\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_access-%{+yyyy.MM.dd}\" } } if \"error\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_error-%{+yyyy.MM.dd}\" } } } EOF [root@db01 /data/soft]#cat > /etc/filebeat/filebeat.yml logstash根据tag区分一个key里的不同日志 [root@db01 /data/soft]#cat >/etc/logstash/conf.d/redis.conf \"127.0.0.1\" port => \"6379\" db => \"0\" key => \"nginx_access\" data_type => \"list\" } redis { host => \"127.0.0.1\" port => \"6379\" db => \"0\" key => \"nginx_error\" data_type => \"list\" } } filter { mutate { convert => [\"upstream_time\", \"float\"] convert => [\"request_time\", \"float\"] } } output { stdout {} if \"access\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_access-%{+yyyy.MM.dd}\" } } if \"error\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_error-%{+yyyy.MM.dd}\" } } } EOF 重启服务 [root@db01 /data/soft]#systemctl restart filebeat.service [root@db01 /data/soft]#/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/redis.conf Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/kafka缓存收集日志.html":{"url":"elk/kafka缓存收集日志.html","title":"kafka缓存收集日志","keywords":"","body":"kafka和zookeeper安装 zookeeper安装 准备工作 # 解压文件 [root@db01 /data/soft]# tar zxf zookeeper-3.4.11.tar.gz -C /opt/ # 创建软连接 [root@db01 /data/soft]# ln -s /opt/zookeeper-3.4.11/ /opt/zookeeper # 安装java 环境 [root@db01 ~]# yum install -y java-1.8.0-openjdk.x86_64 安装配置zookeeper [root@db01 /data/soft]# mkdir -p /data/zookeeper [root@db01 /data/soft]# cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg [root@db01 /data/soft]# vim /opt/zookeeper/conf/zoo.cfg [root@db01 /data/soft]# cat >/opt/zookeeper/conf/zoo.cfg 其他节点配置步骤和节点1一样,只是最后myid不一样而已 [root@db01 /opt]# rsync -avz zookeeper* db02:/opt/ [root@db02 /opt]# cat /data/zookeeper/mid 2 [root@db01 /opt]# rsync -avz zookeeper* db03:/opt/ [root@db03 /opt]# cat /data/zookeeper/mid 3 启动服务&查看状态 #启动 [root@db01 /opt]# /opt/zookeeper/bin/zkServer.sh start #查看状态 [root@db01 /opt]# /opt/zookeeper/bin/zkServer.sh status kafka安装 准备工作 [root@db01 /opt]# tar -xvzf /data/soft/kafka_2.11-1.0.0.tgz -C /opt [root@db01 /opt]# ln -s /opt/kafka_2.11-1.0.0/ /opt/kafka 安装配置 [root@db01 /opt/kafka]# vim /opt/kafka/config/server.properties 21 broker.id=1 31 listeners=PLAINTEXT://10.0.0.51:9092 60 log.dirs=/opt/kafka/logs 103 log.retention.hours=24 123 zookeeper.connect=10.0.0.51:2181,10.0.0.52:2181,10.0.0.53:2181 其他节点配置步骤和节点1一样,只是最listeners不一样而已 [root@db01 /opt]# rsync -avz zookeeper* db02:/opt/ [root@db02 /opt]# sed -i 's#broker.id=1#broker.id=2#g' /opt/kafka/config/server.properties [root@db02 /opt]# sed -i 's#10.0.0.51:9092#10.0.0.52:9092#g' /opt/kafka/config/server.properties [root@db01 /opt]# rsync -avz zookeeper* db03:/opt/ [root@db03 /opt]# sed -i 's#broker.id=1#broker.id=3#g' /opt/kafka/config/server.properties [root@db03 /opt]# sed -i 's#10.0.0.51:9092#10.0.0.53:9092#g' /opt/kafka/config/server.properties 节点1,可以先前台启动,方便查看错误日志 [root@db01 /opt]# /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties [2021-05-06 17:06:33,642] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral) [2021-05-06 17:06:33,644] INFO Registered broker 2 at path /brokers/ids/2 with addresses: EndPoint(10.0.0.52,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.utils.ZkUtils) [2021-05-06 17:06:33,655] INFO Kafka version : 1.0.0 (org.apache.kafka.common.utils.AppInfoParser) [2021-05-06 17:06:33,655] INFO Kafka commitId : aaa7af6d4a11b29d (org.apache.kafka.common.utils.AppInfoParser) [2021-05-06 17:06:33,658] INFO [KafkaServer id=2] started (kafka.server.KafkaServer) # 后台启动 [root@db01 /opt]## /opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties [root@db01 /opt]# tail -f /opt/kafka/logs/server.log ========================= [2021-05-06 17:06:33,658] INFO [KafkaServer id=1] started (kafka.server.KafkaServer) 验证测试 #创建一个topic [root@db01 /opt]# /opt/kafka/bin/kafka-topics.sh --create --zookeeper 10.0.0.51:2181,10.0.0.52:2181,10.0.0.53:2181 --partitions 3 --replication-factor 3 --topic kafkatest OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N Created topic \"kafkatest\". # 获取topic [root@db02 /opt/kafka]# /opt/kafka/bin/kafka-topics.sh --list --zookeeper 10.0.0.51:2181,10.0.0.52:2181,10.0.0.53:2181 OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N kafkatest kafka测试命令发送消息 #创建一个名为messagetest的topic [root@db02 /opt/kafka]# /opt/kafka/bin/kafka-topics.sh --create --zookeeper 10.0.0.51:2181,10.0.0.52:2181,10.0.0.53:2181 --partitions 3 --replication-factor 3 --topic messagetest OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N Created topic \"messagetest\". #发送消息:注意,端口是 kafka的9092,而不是zookeeper的2181 [root@db02 /opt/kafka]# /opt/kafka/bin/kafka-console-producer.sh --broker-list 10.0.0.51:9092,10.0.0.52:9092,10.0.0.53:9092 --topic messagetestOpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N > >biubiu # 其他kafka服务器获取消息 [root@db01 /opt]# /opt/kafka/bin/kafka-console-consumer.sh --zookeeper 10.0.0.51:2181,10.0.0.52:2181,10.0.0.53:2181 --topic messagetest --from-beginning OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper]. hello biubiu kafka收集日志配置 修改filebeat配置 [root@kafka-175 conf.d]# cat >/etc/filebeat/filebeat.yml 修改 logstash配置 cat >/etc/logstash/conf.d/kafka.conf\"10.0.0.51:9092\" topics=>[\"elklog\"] group_id=>\"logstash\" codec => \"json\" } } filter { mutate { convert => [\"upstream_time\", \"float\"] convert => [\"request_time\", \"float\"] } } output { if \"access\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_access-%{+yyyy.MM.dd}\" } } if \"error\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_error-%{+yyyy.MM.dd}\" } } } EOF 重启服务 #启动filebeat [root@db01 ~]# systemctl restart filebeat #启动logstash #启动服务 [root@db01 /data/soft]#/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/redis.conf Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"elk/nginx_keepalived_redis.html":{"url":"elk/nginx_keepalived_redis.html","title":"使用nginx+keepalived代理多台redis","keywords":"","body":"redis集群方案有哨兵和集群，但可惜的是filebeat和logstash都不支持这两种方案。 解决方案如下： 1.使用nginx+keepalived反向代理负载均衡到后面的多台redis 2.考虑到redis故障切换中数据一致性的问题，所以最好我们只使用2台redis,并且只工作一台，另外一台作为backup，只有第一台坏掉后，第二台才会工作。 3.filebeat的oputut的redis地址为keepalived的虚拟I 4.logstash可以启动多个节点来加速读取redis的数据 5.后端可以采用多台es集群来做支撑 redis安装配置：redis安装 keepalived安装配置 #db01 db02上分别安装 [root@db02 /opt/kafka]# yum -y install keepalived #配置主 [root@db01 ~]# cat >/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf nginx反向代理配置 #在 /etc/nginx/nginx.conf 最后添加不能加到conf.d里面添加子配置 stream { upstream redis { server 10.0.0.52:6379 max_fails=2 fail_timeout=10s; server 10.0.0.53:6379 max_fails=2 fail_timeout=10s backup; } server { listen 6379; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass redis; } } filbeat 配置 [root@db01 /data/soft]#cat > /etc/filebeat/filebeat.yml logstach 配置 cat >/etc/logstash/conf.d/redis.conf \"10.0.0.3\" port => \"6379\" db => \"0\" key => \"filebeat\" data_type => \"list\" } } filter { mutate { convert => [\"upstream_time\", \"float\"] convert => [\"request_time\", \"float\"] } } output { stdout {} if \"access\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_access-%{+yyyy.MM.dd}\" } } if \"error\" in [tags] { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"nginx_error-%{+yyyy.MM.dd}\" } } } EOF 重启服务 [root@db01 ~]# systemctl restart filebeat [root@db01]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/redis.conf Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"zabbix/install.html":{"url":"zabbix/install.html","title":"安装","keywords":"","body":"安装----在线安装 第一步 ：下载安装zabbix yum 源文件 [root@zabbix soft]# rpm -ivh https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm 第二步:下载安装zabbix服务端相关软件 #zabbix服务程序软件: zabbix-server-mysql #zabbix服务web软件: zabbix-web-mysql httpd php #数据库服务软件: mariadb-server [root@zabbix soft]# yum install -y zabbix-server-mysql zabbix-web-mysql httpd php mariadb-server 第三步：软件配置 #配置数据库密码 [root@zabbix soft]# vim /etc/zabbix/zabbix_server.conf 126 DBPassword=zabbix #配置时区 [root@zabbix soft]# vim /etc/httpd/conf.d/zabbix.conf 21 php_value date.timezone Asia/Shanghai 第四步：编写配置数据库服务 [root@zabbix soft]# systemctl start mariadb.service # 创建zabbix数据库--zabbix # 创建数据库管理用户 [root@zabbix soft]# mysql Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 2 Server version: 5.5.68-MariaDB MariaDB Server Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]> create database zabbix character set utf8 collate utf8_bin; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]> grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix'; Query OK, 0 rows affected (0.00 sec) # 在zabbix数据库中导入相应的表信息 [root@zabbix soft]# zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix 第五步：启动zabbix程序相关服务 # 数据库服务 zabbix服务 httpd服务 [root@zabbix soft]# systemctl start zabbix-server.service httpd mariadb.service # 配置开启自动启动 [root@zabbix soft]# systemctl enable zabbix-server.service httpd mariadb.service 第六步： 登录zabbix服务端web界面, 进行初始化配置 10051 zabbix-server 服务端端口号 10050 zabbix-agent 客户端端口号 http://10.0.0.101/zabbix/setup.php 默认账户密码：Admin zabbix 一件部署脚本 #/!bin/bash echo \"-----------------下载安装zabbix yum 源文件--------------------\" rpm -ivh https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm rpm -qa|grep zabbix if [ $? -eq 0 ] then echo \"-----------------下载安装zabbix服务端相关软件-----------------\" #zabbix服务程序软件: zabbix-server-mysql #zabbix服务web软件: zabbix-web-mysql httpd php #数据库服务软件: mariadb-server yum install -y zabbix-server-mysql zabbix-web-mysql httpd php mariadb-server rpm -qa | grep zabbix-server-mysql if [ $? -ne 0 ] then echo \" zabbix-server-mysql 安装失败\" exit fi rpm -qa | grep zabbix-web-mysql if [ $? -ne 0 ] then echo \" zabbix-web-mysql 安装失败\" exit fi echo \"-----------------软件配置-------------------------------------\" sed -i.bak 's/# DBPassword=/DBPassword=zabbix/g' /etc/zabbix/zabbix_server.conf sed -i.bak 's#\\# php_value date.timezone Europe/Riga#php_value date.timezone Asia/Shanghai#g' /etc/httpd/conf.d/zabbix.conf echo \"-----------------软件配置-------------------------------------\" systemctl start mariadb.service netstat -lnutp|grep 3306 if [ $? -eq 0 ] then mysql -e \"create database zabbix character set utf8 collate utf8_bin;\" mysql -e \" show databases\"|grep zabbix if [ $? -ne 0 ] then echo \"创建数据库失败\" exit fi mysql -e \"grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';\" mysql -e \" select * from mysql.user\"|grep zabbix if [ $? -ne 0 ] then echo \"创建数据库管理用户失败\" exit fi else echo \"数据库启动失败\" exit fi zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix echo \"-----------------启动zabbix程序相关服务-----------------------\" systemctl start zabbix-server.service httpd mariadb.service systemctl enable zabbix-server.service httpd mariadb.service else echo \"安装失败\" exit fi Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"zabbix/create_monitor.html":{"url":"zabbix/create_monitor.html","title":"创建监控","keywords":"","body":"创建监控 第一步: 配置---主机---创建主机(创建要监控的主机) 第二步: 配置监控的主机 主机信息中: 名称 主机组 监控的主机地址 模板信息中: 指定需要链接的模板信息 第三步: 保存退出,进行监控检查 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"zabbix/custom__monitor.html":{"url":"zabbix/custom__monitor.html","title":"zabbix自定义监控","keywords":"","body":"zabbix自定义监控 监控项: 可以自定义监控收集主机的信息 应用集: 将多个类似的监控项进行整合 便于查看检查 模板: 将多个监控项 触发器 图形都配置在模板中, 方便多个监控的主机进行调用 动作: 指定将报警信息发送给谁OK/定义报警的信息ok/定义报警的类型OK(邮件 微信 短信电话) PS: 宏信息定义方法: https://www.zabbix.com/documentation/4.0/zh/manual/appendix/macros/supported_by_location 触发器: 可以实现报警提示(条件表达式),默认页面提示报警 图形: 将多个图整合成一张,便于分析数据 报警媒介: 定义报警的方式 需求: 监控nginx服务是否启动 1) 在zabbix-agent进行配置文件编写 第一步: 编写自定义监控命令 [root@web01 zabbix_agentd.d]# ps -ef|grep -c [n]ginx 第二步：编写zabbix-agent配置文件 #第一种方法: 直接修改zabbix-agent配置文件参数 UserParameter= # 第二种方法: 在zabbix_agentd.d/目录中编写自定义监控文件 # UserParameter=键(变量名),值(变量信息) # UserParameter=web_state,ps -ef|grep -c [n]ginx [root@web01 zabbix_agentd.d]# cat web_server.conf UserParameter=web_state,ps -ef|grep -c [n]ginx 第三步: 重启zabbix-agent服务 [root@web01 zabbix_agentd.d]# systemctl restart zabbix-agent.service [root@web01 zabbix_agentd.d]# systemctl status zabbix-agent.service 2) 在zabbix-server命令行进行操作 第一步： 检测自定义监控信息是否正确 [root@zabbix ~]# yum -y install zabbix-get [root@zabbix ~]# zabbix_get -s 10.0.0.7 -k 'web_state' 3 3)在zabbix-server网站页面进行配置 第一个历程: 进入到创建监控项页面: 配置---主机---选择相应主机的监控项 第二个历程: 监控项页面如何配置: 名称 键值 更新间隔时间 应用集 第三个历程: 检查是否收集到监控信息 需求2:复杂的自定义监控配置(多个服务状态) # 编辑配置文件 [root@web01 zabbix_agentd.d]# vim server_state.conf UserParameter=server_state[*],netstat -lntup|grep -c $1 # 重启服务 [root@web01 zabbix_agentd.d]# systemctl restart zabbix-agent.service [root@web01 zabbix_agentd.d]# Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"zabbix/wx.html":{"url":"zabbix/wx.html","title":"微信报警配置","keywords":"","body":"微信报警配置 1、需要注册企业微信,并进行配置 我的企业: 01. 获取企业id: ww32d68104ab5f51b0 02. 获取企业二维码: 允许员工加入 管理工具: 01. 成员加入---进行审核通过 应用小程序: 01. 进行创建 02. 收集程序信息 AgentId: Secret: RvQYpaCjWbYMCcwhnPqg1ZYcEGB9cOQCvvlkn-ft6j4 2、脚本wx.py cat /etc/zabbix/zabbix-server.conf AlertScriptsPath=/usr/lib/zabbix/alertscripts --- 放置告警脚本 #!/usr/bin/env python #-*- coding: utf-8 -*- import requests import sys import os import json import logging logging.basicConfig(level = logging.DEBUG, format = '%(asctime)s, %(filename)s, %(levelname)s, %(message)s',datefmt = '%a, %d %b %Y %H:%M:%S',filename = os.path.join('/tmp','weixin.log'),filemode = 'a') corpid='ww4f8d9fad75efbe' 3、修改添加报警媒介---定义发微信配置 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"zabbix/email.html":{"url":"zabbix/email.html","title":"邮件报警配置","keywords":"","body":"邮件报警配置 1、创建触发器 配置---主机---选择相应监控主机触发器---创建触发器 设置好表达式 {web01:server_state[nginx].last()} 2、修改动作配置 配置---动作---将默认动作进行开启 3、建立和163邮箱服务关系 管理---报警媒介类型---创建报警媒介 4、定义接收报警的邮件地址 小人头--报警媒介--设置收件人信息 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 13:36:54 "},"python/mac_install_python3.html":{"url":"python/mac_install_python3.html","title":"mac安装pyhton3.0环境","keywords":"","body":"mac安装pyhton3.0环境 确认系统版本 uname -a 官网下载选择需要的python版本 官网下载链接：https://www.python.org/downloads/macos/ 安装程序 选择安装包 双击运行安装，点击弹出框中的继续 选择安装目录 点击存储选择按钮，在弹出款中选择python存储的位置 确认无误后一路选择继续，在弹出款中输入账号密码，开始安装 环境变量配置 查看python安装目录 运行终端 执行which查看安装路径 which python3 编辑ba sh_file文件 vi ~/.bash_profile #python的路径见上一步 PATH=\"/usr/local/bin/python3/bin:${PATH}\" export PATH alias python=\"/usr/local/bin/python3\" 执行 source ~/.bash_profile 使配置生效 验证结果 新建运行终端输入python 输入 print ('hello world') 到此python环境安装完成 Copyright © wjhlog.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-05-28 15:49:00 "}}